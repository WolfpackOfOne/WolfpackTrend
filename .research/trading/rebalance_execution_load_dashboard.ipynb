{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebalance Execution Load Dashboard\n",
    "\n",
    "This notebook plots execution load and progress as a percentage of NAV for each rebalance cycle, including signal-tier breakdowns.\n",
    "\n",
    "Included sections:\n",
    "- Planned vs filled order load (% NAV) per rebalance\n",
    "- Completion rate and unfilled carryover\n",
    "- Progress quantiles across the 5-day rebalance cycle\n",
    "- Signal-tier dashboard (planned %, filled %, completion %, average day-of-fill)\n",
    "- Buy vs sell execution load\n",
    "- Slippage cost (bps and % NAV)\n",
    "- Participation vs liquidity (planned notional as % of ADV, optional)\n",
    "- Target drift during cycle\n",
    "- Turnover % NAV per rebalance\n",
    "- Outlier annotations for stressed cycles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 180)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "df_snap = read_csv_from_store(f'{TEAM_ID}/daily_snapshots.csv')\n",
    "df_targets = read_csv_from_store(f'{TEAM_ID}/targets.csv')\n",
    "df_signals = read_csv_from_store(f'{TEAM_ID}/signals.csv')\n",
    "df_events = read_csv_from_store(f'{TEAM_ID}/order_events.csv')\n",
    "df_slippage = read_csv_from_store(f'{TEAM_ID}/slippage.csv')\n",
    "\n",
    "if df_snap is None or df_targets is None:\n",
    "    raise ValueError('daily_snapshots.csv and targets.csv are required for this dashboard.')\n",
    "\n",
    "print(f\"Rows -> snapshots: {len(df_snap):,}, targets: {len(df_targets):,}, signals: {0 if df_signals is None else len(df_signals):,}, order_events: {0 if df_events is None else len(df_events):,}, slippage: {0 if df_slippage is None else len(df_slippage):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_tag_value(tag, key):\n",
    "    if pd.isna(tag):\n",
    "        return np.nan\n",
    "    match = re.search(rf'{key}=([^;]+)', str(tag))\n",
    "    return match.group(1) if match else np.nan\n",
    "\n",
    "\n",
    "def tier_from_magnitude(m):\n",
    "    if pd.isna(m):\n",
    "        return 'unknown'\n",
    "    x = abs(float(m))\n",
    "    if x >= 0.70:\n",
    "        return 'strong'\n",
    "    if x >= 0.30:\n",
    "        return 'moderate'\n",
    "    return 'weak'\n",
    "\n",
    "\n",
    "tier_order = ['strong', 'moderate', 'weak', 'unknown']\n",
    "\n",
    "# Snapshots (portfolio NAV reference)\n",
    "df_snap = df_snap.copy()\n",
    "df_snap['date'] = pd.to_datetime(df_snap['date']).dt.normalize()\n",
    "df_snap = to_numeric(df_snap, ['nav', 'gross_exposure', 'net_exposure', 'daily_pnl'])\n",
    "df_snap = df_snap.sort_values('date')\n",
    "nav_by_date = df_snap[['date', 'nav']].drop_duplicates(subset=['date'])\n",
    "\n",
    "# Targets (weekly plan vs actual state)\n",
    "dx = df_targets.copy()\n",
    "dx['date'] = pd.to_datetime(dx['date']).dt.normalize()\n",
    "dx['week_id'] = dx['week_id'].astype(str)\n",
    "missing_week = dx['week_id'].isin(['', 'nan', 'None'])\n",
    "dx.loc[missing_week, 'week_id'] = dx.loc[missing_week, 'date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "dx = to_numeric(\n",
    "    dx,\n",
    "    ['start_w', 'weekly_target_w', 'scheduled_fraction', 'scheduled_w', 'actual_w', 'scale_day']\n",
    ")\n",
    "dx['scale_day'] = dx['scale_day'].fillna(0).astype(int)\n",
    "\n",
    "# Join signal magnitude to assign tier at week/symbol level\n",
    "if df_signals is not None and len(df_signals):\n",
    "    sig = df_signals.copy()\n",
    "    sig['date'] = pd.to_datetime(sig['date']).dt.normalize()\n",
    "    sig = to_numeric(sig, ['magnitude'])\n",
    "    sig['week_id'] = sig['date'].dt.strftime('%Y-%m-%d')\n",
    "    week_signal = (\n",
    "        sig.sort_values(['week_id', 'symbol', 'date'])\n",
    "           .drop_duplicates(subset=['week_id', 'symbol'], keep='last')[['week_id', 'symbol', 'magnitude']]\n",
    "    )\n",
    "else:\n",
    "    week_signal = pd.DataFrame(columns=['week_id', 'symbol', 'magnitude'])\n",
    "\n",
    "dx = dx.merge(week_signal, on=['week_id', 'symbol'], how='left')\n",
    "dx['signal_tier'] = dx['magnitude'].apply(tier_from_magnitude)\n",
    "\n",
    "# Core execution-load metrics in weight-space (% NAV)\n",
    "dx['delta_w'] = dx['weekly_target_w'] - dx['start_w']\n",
    "dx['planned_order_abs'] = dx['delta_w'].abs()\n",
    "dx['remaining_abs'] = (dx['weekly_target_w'] - dx['actual_w']).abs()\n",
    "dx['remaining_abs'] = np.minimum(dx['remaining_abs'], dx['planned_order_abs'])\n",
    "dx['filled_abs'] = (dx['planned_order_abs'] - dx['remaining_abs']).clip(lower=0.0)\n",
    "dx['schedule_drift_abs'] = (dx['actual_w'] - dx['scheduled_w']).abs()\n",
    "\n",
    "dx['buy_planned_abs'] = np.where(dx['delta_w'] > 0, dx['planned_order_abs'], 0.0)\n",
    "dx['sell_planned_abs'] = np.where(dx['delta_w'] < 0, dx['planned_order_abs'], 0.0)\n",
    "dx['buy_filled_abs'] = np.where(dx['delta_w'] > 0, dx['filled_abs'], 0.0)\n",
    "dx['sell_filled_abs'] = np.where(dx['delta_w'] < 0, dx['filled_abs'], 0.0)\n",
    "\n",
    "dx = dx.sort_values(['week_id', 'symbol', 'date'])\n",
    "dx['day_in_cycle'] = dx.groupby(['week_id', 'symbol']).cumcount()\n",
    "\n",
    "# Portfolio-level daily progression inside each rebalance cycle\n",
    "portfolio_daily = (\n",
    "    dx.groupby(['week_id', 'date'], as_index=False)\n",
    "      .agg(\n",
    "          planned_pct_nav=('planned_order_abs', 'sum'),\n",
    "          remaining_pct_nav=('remaining_abs', 'sum'),\n",
    "          filled_pct_nav=('filled_abs', 'sum'),\n",
    "          schedule_drift_pct_nav=('schedule_drift_abs', 'sum'),\n",
    "          buy_planned_pct_nav=('buy_planned_abs', 'sum'),\n",
    "          sell_planned_pct_nav=('sell_planned_abs', 'sum'),\n",
    "          buy_filled_pct_nav=('buy_filled_abs', 'sum'),\n",
    "          sell_filled_pct_nav=('sell_filled_abs', 'sum')\n",
    "      )\n",
    "      .sort_values(['week_id', 'date'])\n",
    ")\n",
    "\n",
    "portfolio_daily['completion_rate'] = np.where(\n",
    "    portfolio_daily['planned_pct_nav'] > 1e-12,\n",
    "    portfolio_daily['filled_pct_nav'] / portfolio_daily['planned_pct_nav'],\n",
    "    0.0\n",
    ")\n",
    "portfolio_daily['day_in_cycle'] = portfolio_daily.groupby('week_id').cumcount()\n",
    "\n",
    "# Rebalance-level summary\n",
    "weekly = (\n",
    "    portfolio_daily.groupby('week_id', as_index=False)\n",
    "                 .agg(\n",
    "                     start_date=('date', 'min'),\n",
    "                     end_date=('date', 'max'),\n",
    "                     planned_pct_nav=('planned_pct_nav', 'max'),\n",
    "                     final_filled_pct_nav=('filled_pct_nav', 'last'),\n",
    "                     final_completion_rate=('completion_rate', 'last'),\n",
    "                     final_remaining_pct_nav=('remaining_pct_nav', 'last')\n",
    "                 )\n",
    "                 .sort_values('start_date')\n",
    ")\n",
    "weekly['unfilled_carryover_pct_nav'] = (\n",
    "    weekly['planned_pct_nav'] - weekly['final_filled_pct_nav']\n",
    ").clip(lower=0.0)\n",
    "weekly = weekly.merge(\n",
    "    nav_by_date.rename(columns={'date': 'start_date', 'nav': 'week_start_nav'}),\n",
    "    on='start_date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Symbol-level week summary (for tier breakdowns)\n",
    "symbol_final = (\n",
    "    dx.groupby(['week_id', 'symbol', 'signal_tier'], as_index=False)\n",
    "      .agg(\n",
    "          delta_w=('delta_w', 'max'),\n",
    "          planned_pct_nav=('planned_order_abs', 'max'),\n",
    "          final_filled_pct_nav=('filled_abs', 'last'),\n",
    "          final_remaining_pct_nav=('remaining_abs', 'last')\n",
    "      )\n",
    ")\n",
    "symbol_final['completion_rate'] = np.where(\n",
    "    symbol_final['planned_pct_nav'] > 1e-12,\n",
    "    symbol_final['final_filled_pct_nav'] / symbol_final['planned_pct_nav'],\n",
    "    0.0\n",
    ")\n",
    "symbol_final['buy_planned_pct_nav'] = np.where(symbol_final['delta_w'] > 0, symbol_final['planned_pct_nav'], 0.0)\n",
    "symbol_final['sell_planned_pct_nav'] = np.where(symbol_final['delta_w'] < 0, symbol_final['planned_pct_nav'], 0.0)\n",
    "symbol_final['buy_filled_pct_nav'] = np.where(symbol_final['delta_w'] > 0, symbol_final['final_filled_pct_nav'], 0.0)\n",
    "symbol_final['sell_filled_pct_nav'] = np.where(symbol_final['delta_w'] < 0, symbol_final['final_filled_pct_nav'], 0.0)\n",
    "\n",
    "# Average day-of-fill for each week/symbol using fill increments\n",
    "dx_fill = dx[['week_id', 'symbol', 'day_in_cycle', 'filled_abs']].copy()\n",
    "dx_fill['prev_filled'] = dx_fill.groupby(['week_id', 'symbol'])['filled_abs'].shift(1).fillna(0.0)\n",
    "dx_fill['fill_increment'] = (dx_fill['filled_abs'] - dx_fill['prev_filled']).clip(lower=0.0)\n",
    "\n",
    "def weighted_fill_day(g):\n",
    "    w = g['fill_increment'].to_numpy(dtype=float)\n",
    "    d = g['day_in_cycle'].to_numpy(dtype=float)\n",
    "    total_w = w.sum()\n",
    "    if total_w <= 1e-12:\n",
    "        return np.nan\n",
    "    return float((d * w).sum() / total_w)\n",
    "\n",
    "fill_day = (\n",
    "    dx_fill.groupby(['week_id', 'symbol'])\n",
    "           .apply(weighted_fill_day)\n",
    "           .reset_index(name='avg_fill_day')\n",
    ")\n",
    "symbol_final = symbol_final.merge(fill_day, on=['week_id', 'symbol'], how='left')\n",
    "\n",
    "# Tier summary: averages across rebalances\n",
    "tier_week = (\n",
    "    symbol_final.groupby(['week_id', 'signal_tier'], as_index=False)\n",
    "               .agg(\n",
    "                   planned_pct_nav=('planned_pct_nav', 'sum'),\n",
    "                   filled_pct_nav=('final_filled_pct_nav', 'sum')\n",
    "               )\n",
    ")\n",
    "tier_week['completion_rate'] = np.where(\n",
    "    tier_week['planned_pct_nav'] > 1e-12,\n",
    "    tier_week['filled_pct_nav'] / tier_week['planned_pct_nav'],\n",
    "    0.0\n",
    ")\n",
    "\n",
    "def weighted_tier_fill_day(g):\n",
    "    mask = g['avg_fill_day'].notna() & (g['final_filled_pct_nav'] > 0)\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "    vals = g.loc[mask, 'avg_fill_day'].to_numpy(dtype=float)\n",
    "    w = g.loc[mask, 'final_filled_pct_nav'].to_numpy(dtype=float)\n",
    "    return float((vals * w).sum() / w.sum())\n",
    "\n",
    "tier_fill = (\n",
    "    symbol_final.groupby(['week_id', 'signal_tier'])\n",
    "               .apply(weighted_tier_fill_day)\n",
    "               .reset_index(name='avg_fill_day')\n",
    ")\n",
    "tier_week = tier_week.merge(tier_fill, on=['week_id', 'signal_tier'], how='left')\n",
    "\n",
    "tier_summary = (\n",
    "    tier_week.groupby('signal_tier', as_index=False)\n",
    "             .agg(\n",
    "                 planned_pct_nav=('planned_pct_nav', 'mean'),\n",
    "                 filled_pct_nav=('filled_pct_nav', 'mean'),\n",
    "                 completion_rate=('completion_rate', 'mean'),\n",
    "                 avg_fill_day=('avg_fill_day', 'mean'),\n",
    "                 weeks=('week_id', 'nunique')\n",
    "             )\n",
    ")\n",
    "tier_summary['signal_tier'] = pd.Categorical(tier_summary['signal_tier'], categories=tier_order, ordered=True)\n",
    "tier_summary = tier_summary.sort_values('signal_tier')\n",
    "\n",
    "# Parse order events for turnover/slippage-by-tier/ADV participation\n",
    "if df_events is not None and len(df_events):\n",
    "    events = df_events.copy()\n",
    "    events['date'] = pd.to_datetime(events['date']).dt.normalize()\n",
    "    events = to_numeric(events, ['quantity', 'fill_quantity', 'fill_price', 'limit_price', 'market_price_at_submit'])\n",
    "    events['tier'] = events['tag'].apply(lambda t: parse_tag_value(t, 'tier')).fillna('unknown')\n",
    "    events['week_id'] = events['tag'].apply(lambda t: parse_tag_value(t, 'week_id'))\n",
    "    missing_event_week = events['week_id'].isna() | (events['week_id'].astype(str).str.strip() == '')\n",
    "    events.loc[missing_event_week, 'week_id'] = events.loc[missing_event_week, 'date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    fills = events[events['fill_quantity'].abs() > 1e-12].copy()\n",
    "    fills['fill_notional'] = fills['fill_quantity'].abs() * fills['fill_price'].abs()\n",
    "    fills['expected_notional'] = fills['fill_quantity'].abs() * fills['market_price_at_submit'].abs()\n",
    "    fills['implied_slippage_dollars'] = np.where(\n",
    "        fills['market_price_at_submit'].abs() > 1e-12,\n",
    "        (fills['fill_price'] - fills['market_price_at_submit']) * fills['fill_quantity'],\n",
    "        np.nan\n",
    "    )\n",
    "    fills = fills.merge(nav_by_date, on='date', how='left')\n",
    "    fills['fill_pct_nav'] = np.where(fills['nav'] > 1e-12, fills['fill_notional'] / fills['nav'], np.nan)\n",
    "    fills['implied_slippage_pct_nav'] = np.where(fills['nav'] > 1e-12, fills['implied_slippage_dollars'] / fills['nav'], np.nan)\n",
    "else:\n",
    "    events = pd.DataFrame(columns=['date', 'week_id', 'tier'])\n",
    "    fills = pd.DataFrame(columns=[\n",
    "        'date', 'week_id', 'tier', 'symbol', 'fill_quantity', 'fill_price', 'market_price_at_submit',\n",
    "        'fill_notional', 'expected_notional', 'implied_slippage_dollars', 'nav', 'fill_pct_nav', 'implied_slippage_pct_nav'\n",
    "    ])\n",
    "\n",
    "if len(fills):\n",
    "    daily_turnover = (\n",
    "        fills.groupby('date', as_index=False)\n",
    "             .agg(\n",
    "                 filled_notional=('fill_notional', 'sum'),\n",
    "                 implied_slippage_dollars=('implied_slippage_dollars', 'sum'),\n",
    "                 nav=('nav', 'last')\n",
    "             )\n",
    "    )\n",
    "    daily_turnover['turnover_pct_nav'] = np.where(\n",
    "        daily_turnover['nav'] > 1e-12,\n",
    "        daily_turnover['filled_notional'] / daily_turnover['nav'],\n",
    "        np.nan\n",
    "    )\n",
    "    daily_turnover['implied_slippage_bps'] = np.where(\n",
    "        daily_turnover['filled_notional'] > 1e-12,\n",
    "        10000.0 * daily_turnover['implied_slippage_dollars'] / daily_turnover['filled_notional'],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    weekly_turnover_raw = (\n",
    "        fills.groupby('week_id', as_index=False)\n",
    "             .agg(\n",
    "                 turnover_pct_nav=('fill_pct_nav', 'sum'),\n",
    "                 filled_notional=('fill_notional', 'sum'),\n",
    "                 implied_slippage_dollars=('implied_slippage_dollars', 'sum')\n",
    "             )\n",
    "    )\n",
    "else:\n",
    "    daily_turnover = pd.DataFrame(columns=['date', 'filled_notional', 'implied_slippage_dollars', 'nav', 'turnover_pct_nav', 'implied_slippage_bps'])\n",
    "    weekly_turnover_raw = pd.DataFrame(columns=['week_id', 'turnover_pct_nav', 'filled_notional', 'implied_slippage_dollars'])\n",
    "\n",
    "# Authoritative slippage stream (from slippage logger)\n",
    "if df_slippage is not None and len(df_slippage):\n",
    "    slp = df_slippage.copy()\n",
    "    slp['date'] = pd.to_datetime(slp['date']).dt.normalize()\n",
    "    slp = to_numeric(slp, ['slippage_dollars'])\n",
    "    daily_slippage = (\n",
    "        slp.groupby('date', as_index=False)\n",
    "           .agg(slippage_dollars=('slippage_dollars', 'sum'))\n",
    "    )\n",
    "else:\n",
    "    daily_slippage = pd.DataFrame(columns=['date', 'slippage_dollars'])\n",
    "\n",
    "daily_slippage = nav_by_date.merge(daily_slippage, on='date', how='left')\n",
    "daily_slippage['slippage_dollars'] = daily_slippage['slippage_dollars'].fillna(0.0)\n",
    "daily_slippage = daily_slippage.merge(\n",
    "    daily_turnover[['date', 'filled_notional']],\n",
    "    on='date',\n",
    "    how='left'\n",
    ")\n",
    "daily_slippage['filled_notional'] = daily_slippage['filled_notional'].fillna(0.0)\n",
    "daily_slippage['slippage_pct_nav'] = np.where(\n",
    "    daily_slippage['nav'] > 1e-12,\n",
    "    daily_slippage['slippage_dollars'] / daily_slippage['nav'],\n",
    "    np.nan\n",
    ")\n",
    "daily_slippage['slippage_bps'] = np.where(\n",
    "    daily_slippage['filled_notional'] > 1e-12,\n",
    "    10000.0 * daily_slippage['slippage_dollars'] / daily_slippage['filled_notional'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Week-level unified analytics table\n",
    "weekly_turnover = weekly[['week_id', 'start_date']].merge(weekly_turnover_raw, on='week_id', how='left')\n",
    "\n",
    "date_to_week = portfolio_daily[['date', 'week_id']].drop_duplicates(subset=['date'])\n",
    "weekly_slippage = (\n",
    "    date_to_week.merge(daily_slippage[['date', 'slippage_dollars']], on='date', how='left')\n",
    "                .groupby('week_id', as_index=False)\n",
    "                .agg(week_slippage_dollars=('slippage_dollars', 'sum'))\n",
    ")\n",
    "\n",
    "weekly_analytics = (\n",
    "    weekly.merge(weekly_turnover[['week_id', 'turnover_pct_nav', 'filled_notional']], on='week_id', how='left')\n",
    "          .merge(weekly_slippage, on='week_id', how='left')\n",
    "          .fillna({'turnover_pct_nav': 0.0, 'filled_notional': 0.0, 'week_slippage_dollars': 0.0})\n",
    ")\n",
    "\n",
    "print(f'Weeks analyzed: {len(weekly_analytics):,}')\n",
    "display(weekly_analytics.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weekly = weekly_analytics.sort_values('start_date').reset_index(drop=True)\n",
    "x = np.arange(len(plot_weekly))\n",
    "step = max(1, len(plot_weekly) // 12)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 11), sharex=True)\n",
    "\n",
    "# Planned vs filled load per rebalance (% NAV)\n",
    "width = 0.42\n",
    "axes[0].bar(x - width / 2, 100 * plot_weekly['planned_pct_nav'], width=width, color='#1f77b4', label='Planned')\n",
    "axes[0].bar(x + width / 2, 100 * plot_weekly['final_filled_pct_nav'], width=width, color='#2ca02c', label='Filled')\n",
    "axes[0].set_ylabel('% NAV')\n",
    "axes[0].set_title('Planned vs Filled Rebalance Load (% NAV)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Completion rate + unfilled carryover\n",
    "axes[1].plot(x, 100 * plot_weekly['final_completion_rate'], color='#d62728', marker='o', linewidth=1.8, label='Completion rate')\n",
    "axes[1].set_ylabel('Completion (%)')\n",
    "axes[1].set_ylim(0, 110)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax2 = axes[1].twinx()\n",
    "ax2.bar(x, 100 * plot_weekly['unfilled_carryover_pct_nav'], color='#ff9896', alpha=0.55, label='Unfilled carryover')\n",
    "ax2.set_ylabel('Unfilled (% NAV)')\n",
    "\n",
    "lines_1, labels_1 = axes[1].get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "axes[1].legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper right')\n",
    "axes[1].set_title('Completion Rate and Carryover by Rebalance')\n",
    "\n",
    "axes[1].set_xticks(x[::step])\n",
    "axes[1].set_xticklabels(plot_weekly['week_id'].iloc[::step], rotation=45, ha='right')\n",
    "axes[1].set_xlabel('week_id (rebalance date)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress quantiles across the cycle (as filled load % NAV)\n",
    "progress_quant = (\n",
    "    portfolio_daily.groupby('day_in_cycle')['filled_pct_nav']\n",
    "                 .quantile([0.25, 0.5, 0.75])\n",
    "                 .unstack()\n",
    "                 .rename(columns={0.25: 'p25', 0.5: 'p50', 0.75: 'p75'})\n",
    "                 .reset_index()\n",
    ")\n",
    "progress_mean = (\n",
    "    portfolio_daily.groupby('day_in_cycle', as_index=False)['filled_pct_nav']\n",
    "                 .mean()\n",
    "                 .rename(columns={'filled_pct_nav': 'mean'})\n",
    ")\n",
    "progress_plot = progress_quant.merge(progress_mean, on='day_in_cycle', how='left').sort_values('day_in_cycle')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(progress_plot['day_in_cycle'], 100 * progress_plot['p25'], marker='o', label='P25', color='#8da0cb')\n",
    "ax.plot(progress_plot['day_in_cycle'], 100 * progress_plot['p50'], marker='o', label='P50 (Median)', color='#1f77b4')\n",
    "ax.plot(progress_plot['day_in_cycle'], 100 * progress_plot['p75'], marker='o', label='P75', color='#66c2a5')\n",
    "ax.plot(progress_plot['day_in_cycle'], 100 * progress_plot['mean'], marker='s', linestyle='--', label='Mean', color='#d62728')\n",
    "ax.set_title('Execution Progress by Day in Rebalance Cycle (% NAV Filled)')\n",
    "ax.set_xlabel('Day in cycle (0-indexed)')\n",
    "ax.set_ylabel('Filled load (% NAV)')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(progress_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal-tier split dashboard\n",
    "tier_plot = tier_summary.copy()\n",
    "tier_plot = tier_plot[tier_plot['signal_tier'].isin(tier_order)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "sns.barplot(data=tier_plot, x='signal_tier', y=100 * tier_plot['planned_pct_nav'], order=tier_order, ax=axes[0, 0], color='#1f77b4')\n",
    "axes[0, 0].set_title('Avg Planned Load by Tier')\n",
    "axes[0, 0].set_xlabel('Signal tier')\n",
    "axes[0, 0].set_ylabel('Planned (% NAV)')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.barplot(data=tier_plot, x='signal_tier', y=100 * tier_plot['filled_pct_nav'], order=tier_order, ax=axes[0, 1], color='#2ca02c')\n",
    "axes[0, 1].set_title('Avg Filled Load by Tier')\n",
    "axes[0, 1].set_xlabel('Signal tier')\n",
    "axes[0, 1].set_ylabel('Filled (% NAV)')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.barplot(data=tier_plot, x='signal_tier', y=100 * tier_plot['completion_rate'], order=tier_order, ax=axes[1, 0], color='#d62728')\n",
    "axes[1, 0].set_title('Avg Completion Rate by Tier')\n",
    "axes[1, 0].set_xlabel('Signal tier')\n",
    "axes[1, 0].set_ylabel('Completion (%)')\n",
    "axes[1, 0].set_ylim(0, 110)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.barplot(data=tier_plot, x='signal_tier', y='avg_fill_day', order=tier_order, ax=axes[1, 1], color='#9467bd')\n",
    "axes[1, 1].set_title('Avg Day-of-Fill by Tier')\n",
    "axes[1, 1].set_xlabel('Signal tier')\n",
    "axes[1, 1].set_ylabel('Weighted average fill day (0-indexed)')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(tier_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Average holding period (strategy-level and by signal tier)\ndf_positions = read_csv_from_store(f'{TEAM_ID}/positions.csv')\n\nif df_positions is None or len(df_positions) == 0:\n    print('positions.csv not available; skipping holding-period analysis.')\nelse:\n    pos = df_positions.copy()\n    pos['date'] = pd.to_datetime(pos['date']).dt.normalize()\n    pos = to_numeric(pos, ['quantity', 'invested'])\n\n    if 'quantity' not in pos.columns:\n        raise ValueError('positions.csv must contain a quantity column.')\n\n    pos['quantity'] = pd.to_numeric(pos['quantity'], errors='coerce').fillna(0.0)\n\n    if 'invested' in pos.columns:\n        pos['is_invested'] = pd.to_numeric(pos['invested'], errors='coerce').fillna(0.0) > 0\n    else:\n        pos['is_invested'] = pos['quantity'].abs() > 1e-12\n\n    pos = pos.sort_values(['symbol', 'date'])\n    pos = pos.drop_duplicates(['symbol', 'date'], keep='last')\n\n    prev_invested = pos.groupby('symbol')['is_invested'].shift(1).fillna(False)\n    pos['start_flag'] = pos['is_invested'] & (~prev_invested)\n    pos['episode_id'] = pos.groupby('symbol')['start_flag'].cumsum()\n\n    invested_rows = pos[pos['is_invested']].copy()\n\n    episodes = (\n        invested_rows.groupby(['symbol', 'episode_id'], as_index=False)\n                    .agg(\n                        start_date=('date', 'min'),\n                        end_date=('date', 'max'),\n                        holding_days=('date', 'count'),\n                        entry_qty=('quantity', 'first'),\n                        exit_qty=('quantity', 'last')\n                    )\n    )\n\n    if len(episodes) == 0:\n        print('No invested episodes found in positions.csv.')\n    else:\n        last_state = (\n            pos.sort_values('date')\n               .groupby('symbol', as_index=False)\n               .tail(1)[['symbol', 'date', 'is_invested']]\n               .rename(columns={'date': 'symbol_last_date', 'is_invested': 'symbol_last_invested'})\n        )\n        episodes = episodes.merge(last_state, on='symbol', how='left')\n        episodes['open_censored'] = (\n            (episodes['end_date'] == episodes['symbol_last_date']) &\n            (episodes['symbol_last_invested'])\n        )\n\n        episodes['side'] = np.where(episodes['entry_qty'] >= 0, 'long', 'short')\n        episodes['entry_sign'] = np.where(episodes['entry_qty'] >= 0, 1, -1)\n\n        # Primary tier attribution from entry fills on episode start date.\n        if len(fills):\n            entry_fills = fills.copy()\n            entry_fills['tier_clean'] = entry_fills['tier'].astype(str).str.lower().str.strip()\n            entry_fills['fill_sign'] = np.where(entry_fills['fill_quantity'] >= 0, 1, -1)\n            if 'fill_notional' not in entry_fills.columns:\n                entry_fills['fill_notional'] = entry_fills['fill_quantity'].abs() * entry_fills['fill_price'].abs()\n\n            entry_fills = entry_fills[\n                entry_fills['tier_clean'].isin(['strong', 'moderate', 'weak']) &\n                (entry_fills['fill_notional'] > 0)\n            ]\n\n            fill_match = episodes.merge(\n                entry_fills[['symbol', 'date', 'fill_sign', 'tier_clean', 'fill_notional']],\n                left_on=['symbol', 'start_date'],\n                right_on=['symbol', 'date'],\n                how='left'\n            )\n            fill_match = fill_match[fill_match['fill_sign'] == fill_match['entry_sign']]\n            fill_match = fill_match.sort_values(['symbol', 'episode_id', 'fill_notional'], ascending=[True, True, False])\n            tier_map = (\n                fill_match.drop_duplicates(['symbol', 'episode_id'], keep='first')\n                          [['symbol', 'episode_id', 'tier_clean']]\n            )\n\n            episodes = episodes.merge(tier_map, on=['symbol', 'episode_id'], how='left')\n            episodes = episodes.rename(columns={'tier_clean': 'entry_tier'})\n        else:\n            episodes['entry_tier'] = np.nan\n\n        # Fallback: same-day signal tier at episode start.\n        if df_signals is not None and len(df_signals):\n            sig = df_signals[['date', 'symbol', 'magnitude']].copy()\n            sig['date'] = pd.to_datetime(sig['date']).dt.normalize()\n            sig = to_numeric(sig, ['magnitude'])\n            sig['signal_tier'] = sig['magnitude'].apply(tier_from_magnitude)\n            sig = sig.sort_values(['symbol', 'date']).drop_duplicates(['symbol', 'date'], keep='last')\n\n            episodes = episodes.merge(\n                sig[['symbol', 'date', 'signal_tier']].rename(columns={'date': 'start_date', 'signal_tier': 'entry_tier_signal'}),\n                on=['symbol', 'start_date'],\n                how='left'\n            )\n            episodes['entry_tier'] = episodes['entry_tier'].fillna(episodes['entry_tier_signal'])\n            episodes = episodes.drop(columns=['entry_tier_signal'])\n\n        episodes['entry_tier'] = episodes['entry_tier'].fillna('unknown')\n\n        closed = episodes[~episodes['open_censored']].copy()\n\n        if len(closed) == 0:\n            print('No closed episodes available yet; holding-period plot skipped.')\n        else:\n            tier_order_hp = tier_order  # tier_order already includes 'unknown'\n\n            overall_avg = float(closed['holding_days'].mean())\n            overall_med = float(closed['holding_days'].median())\n\n            tier_holding = (\n                closed.groupby('entry_tier', as_index=False)\n                      .agg(\n                          episodes=('holding_days', 'count'),\n                          avg_holding_days=('holding_days', 'mean'),\n                          median_holding_days=('holding_days', 'median')\n                      )\n            )\n            tier_holding['entry_tier'] = pd.Categorical(tier_holding['entry_tier'], categories=tier_order_hp, ordered=True)\n            tier_holding = tier_holding.sort_values('entry_tier')\n\n            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n            sns.barplot(\n                data=tier_holding,\n                x='entry_tier',\n                y='avg_holding_days',\n                order=tier_order_hp,\n                ax=axes[0],\n                color='#1f77b4'\n            )\n            axes[0].axhline(overall_avg, color='#d62728', linestyle='--', linewidth=2, label=f'Overall avg = {overall_avg:.2f}d')\n            axes[0].set_title('Average Holding Period by Entry Signal Tier')\n            axes[0].set_xlabel('Entry signal tier')\n            axes[0].set_ylabel('Avg holding period (trading days)')\n            axes[0].grid(axis='y', alpha=0.3)\n            axes[0].legend()\n\n            sns.boxplot(\n                data=closed,\n                x='entry_tier',\n                y='holding_days',\n                order=tier_order_hp,\n                ax=axes[1]\n            )\n            axes[1].axhline(overall_avg, color='#d62728', linestyle='--', linewidth=2)\n            axes[1].set_title('Holding Period Distribution by Entry Tier')\n            axes[1].set_xlabel('Entry signal tier')\n            axes[1].set_ylabel('Holding period (trading days)')\n            axes[1].grid(axis='y', alpha=0.3)\n\n            plt.tight_layout()\n            plt.show()\n\n            overall_summary = pd.DataFrame([\n                {\n                    'group': 'strategy_overall',\n                    'episodes': int(len(closed)),\n                    'avg_holding_days': overall_avg,\n                    'median_holding_days': overall_med\n                }\n            ])\n\n            print('Holding-period summary (closed episodes only):')\n            display(overall_summary)\n            display(tier_holding)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buy vs sell execution load by rebalance\n",
    "weekly_side = (\n",
    "    portfolio_daily.groupby('week_id', as_index=False)\n",
    "                  .agg(\n",
    "                      start_date=('date', 'min'),\n",
    "                      planned_buy_pct_nav=('buy_planned_pct_nav', 'max'),\n",
    "                      planned_sell_pct_nav=('sell_planned_pct_nav', 'max'),\n",
    "                      filled_buy_pct_nav=('buy_filled_pct_nav', 'last'),\n",
    "                      filled_sell_pct_nav=('sell_filled_pct_nav', 'last')\n",
    "                  )\n",
    "                  .sort_values('start_date')\n",
    ")\n",
    "\n",
    "x = np.arange(len(weekly_side))\n",
    "step = max(1, len(weekly_side) // 12)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 11), sharex=True)\n",
    "\n",
    "axes[0].bar(x, 100 * weekly_side['planned_buy_pct_nav'], color='#2ca02c', label='Planned buys')\n",
    "axes[0].bar(x, -100 * weekly_side['planned_sell_pct_nav'], color='#ff7f0e', label='Planned sells')\n",
    "axes[0].axhline(0, color='black', linewidth=0.8)\n",
    "axes[0].set_ylabel('% NAV')\n",
    "axes[0].set_title('Planned Buy vs Sell Load (% NAV)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(x, 100 * weekly_side['filled_buy_pct_nav'], color='#1f77b4', label='Filled buys')\n",
    "axes[1].bar(x, -100 * weekly_side['filled_sell_pct_nav'], color='#d62728', label='Filled sells')\n",
    "axes[1].axhline(0, color='black', linewidth=0.8)\n",
    "axes[1].set_ylabel('% NAV')\n",
    "axes[1].set_title('Filled Buy vs Sell Load (% NAV)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].set_xticks(x[::step])\n",
    "axes[1].set_xticklabels(weekly_side['week_id'].iloc[::step], rotation=45, ha='right')\n",
    "axes[1].set_xlabel('week_id (rebalance date)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(weekly_side.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slippage: overall and by tier\n",
    "if len(fills):\n",
    "    tier_slippage = (\n",
    "        fills.groupby('tier', as_index=False)\n",
    "             .agg(\n",
    "                 fill_notional=('fill_notional', 'sum'),\n",
    "                 slippage_dollars=('implied_slippage_dollars', 'sum'),\n",
    "                 slippage_pct_nav=('implied_slippage_pct_nav', 'sum'),\n",
    "                 fill_rows=('fill_quantity', 'count')\n",
    "             )\n",
    "    )\n",
    "    tier_slippage['slippage_bps'] = np.where(\n",
    "        tier_slippage['fill_notional'] > 1e-12,\n",
    "        10000.0 * tier_slippage['slippage_dollars'] / tier_slippage['fill_notional'],\n",
    "        np.nan\n",
    "    )\n",
    "    tier_slippage['tier'] = pd.Categorical(tier_slippage['tier'], categories=tier_order + ['exit'], ordered=True)\n",
    "    tier_slippage = tier_slippage.sort_values('tier')\n",
    "else:\n",
    "    tier_slippage = pd.DataFrame(columns=['tier', 'fill_notional', 'slippage_dollars', 'slippage_pct_nav', 'fill_rows', 'slippage_bps'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "daily_slip_plot = daily_slippage.sort_values('date')\n",
    "axes[0].plot(daily_slip_plot['date'], 10000 * daily_slip_plot['slippage_pct_nav'], color='#d62728', linewidth=1.4, label='Slippage (bps of NAV)')\n",
    "axes[0].set_title('Daily Slippage (from slippage.csv)')\n",
    "axes[0].set_ylabel('bps of NAV')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "ax2 = axes[0].twinx()\n",
    "ax2.plot(daily_slip_plot['date'], daily_slip_plot['slippage_bps'], color='#1f77b4', linewidth=1.2, alpha=0.7, label='Slippage (bps of filled notional)')\n",
    "ax2.set_ylabel('bps of filled notional')\n",
    "\n",
    "lines_1, labels_1 = axes[0].get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "axes[0].legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper right')\n",
    "\n",
    "if len(tier_slippage):\n",
    "    width = 0.38\n",
    "    x = np.arange(len(tier_slippage))\n",
    "    axes[1].bar(x - width / 2, tier_slippage['slippage_bps'], width=width, color='#9467bd', label='Slippage bps')\n",
    "    axes[1].bar(x + width / 2, 100 * tier_slippage['slippage_pct_nav'], width=width, color='#ff9896', label='Slippage % NAV')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(tier_slippage['tier'].astype(str))\n",
    "    axes[1].set_title('Implied Slippage by Signal Tier (from order_events fill prices)')\n",
    "    axes[1].set_ylabel('bps / %NAV')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].legend()\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No fill rows available in order_events.csv', ha='center', va='center')\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(tier_slippage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participation vs liquidity (planned notional as % of ADV20)\n",
    "symbol_week = symbol_final.merge(\n",
    "    weekly[['week_id', 'start_date', 'week_start_nav']],\n",
    "    on='week_id',\n",
    "    how='left'\n",
    ")\n",
    "symbol_week['planned_notional'] = symbol_week['planned_pct_nav'] * symbol_week['week_start_nav']\n",
    "\n",
    "participation = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    adv_input = symbol_week[symbol_week['planned_notional'] > 0].copy()\n",
    "    adv_input = adv_input.dropna(subset=['start_date'])\n",
    "\n",
    "    tickers = sorted(adv_input['symbol'].dropna().astype(str).unique())\n",
    "    qc_symbols = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            qc_symbols[ticker] = qb.AddEquity(ticker, Resolution.Daily).Symbol\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if qc_symbols:\n",
    "        hist_start = adv_input['start_date'].min() - pd.Timedelta(days=90)\n",
    "        hist_end = adv_input['start_date'].max() + pd.Timedelta(days=1)\n",
    "        hist = qb.History(list(qc_symbols.values()), hist_start, hist_end, Resolution.Daily)\n",
    "\n",
    "        if hist is not None and len(hist):\n",
    "            hist = hist.reset_index()\n",
    "\n",
    "            symbol_col = 'symbol' if 'symbol' in hist.columns else hist.columns[0]\n",
    "            if 'time' in hist.columns:\n",
    "                time_col = 'time'\n",
    "            elif 'end_time' in hist.columns:\n",
    "                time_col = 'end_time'\n",
    "            else:\n",
    "                time_col = hist.columns[1]\n",
    "\n",
    "            close_col = 'close' if 'close' in hist.columns else 'Close'\n",
    "            volume_col = 'volume' if 'volume' in hist.columns else 'Volume'\n",
    "\n",
    "            hist['symbol'] = hist[symbol_col].astype(str).str.split(' ').str[0]\n",
    "            hist['date'] = pd.to_datetime(hist[time_col]).dt.normalize()\n",
    "            hist['close_val'] = pd.to_numeric(hist[close_col], errors='coerce')\n",
    "            hist['volume_val'] = pd.to_numeric(hist[volume_col], errors='coerce')\n",
    "            hist['dollar_volume'] = hist['close_val'] * hist['volume_val']\n",
    "\n",
    "            hist = hist.sort_values(['symbol', 'date'])\n",
    "            hist['adv20'] = hist.groupby('symbol')['dollar_volume'].transform(lambda s: s.rolling(20, min_periods=5).mean())\n",
    "\n",
    "            adv_lookup = hist[['symbol', 'date', 'adv20']].dropna()\n",
    "\n",
    "            participation = adv_input.merge(\n",
    "                adv_lookup,\n",
    "                left_on=['symbol', 'start_date'],\n",
    "                right_on=['symbol', 'date'],\n",
    "                how='left'\n",
    "            )\n",
    "            participation['participation_pct_adv'] = 100.0 * participation['planned_notional'] / participation['adv20']\n",
    "            participation = participation.replace([np.inf, -np.inf], np.nan)\n",
    "            participation = participation.dropna(subset=['participation_pct_adv'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'ADV participation section skipped due to error: {e}')\n",
    "\n",
    "if len(participation):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=participation,\n",
    "        x='signal_tier',\n",
    "        y='participation_pct_adv',\n",
    "        order=tier_order,\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('Planned Notional as % of ADV20 by Tier')\n",
    "    axes[0].set_xlabel('Signal tier')\n",
    "    axes[0].set_ylabel('% ADV20')\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    tier_adv = (\n",
    "        participation.groupby('signal_tier', as_index=False)\n",
    "                     .agg(\n",
    "                         mean_participation=('participation_pct_adv', 'mean'),\n",
    "                         median_participation=('participation_pct_adv', 'median'),\n",
    "                         count=('participation_pct_adv', 'count')\n",
    "                     )\n",
    "    )\n",
    "    tier_adv['signal_tier'] = pd.Categorical(tier_adv['signal_tier'], categories=tier_order, ordered=True)\n",
    "    tier_adv = tier_adv.sort_values('signal_tier')\n",
    "\n",
    "    width = 0.38\n",
    "    x = np.arange(len(tier_adv))\n",
    "    axes[1].bar(x - width / 2, tier_adv['mean_participation'], width=width, color='#1f77b4', label='Mean %ADV')\n",
    "    axes[1].bar(x + width / 2, tier_adv['median_participation'], width=width, color='#2ca02c', label='Median %ADV')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(tier_adv['signal_tier'].astype(str))\n",
    "    axes[1].set_title('Participation Summary by Tier')\n",
    "    axes[1].set_xlabel('Signal tier')\n",
    "    axes[1].set_ylabel('% ADV20')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    display(tier_adv)\n",
    "else:\n",
    "    print('Participation vs ADV plot skipped: unable to build ADV20 history for this dataset.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target drift while orders are working\n",
    "drift_plot = portfolio_daily.sort_values('date')\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "axes[0].plot(drift_plot['date'], 100 * drift_plot['remaining_pct_nav'], color='#d62728', linewidth=1.6, label='Distance to weekly target')\n",
    "axes[0].plot(drift_plot['date'], 100 * drift_plot['schedule_drift_pct_nav'], color='#1f77b4', linewidth=1.4, alpha=0.8, label='Distance to scheduled target')\n",
    "axes[0].set_title('Target Drift Through Time (% NAV)')\n",
    "axes[0].set_ylabel('% NAV')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "day_drift = (\n",
    "    portfolio_daily.groupby('day_in_cycle', as_index=False)\n",
    "                 .agg(\n",
    "                     mean_target_drift=('remaining_pct_nav', 'mean'),\n",
    "                     mean_schedule_drift=('schedule_drift_pct_nav', 'mean')\n",
    "                 )\n",
    ")\n",
    "\n",
    "axes[1].plot(day_drift['day_in_cycle'], 100 * day_drift['mean_target_drift'], marker='o', color='#d62728', label='Mean distance to weekly target')\n",
    "axes[1].plot(day_drift['day_in_cycle'], 100 * day_drift['mean_schedule_drift'], marker='s', color='#1f77b4', label='Mean distance to scheduled target')\n",
    "axes[1].set_title('Average Drift Profile by Day in Cycle')\n",
    "axes[1].set_xlabel('Day in cycle (0-indexed)')\n",
    "axes[1].set_ylabel('% NAV')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(day_drift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turnover % NAV per rebalance (from order fills)\n",
    "turnover_plot = (\n",
    "    weekly[['week_id', 'start_date']]\n",
    "    .merge(weekly_turnover_raw, on='week_id', how='left')\n",
    "    .fillna({'turnover_pct_nav': 0.0, 'filled_notional': 0.0, 'implied_slippage_dollars': 0.0})\n",
    "    .sort_values('start_date')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "x = np.arange(len(turnover_plot))\n",
    "step = max(1, len(turnover_plot) // 12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.bar(x, 100 * turnover_plot['turnover_pct_nav'], color='#17becf', alpha=0.75, label='Turnover % NAV')\n",
    "ax.set_title('Turnover per Rebalance (% NAV)')\n",
    "ax.set_ylabel('Turnover (% NAV)')\n",
    "ax.set_xlabel('week_id (rebalance date)')\n",
    "ax.set_xticks(x[::step])\n",
    "ax.set_xticklabels(turnover_plot['week_id'].iloc[::step], rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, turnover_plot['filled_notional'], color='#1f77b4', linewidth=1.4, marker='o', alpha=0.7, label='Filled notional ($)')\n",
    "ax2.set_ylabel('Filled notional ($)')\n",
    "\n",
    "lines_1, labels_1 = ax.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(turnover_plot.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized turnover (% NAV/year): compare three definitions weekly\n",
    "if len(fills):\n",
    "    fill_defs = fills.copy()\n",
    "    fill_defs['buy_notional'] = np.where(fill_defs['fill_quantity'] > 0, fill_defs['fill_notional'], 0.0)\n",
    "    fill_defs['sell_notional'] = np.where(fill_defs['fill_quantity'] < 0, fill_defs['fill_notional'], 0.0)\n",
    "\n",
    "    weekly_trade_defs = (\n",
    "        fill_defs.groupby('week_id', as_index=False)\n",
    "                 .agg(\n",
    "                     buy_notional=('buy_notional', 'sum'),\n",
    "                     sell_notional=('sell_notional', 'sum'),\n",
    "                     gross_notional=('fill_notional', 'sum')\n",
    "                 )\n",
    "    )\n",
    "else:\n",
    "    weekly_trade_defs = pd.DataFrame(columns=['week_id', 'buy_notional', 'sell_notional', 'gross_notional'])\n",
    "\n",
    "week_nav = (\n",
    "    date_to_week.merge(nav_by_date, on='date', how='left')\n",
    "               .groupby('week_id', as_index=False)\n",
    "               .agg(\n",
    "                   start_date=('date', 'min'),\n",
    "                   avg_nav=('nav', 'mean')\n",
    "               )\n",
    ")\n",
    "\n",
    "annual_turn = (\n",
    "    week_nav.merge(weekly_trade_defs, on='week_id', how='left')\n",
    "            .fillna({'buy_notional': 0.0, 'sell_notional': 0.0, 'gross_notional': 0.0})\n",
    "            .sort_values('start_date')\n",
    "            .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "annual_turn['gross_weekly_pct_nav'] = np.where(\n",
    "    annual_turn['avg_nav'] > 1e-12,\n",
    "    annual_turn['gross_notional'] / annual_turn['avg_nav'],\n",
    "    np.nan\n",
    ")\n",
    "annual_turn['one_way_weekly_pct_nav'] = np.where(\n",
    "    annual_turn['avg_nav'] > 1e-12,\n",
    "    np.minimum(annual_turn['buy_notional'], annual_turn['sell_notional']) / annual_turn['avg_nav'],\n",
    "    np.nan\n",
    ")\n",
    "annual_turn['half_gross_weekly_pct_nav'] = 0.5 * annual_turn['gross_weekly_pct_nav']\n",
    "\n",
    "annual_turn['annualized_gross_pct_nav'] = 100.0 * annual_turn['gross_weekly_pct_nav'] * 52.0\n",
    "annual_turn['annualized_one_way_pct_nav'] = 100.0 * annual_turn['one_way_weekly_pct_nav'] * 52.0\n",
    "annual_turn['annualized_half_gross_pct_nav'] = 100.0 * annual_turn['half_gross_weekly_pct_nav'] * 52.0\n",
    "\n",
    "for col in ['annualized_gross_pct_nav', 'annualized_one_way_pct_nav', 'annualized_half_gross_pct_nav']:\n",
    "    annual_turn[f'{col}_4w_ma'] = annual_turn[col].rolling(4, min_periods=1).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "ax.plot(\n",
    "    annual_turn['start_date'],\n",
    "    annual_turn['annualized_gross_pct_nav'],\n",
    "    color='#2ca02c',\n",
    "    linewidth=1.8,\n",
    "    alpha=0.65,\n",
    "    label='Gross traded notional (buys + sells)'\n",
    ")\n",
    "ax.plot(\n",
    "    annual_turn['start_date'],\n",
    "    annual_turn['annualized_one_way_pct_nav'],\n",
    "    color='#1f77b4',\n",
    "    linewidth=2.1,\n",
    "    label='One-way turnover min(buys, sells)'\n",
    ")\n",
    "ax.plot(\n",
    "    annual_turn['start_date'],\n",
    "    annual_turn['annualized_half_gross_pct_nav'],\n",
    "    color='#ff7f0e',\n",
    "    linewidth=1.8,\n",
    "    label='Half-gross proxy 0.5 * (buys + sells)'\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    annual_turn['start_date'],\n",
    "    annual_turn['annualized_gross_pct_nav_4w_ma'],\n",
    "    color='#2ca02c',\n",
    "    linewidth=1.4,\n",
    "    linestyle='--',\n",
    "    alpha=0.9,\n",
    "    label='Gross (4w MA)'\n",
    ")\n",
    "ax.plot(\n",
    "    annual_turn['start_date'],\n",
    "    annual_turn['annualized_one_way_pct_nav_4w_ma'],\n",
    "    color='#1f77b4',\n",
    "    linewidth=1.6,\n",
    "    linestyle='--',\n",
    "    alpha=0.95,\n",
    "    label='One-way (4w MA)'\n",
    ")\n",
    "ax.plot(\n",
    "    annual_turn['start_date'],\n",
    "    annual_turn['annualized_half_gross_pct_nav_4w_ma'],\n",
    "    color='#ff7f0e',\n",
    "    linewidth=1.4,\n",
    "    linestyle='--',\n",
    "    alpha=0.9,\n",
    "    label='Half-gross (4w MA)'\n",
    ")\n",
    "\n",
    "ax.set_title('Annualized Portfolio Turnover by Rebalance Week (Three Definitions)')\n",
    "ax.set_xlabel('Rebalance week')\n",
    "ax.set_ylabel('Annualized turnover (% NAV / year)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend(ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(\n",
    "    annual_turn[\n",
    "        [\n",
    "            'week_id',\n",
    "            'start_date',\n",
    "            'buy_notional',\n",
    "            'sell_notional',\n",
    "            'gross_notional',\n",
    "            'avg_nav',\n",
    "            'annualized_gross_pct_nav',\n",
    "            'annualized_one_way_pct_nav',\n",
    "            'annualized_half_gross_pct_nav'\n",
    "        ]\n",
    "    ].tail(12)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier annotation chart\n",
    "out = weekly_analytics.copy().sort_values('start_date').reset_index(drop=True)\n",
    "\n",
    "out['size_rank'] = out['planned_pct_nav'].rank(pct=True)\n",
    "out['carry_rank'] = out['unfilled_carryover_pct_nav'].rank(pct=True)\n",
    "out['slip_rank'] = out['week_slippage_dollars'].abs().rank(pct=True)\n",
    "out['outlier_score'] = (out['size_rank'] + out['carry_rank'] + out['slip_rank']) / 3.0\n",
    "\n",
    "top_outliers = out.nlargest(10, 'outlier_score').copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "bubble_size = 4000 * (out['turnover_pct_nav'].fillna(0.0) + 0.01)\n",
    "sc = ax.scatter(\n",
    "    100 * out['planned_pct_nav'],\n",
    "    100 * out['final_completion_rate'],\n",
    "    s=bubble_size,\n",
    "    c=out['outlier_score'],\n",
    "    cmap='YlOrRd',\n",
    "    alpha=0.8,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "plt.colorbar(sc, ax=ax, label='Outlier score')\n",
    "\n",
    "for _, row in top_outliers.iterrows():\n",
    "    ax.annotate(\n",
    "        row['week_id'],\n",
    "        (100 * row['planned_pct_nav'], 100 * row['final_completion_rate']),\n",
    "        textcoords='offset points',\n",
    "        xytext=(5, 4),\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "ax.set_title('Outlier Cycles: Load Size vs Completion (bubble=turnover %NAV)')\n",
    "ax.set_xlabel('Planned load (% NAV)')\n",
    "ax.set_ylabel('Final completion (%)')\n",
    "ax.set_ylim(0, 110)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(\n",
    "    top_outliers[\n",
    "        [\n",
    "            'week_id', 'start_date', 'planned_pct_nav', 'final_filled_pct_nav',\n",
    "            'final_completion_rate', 'unfilled_carryover_pct_nav',\n",
    "            'turnover_pct_nav', 'week_slippage_dollars', 'outlier_score'\n",
    "        ]\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}