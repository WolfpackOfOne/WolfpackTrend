{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Fill Aggressiveness\n",
    "\n",
    "Measure how aggressively weekly orders are getting filled each trading day.\n",
    "\n",
    "Uses `{TEAM_ID}/targets.csv` and computes:\n",
    "- `incremental_fill_pct`: percent of the weekly order filled *on that day* (same-day observed)\n",
    "- `cumulative_fill_pct`: percent of the weekly order filled *by that day* (same-day observed)\n",
    "- `incremental_fill_pct_settled_t1`: same metric but using next-day `actual_w` as a T+1 settled proxy\n",
    "- `cumulative_fill_pct_settled_t1`: cumulative T+1 settled proxy\n",
    "\n",
    "Plots are shown by:\n",
    "- calendar weekday (Mon-Fri)\n",
    "- scale day index (0..N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5t2yr81fbl4",
   "source": [
    "## Data Loading \u2014 Targets Log\n",
    "\n",
    "Loads the `targets.csv` log from ObjectStore, which records the daily scheduled and actual weight for each symbol within each rebalance week. The head preview confirms columns like `start_w`, `weekly_target_w`, `scheduled_fraction`, `actual_w`, and `scale_day` are present. All subsequent fill-aggressiveness calculations depend on this DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "q367z5cgkhr",
   "source": [
    "## Fill Aggressiveness Metric Computation\n",
    "\n",
    "This cell derives four fill-aggressiveness metrics per row: same-day incremental and cumulative fill percentage, and T+1 settled proxies that substitute the next trading day's actual weight for a more conservative estimate. The preview shows the computed columns alongside original target data for the first 15 rows. The two proxy series allow side-by-side comparison to detect systematic same-day versus settlement timing discrepancies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "7dwcizorckl",
   "source": [
    "## Weekday Fill Summary Tables\n",
    "\n",
    "These two tables summarize fill aggressiveness by calendar weekday at both the portfolio level (weighted by weekly order size) and the symbol level (simple mean). They show what fraction of the weekly order is typically filled on each day and how cumulative fill builds up across the trading week. Comparing same-day versus T+1 settled numbers by weekday highlights whether reported aggressiveness on any given day is partly an artifact of next-day settlement."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "s7nl4ry8oap",
   "source": [
    "## Weekday Fill Aggressiveness Charts\n",
    "\n",
    "These two side-by-side charts visualize fill aggressiveness by calendar weekday. The left panel shows the average fraction of the weekly order filled on each weekday, comparing same-day versus T+1 settled estimates as grouped bars. The right panel shows the cumulative fill percentage reached by each weekday, **restricted to complete 5-day symbol-weeks** to avoid Simpson's paradox (where changing composition across weekdays can make cumulative averages appear to decrease)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "v8t2r8cvebe",
   "source": [
    "## Scale Day Fill Profile \u2014 Table and Charts\n",
    "\n",
    "This section repeats the fill analysis indexed by scale day (0, 1, 2\u2026) rather than calendar weekday, which is more natural for the strategy's 5-day scaling schedule. The table shows average incremental and cumulative fill percentage for each scale day, and the two charts display the same data as grouped bars and a cumulative line. Front-loaded curves for strong-tier signals and a flatter profile for weak-tier signals would confirm the tier-dependent scaling design is working as intended."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "omustc1ubcm",
   "source": [
    "## Earliest Fill Symbol Ranking (Days 0\u20131)\n",
    "\n",
    "This table ranks individual symbols by their average fill aggressiveness on the first two scale days, highlighting which names tend to complete their orders earliest in the week. Consistently early fillers are typically more liquid stocks where limit orders at tight offsets clear quickly, while late fillers may require wider limits or more patient execution. Use this table alongside signal tier data to check whether early fills align with the strong-signal design intent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = read_csv_from_store(f'{TEAM_ID}/targets.csv')\n",
    "\n",
    "if df_targets is None:\n",
    "    raise ValueError('targets.csv is required. Run a backtest with target-state logging first.')\n",
    "\n",
    "df_targets['date'] = pd.to_datetime(df_targets['date'])\n",
    "numeric_cols = ['start_w', 'weekly_target_w', 'scheduled_fraction', 'scheduled_w', 'actual_w', 'scale_day']\n",
    "for col in numeric_cols:\n",
    "    if col in df_targets.columns:\n",
    "        df_targets[col] = pd.to_numeric(df_targets[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "print(f'target rows: {len(df_targets):,}')\n",
    "display(df_targets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_targets.copy()\n",
    "\n",
    "# Keep canonical sort for week/day calculations.\n",
    "df = df.sort_values(['week_id', 'symbol', 'date'])\n",
    "\n",
    "# Weekly order size and remaining amount (same-day observed)\n",
    "df['weekly_order_abs'] = (df['weekly_target_w'] - df['start_w']).abs()\n",
    "df['remaining_abs'] = (df['weekly_target_w'] - df['actual_w']).abs()\n",
    "df['remaining_abs'] = np.minimum(df['remaining_abs'], df['weekly_order_abs'])\n",
    "\n",
    "# T+1 settled proxy: use next trading day's actual weight WITHIN THE SAME WEEK_ID.\n",
    "# This avoids cross-cycle contamination on the final day of a rebalance cycle.\n",
    "df['actual_w_settled_t1'] = (\n",
    "    df.groupby(['week_id', 'symbol'])['actual_w']\n",
    "      .shift(-1)\n",
    "      .fillna(df['actual_w'])\n",
    ")\n",
    "\n",
    "# Same-day incremental/cumulative fill.\n",
    "df['prior_remaining_abs'] = df.groupby(['week_id', 'symbol'])['remaining_abs'].shift(1)\n",
    "df['prior_remaining_abs'] = df['prior_remaining_abs'].fillna(df['weekly_order_abs'])\n",
    "df['incremental_fill_abs'] = (df['prior_remaining_abs'] - df['remaining_abs']).clip(lower=0)\n",
    "\n",
    "df['incremental_fill_pct'] = np.where(\n",
    "    df['weekly_order_abs'] > 1e-10,\n",
    "    df['incremental_fill_abs'] / df['weekly_order_abs'],\n",
    "    0.0\n",
    ")\n",
    "\n",
    "df['cumulative_fill_pct'] = np.where(\n",
    "    df['weekly_order_abs'] > 1e-10,\n",
    "    (df['weekly_order_abs'] - df['remaining_abs']) / df['weekly_order_abs'],\n",
    "    1.0\n",
    ")\n",
    "\n",
    "# T+1 settled incremental/cumulative fill.\n",
    "df['remaining_abs_settled_t1'] = (df['weekly_target_w'] - df['actual_w_settled_t1']).abs()\n",
    "df['remaining_abs_settled_t1'] = np.minimum(df['remaining_abs_settled_t1'], df['weekly_order_abs'])\n",
    "\n",
    "df['prior_remaining_abs_settled_t1'] = df.groupby(['week_id', 'symbol'])['remaining_abs_settled_t1'].shift(1)\n",
    "df['prior_remaining_abs_settled_t1'] = df['prior_remaining_abs_settled_t1'].fillna(df['weekly_order_abs'])\n",
    "df['incremental_fill_abs_settled_t1'] = (\n",
    "    df['prior_remaining_abs_settled_t1'] - df['remaining_abs_settled_t1']\n",
    ").clip(lower=0)\n",
    "\n",
    "df['incremental_fill_pct_settled_t1'] = np.where(\n",
    "    df['weekly_order_abs'] > 1e-10,\n",
    "    df['incremental_fill_abs_settled_t1'] / df['weekly_order_abs'],\n",
    "    0.0\n",
    ")\n",
    "\n",
    "df['cumulative_fill_pct_settled_t1'] = np.where(\n",
    "    df['weekly_order_abs'] > 1e-10,\n",
    "    (df['weekly_order_abs'] - df['remaining_abs_settled_t1']) / df['weekly_order_abs'],\n",
    "    1.0\n",
    ")\n",
    "\n",
    "# Clip to [0,1] for stable plotting.\n",
    "for col in [\n",
    "    'incremental_fill_pct',\n",
    "    'cumulative_fill_pct',\n",
    "    'incremental_fill_pct_settled_t1',\n",
    "    'cumulative_fill_pct_settled_t1'\n",
    "]:\n",
    "    df[col] = df[col].clip(0, 1)\n",
    "\n",
    "df['weekday'] = df['date'].dt.day_name()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "df['weekday'] = pd.Categorical(df['weekday'], categories=weekday_order, ordered=True)\n",
    "\n",
    "df = df[df['weekly_order_abs'] > 1e-10].copy()\n",
    "\n",
    "display(\n",
    "    df[[\n",
    "        'date', 'week_id', 'symbol', 'scale_day',\n",
    "        'incremental_fill_pct', 'cumulative_fill_pct',\n",
    "        'incremental_fill_pct_settled_t1', 'cumulative_fill_pct_settled_t1'\n",
    "    ]].head(15)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio-level daily fill aggressiveness (weighted by order size)\n",
    "by_date = (\n",
    "    df.groupby('date', as_index=False)\n",
    "      .agg(\n",
    "          incremental_fill_abs=('incremental_fill_abs', 'sum'),\n",
    "          incremental_fill_abs_settled_t1=('incremental_fill_abs_settled_t1', 'sum'),\n",
    "          weekly_order_abs=('weekly_order_abs', 'sum')\n",
    "      )\n",
    ")\n",
    "by_date['incremental_fill_pct'] = np.where(\n",
    "    by_date['weekly_order_abs'] > 1e-10,\n",
    "    by_date['incremental_fill_abs'] / by_date['weekly_order_abs'],\n",
    "    0.0\n",
    ")\n",
    "by_date['incremental_fill_pct_settled_t1'] = np.where(\n",
    "    by_date['weekly_order_abs'] > 1e-10,\n",
    "    by_date['incremental_fill_abs_settled_t1'] / by_date['weekly_order_abs'],\n",
    "    0.0\n",
    ")\n",
    "by_date['weekday'] = pd.Categorical(by_date['date'].dt.day_name(), categories=weekday_order, ordered=True)\n",
    "\n",
    "weekday_weighted = (\n",
    "    by_date.groupby('weekday', as_index=False)\n",
    "           .agg(\n",
    "               avg_incremental_fill_pct=('incremental_fill_pct', 'mean'),\n",
    "               avg_incremental_fill_pct_settled_t1=('incremental_fill_pct_settled_t1', 'mean')\n",
    "           )\n",
    ")\n",
    "\n",
    "# Cumulative fill by weekday: only use complete 5-day symbol-weeks to avoid\n",
    "# Simpson's paradox (changing composition across weekdays).\n",
    "weekday_counts = df.groupby(['week_id', 'symbol'])['weekday'].nunique()\n",
    "complete_pairs = weekday_counts[weekday_counts == 5].reset_index()[['week_id', 'symbol']]\n",
    "df_complete = df.merge(complete_pairs, on=['week_id', 'symbol'], how='inner')\n",
    "\n",
    "weekday_cumulative = (\n",
    "    df_complete.groupby('weekday', as_index=False)\n",
    "               .agg(\n",
    "                   avg_cumulative_fill_pct=('cumulative_fill_pct', 'mean'),\n",
    "                   avg_cumulative_fill_pct_settled_t1=('cumulative_fill_pct_settled_t1', 'mean'),\n",
    "                   n_obs=('cumulative_fill_pct', 'count')\n",
    "               )\n",
    ")\n",
    "\n",
    "weekday_symbol_mean = (\n",
    "    df.groupby('weekday', as_index=False)\n",
    "      .agg(\n",
    "          avg_incremental_fill_pct=('incremental_fill_pct', 'mean'),\n",
    "          avg_incremental_fill_pct_settled_t1=('incremental_fill_pct_settled_t1', 'mean'),\n",
    "          med_incremental_fill_pct=('incremental_fill_pct', 'median'),\n",
    "          avg_cumulative_fill_pct=('cumulative_fill_pct', 'mean'),\n",
    "          avg_cumulative_fill_pct_settled_t1=('cumulative_fill_pct_settled_t1', 'mean')\n",
    "      )\n",
    ")\n",
    "\n",
    "print(f'Complete 5-day symbol-weeks: {len(complete_pairs):,} '\n",
    "      f'(of {len(weekday_counts):,} total)')\n",
    "display(weekday_weighted)\n",
    "display(weekday_cumulative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "weekday_plot = weekday_weighted.copy()\n",
    "weekday_plot['same_day_pct'] = 100 * weekday_plot['avg_incremental_fill_pct']\n",
    "weekday_plot['settled_t1_pct'] = 100 * weekday_plot['avg_incremental_fill_pct_settled_t1']\n",
    "weekday_long = weekday_plot.melt(\n",
    "    id_vars='weekday',\n",
    "    value_vars=['same_day_pct', 'settled_t1_pct'],\n",
    "    var_name='series',\n",
    "    value_name='value'\n",
    ")\n",
    "weekday_long['series'] = weekday_long['series'].map({\n",
    "    'same_day_pct': 'Same-day observed',\n",
    "    'settled_t1_pct': 'T+1 settled proxy'\n",
    "})\n",
    "\n",
    "sns.barplot(\n",
    "    data=weekday_long,\n",
    "    x='weekday',\n",
    "    y='value',\n",
    "    hue='series',\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Weighted Average % Filled Per Day (Calendar Weekday)')\n",
    "axes[0].set_ylabel('% of weekly order filled on that day')\n",
    "axes[0].set_xlabel('Weekday')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Use complete 5-day weeks only so cumulative is guaranteed monotonic.\n",
    "axes[1].plot(\n",
    "    weekday_cumulative['weekday'],\n",
    "    100 * weekday_cumulative['avg_cumulative_fill_pct'],\n",
    "    marker='o',\n",
    "    color='#d62728',\n",
    "    label='Same-day observed'\n",
    ")\n",
    "axes[1].plot(\n",
    "    weekday_cumulative['weekday'],\n",
    "    100 * weekday_cumulative['avg_cumulative_fill_pct_settled_t1'],\n",
    "    marker='o',\n",
    "    color='#1f77b4',\n",
    "    label='T+1 settled proxy'\n",
    ")\n",
    "axes[1].set_title('Average Cumulative Fill % By Weekday (Complete Weeks Only)')\n",
    "axes[1].set_ylabel('Cumulative fill %')\n",
    "axes[1].set_xlabel('Weekday')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_day_profile = (\n",
    "    df.groupby('scale_day', as_index=False)\n",
    "      .agg(\n",
    "          avg_incremental_fill_pct=('incremental_fill_pct', 'mean'),\n",
    "          avg_incremental_fill_pct_settled_t1=('incremental_fill_pct_settled_t1', 'mean'),\n",
    "          med_incremental_fill_pct=('incremental_fill_pct', 'median'),\n",
    "          avg_cumulative_fill_pct=('cumulative_fill_pct', 'mean'),\n",
    "          avg_cumulative_fill_pct_settled_t1=('cumulative_fill_pct_settled_t1', 'mean')\n",
    "      )\n",
    "      .sort_values('scale_day')\n",
    ")\n",
    "\n",
    "display(scale_day_profile)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "x = scale_day_profile['scale_day'].to_numpy()\n",
    "width = 0.38\n",
    "axes[0].bar(x - width/2, 100 * scale_day_profile['avg_incremental_fill_pct'], width=width, color='#2ca02c', label='Same-day observed')\n",
    "axes[0].bar(x + width/2, 100 * scale_day_profile['avg_incremental_fill_pct_settled_t1'], width=width, color='#1f77b4', label='T+1 settled proxy')\n",
    "axes[0].set_title('Average % Filled Per Scale Day')\n",
    "axes[0].set_xlabel('Scale day')\n",
    "axes[0].set_ylabel('% of weekly order filled that day')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].plot(scale_day_profile['scale_day'], 100 * scale_day_profile['avg_cumulative_fill_pct'], marker='o', color='#9467bd', label='Same-day observed')\n",
    "axes[1].plot(scale_day_profile['scale_day'], 100 * scale_day_profile['avg_cumulative_fill_pct_settled_t1'], marker='o', color='#ff7f0e', label='T+1 settled proxy')\n",
    "axes[1].set_title('Average Cumulative Fill % by Scale Day')\n",
    "axes[1].set_xlabel('Scale day')\n",
    "axes[1].set_ylabel('Cumulative fill %')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: symbol-level ranking of aggressiveness on day 0 and day 1\n",
    "day_early = df[df['scale_day'].isin([0, 1])]\n",
    "symbol_early = (\n",
    "    day_early.groupby('symbol', as_index=False)\n",
    "             .agg(avg_fill_day0_1=('incremental_fill_pct', 'mean'),\n",
    "                  observations=('incremental_fill_pct', 'count'))\n",
    "             .sort_values('avg_fill_day0_1', ascending=False)\n",
    ")\n",
    "\n",
    "print('Most aggressive early fills (day 0-1):')\n",
    "display(symbol_early.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle Carryover and Convergence (By Signal Strength)\n",
    "\n",
    "This section tracks the per-cycle completion dynamic you described:\n",
    "- cycle start gap = `|weekly_target_w - start_w|`\n",
    "- cycle end gap = `|weekly_target_w - week_end_actual_w|`\n",
    "- cycle fill % = `(start gap - end gap) / start gap`\n",
    "- carryover % = `1 - cycle fill %`\n",
    "\n",
    "Signal strength/tier is attached from `order_events.csv` order tags when available, with fallback to `signals.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build weekly diagnostics from the per-day target-state table.\n",
    "weekly_diag = (\n",
    "    df.sort_values(['week_id', 'symbol', 'date'])\n",
    "      .groupby(['week_id', 'symbol'], as_index=False)\n",
    "      .agg(\n",
    "          week_start=('date', 'min'),\n",
    "          week_end=('date', 'max'),\n",
    "          start_w=('start_w', 'first'),\n",
    "          weekly_target_w=('weekly_target_w', 'first'),\n",
    "          week_end_actual_w=('actual_w', 'last'),\n",
    "          max_scale_day=('scale_day', 'max')\n",
    "      )\n",
    ")\n",
    "\n",
    "EPS = 1e-10\n",
    "SCALE_DAYS_EXPECTED = 5\n",
    "TARGET_STABILITY_TOL = 0.0025  # 25 bps target-change tolerance for episode stitching\n",
    "\n",
    "weekly_diag['gap_start_abs'] = (weekly_diag['weekly_target_w'] - weekly_diag['start_w']).abs()\n",
    "weekly_diag['gap_end_abs'] = (weekly_diag['weekly_target_w'] - weekly_diag['week_end_actual_w']).abs()\n",
    "weekly_diag['filled_abs'] = (weekly_diag['gap_start_abs'] - weekly_diag['gap_end_abs']).clip(lower=0)\n",
    "\n",
    "weekly_diag['cycle_fill_pct'] = np.where(\n",
    "    weekly_diag['gap_start_abs'] > EPS,\n",
    "    weekly_diag['filled_abs'] / weekly_diag['gap_start_abs'],\n",
    "    1.0\n",
    ").clip(0, 1)\n",
    "weekly_diag['carryover_pct'] = (1.0 - weekly_diag['cycle_fill_pct']).clip(0, 1)\n",
    "\n",
    "# Only compare complete 5-scale-day cycles for fairness.\n",
    "weekly_diag['full_scale_week'] = weekly_diag['max_scale_day'] >= (SCALE_DAYS_EXPECTED - 1)\n",
    "\n",
    "# Intent classification for cleaner comparisons.\n",
    "start_abs = weekly_diag['start_w'].abs()\n",
    "target_abs = weekly_diag['weekly_target_w'].abs()\n",
    "flip = (\n",
    "    (start_abs > EPS) & (target_abs > EPS) &\n",
    "    (np.sign(weekly_diag['start_w']) != np.sign(weekly_diag['weekly_target_w']))\n",
    ")\n",
    "exit_to_zero = (start_abs > EPS) & (target_abs <= EPS)\n",
    "entry_new = (start_abs <= EPS) & (target_abs > EPS)\n",
    "increase = (~flip) & (~entry_new) & (target_abs > start_abs + EPS)\n",
    "decrease = (~flip) & (~exit_to_zero) & (target_abs + EPS < start_abs)\n",
    "\n",
    "weekly_diag['intent_bucket'] = np.select(\n",
    "    [flip, exit_to_zero, entry_new, increase, decrease],\n",
    "    ['flip', 'exit_to_zero', 'entry_new', 'increase', 'decrease'],\n",
    "    default='flat_or_small_change'\n",
    ")\n",
    "\n",
    "\n",
    "def parse_order_tag(tag):\n",
    "    parsed = {}\n",
    "    if pd.isna(tag):\n",
    "        return parsed\n",
    "    for part in str(tag).split(';'):\n",
    "        if '=' not in part:\n",
    "            continue\n",
    "        k, v = part.split('=', 1)\n",
    "        parsed[k.strip()] = v.strip()\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def tier_from_strength(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if x >= 0.70:\n",
    "        return 'strong'\n",
    "    if x >= 0.30:\n",
    "        return 'moderate'\n",
    "    return 'weak'\n",
    "\n",
    "\n",
    "def choose_tier(series):\n",
    "    s = series.dropna().astype(str)\n",
    "    s = s[s != '']\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    non_exit = s[s != 'exit']\n",
    "    if not non_exit.empty:\n",
    "        return non_exit.mode().iloc[0]\n",
    "    if (s == 'exit').any():\n",
    "        return 'exit'\n",
    "    return s.mode().iloc[0]\n",
    "\n",
    "\n",
    "# 1) Primary tier/strength mapping from order-event tags.\n",
    "week_signal_order = None\n",
    "df_order_events = read_csv_from_store(f'{TEAM_ID}/order_events.csv')\n",
    "if df_order_events is not None and 'tag' in df_order_events.columns:\n",
    "    oe = df_order_events.copy()\n",
    "    oe['symbol'] = oe['symbol'].astype(str)\n",
    "\n",
    "    if 'date' in oe.columns:\n",
    "        oe['date'] = pd.to_datetime(oe['date'], errors='coerce')\n",
    "\n",
    "    tag_map = oe['tag'].apply(parse_order_tag)\n",
    "    oe['week_id'] = tag_map.apply(lambda x: x.get('week_id', ''))\n",
    "    oe['signal_tier'] = tag_map.apply(lambda x: x.get('tier', ''))\n",
    "    oe['signal_strength'] = pd.to_numeric(\n",
    "        tag_map.apply(lambda x: x.get('signal', np.nan)),\n",
    "        errors='coerce'\n",
    "    ).abs()\n",
    "\n",
    "    oe = oe[(oe['week_id'].notna()) & (oe['week_id'] != '')]\n",
    "\n",
    "    # Collapse repeated status rows per order id, keep last event row.\n",
    "    if 'order_id' in oe.columns:\n",
    "        sort_cols = ['order_id'] + (['date'] if 'date' in oe.columns else [])\n",
    "        oe = oe.sort_values(sort_cols).drop_duplicates(subset=['order_id'], keep='last')\n",
    "\n",
    "    if not oe.empty:\n",
    "        week_signal_order = (\n",
    "            oe.groupby(['week_id', 'symbol'], as_index=False)\n",
    "              .agg(\n",
    "                  signal_strength=('signal_strength', 'median'),\n",
    "                  signal_tier=('signal_tier', choose_tier)\n",
    "              )\n",
    "        )\n",
    "\n",
    "# 2) Fallback mapping from signals.csv magnitude.\n",
    "week_signal_fallback = None\n",
    "df_signals = read_csv_from_store(f'{TEAM_ID}/signals.csv')\n",
    "if df_signals is not None and {'date', 'symbol', 'magnitude'}.issubset(df_signals.columns):\n",
    "    sig = df_signals.copy()\n",
    "    sig['date'] = pd.to_datetime(sig['date'], errors='coerce')\n",
    "    sig = sig.dropna(subset=['date'])\n",
    "    sig['week_id'] = sig['date'].dt.strftime('%Y-%m-%d')\n",
    "    sig['symbol'] = sig['symbol'].astype(str)\n",
    "    sig['signal_strength'] = pd.to_numeric(sig['magnitude'], errors='coerce').abs()\n",
    "\n",
    "    week_signal_fallback = (\n",
    "        sig.groupby(['week_id', 'symbol'], as_index=False)\n",
    "           .agg(signal_strength=('signal_strength', 'max'))\n",
    "    )\n",
    "    week_signal_fallback['signal_tier'] = week_signal_fallback['signal_strength'].apply(tier_from_strength)\n",
    "\n",
    "# Merge mappings with order-event precedence.\n",
    "if week_signal_order is None:\n",
    "    week_signal_order = pd.DataFrame(columns=['week_id', 'symbol', 'signal_strength', 'signal_tier'])\n",
    "if week_signal_fallback is None:\n",
    "    week_signal_fallback = pd.DataFrame(columns=['week_id', 'symbol', 'signal_strength', 'signal_tier'])\n",
    "\n",
    "week_signal = week_signal_order.merge(\n",
    "    week_signal_fallback,\n",
    "    on=['week_id', 'symbol'],\n",
    "    how='outer',\n",
    "    suffixes=('_order', '_fallback')\n",
    ")\n",
    "\n",
    "week_signal['signal_strength'] = week_signal['signal_strength_order'].fillna(week_signal['signal_strength_fallback'])\n",
    "week_signal['signal_tier'] = week_signal['signal_tier_order'].replace('', np.nan)\n",
    "week_signal['signal_tier'] = week_signal['signal_tier'].fillna(week_signal['signal_tier_fallback'])\n",
    "week_signal['signal_tier'] = week_signal['signal_tier'].fillna(week_signal['signal_strength'].apply(tier_from_strength))\n",
    "week_signal = week_signal[['week_id', 'symbol', 'signal_strength', 'signal_tier']].drop_duplicates(['week_id', 'symbol'], keep='last')\n",
    "\n",
    "weekly_diag = weekly_diag.merge(week_signal, on=['week_id', 'symbol'], how='left')\n",
    "weekly_diag['signal_tier_raw'] = weekly_diag['signal_tier'].fillna('unknown')\n",
    "weekly_diag['signal_tier'] = weekly_diag['signal_tier_raw']\n",
    "\n",
    "# Cleanup unknowns:\n",
    "# Unknowns are typically pure exit weeks (target=0) with no active signal emitted/tagged.\n",
    "mask_unknown_exit = (\n",
    "    (weekly_diag['signal_tier'] == 'unknown') &\n",
    "    (weekly_diag['intent_bucket'] == 'exit_to_zero')\n",
    ")\n",
    "weekly_diag.loc[mask_unknown_exit, 'signal_tier'] = 'exit'\n",
    "weekly_diag.loc[mask_unknown_exit, 'signal_strength'] = weekly_diag.loc[mask_unknown_exit, 'signal_strength'].fillna(0.0)\n",
    "\n",
    "# Track prior non-exit tier so exits can be attributed to the tier being unwound.\n",
    "weekly_diag = weekly_diag.sort_values(['symbol', 'week_start']).reset_index(drop=True)\n",
    "weekly_diag['prior_non_exit_tier'] = (\n",
    "    weekly_diag['signal_tier']\n",
    "      .where(weekly_diag['signal_tier'].isin(['strong', 'moderate', 'weak']))\n",
    "      .groupby(weekly_diag['symbol'])\n",
    "      .ffill()\n",
    ")\n",
    "weekly_diag['exit_from_tier'] = np.where(\n",
    "    weekly_diag['signal_tier'] == 'exit',\n",
    "    weekly_diag['prior_non_exit_tier'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Effective tier for optional attribution of exits to prior tier.\n",
    "weekly_diag['signal_tier_effective'] = weekly_diag['signal_tier']\n",
    "mask_exit_with_parent = (\n",
    "    (weekly_diag['signal_tier'] == 'exit') &\n",
    "    (weekly_diag['exit_from_tier'].notna())\n",
    ")\n",
    "weekly_diag.loc[mask_exit_with_parent, 'signal_tier_effective'] = weekly_diag.loc[mask_exit_with_parent, 'exit_from_tier']\n",
    "\n",
    "# Symbol-week sequence metrics for reset diagnostics.\n",
    "weekly_diag['prev_target_w'] = weekly_diag.groupby('symbol')['weekly_target_w'].shift(1)\n",
    "weekly_diag['target_shift_abs'] = (weekly_diag['weekly_target_w'] - weekly_diag['prev_target_w']).abs()\n",
    "weekly_diag['prev_carryover_pct'] = weekly_diag.groupby('symbol')['carryover_pct'].shift(1)\n",
    "\n",
    "weekly_diag['same_target_prev'] = weekly_diag['target_shift_abs'] <= TARGET_STABILITY_TOL\n",
    "weekly_diag['prev_tier'] = weekly_diag.groupby('symbol')['signal_tier'].shift(1)\n",
    "weekly_diag['same_tier_prev'] = weekly_diag['signal_tier'].eq(weekly_diag['prev_tier'])\n",
    "\n",
    "weekly_diag['new_episode'] = (\n",
    "    weekly_diag['prev_target_w'].isna() |\n",
    "    (~weekly_diag['same_target_prev']) |\n",
    "    (~weekly_diag['same_tier_prev'])\n",
    ")\n",
    "weekly_diag['episode_id'] = weekly_diag.groupby('symbol')['new_episode'].cumsum()\n",
    "weekly_diag['week_in_episode'] = weekly_diag.groupby(['symbol', 'episode_id']).cumcount() + 1\n",
    "\n",
    "weekly_diag['target_reset_with_carryover'] = (\n",
    "    (weekly_diag['target_shift_abs'] > TARGET_STABILITY_TOL) &\n",
    "    (weekly_diag['prev_carryover_pct'].fillna(0) > 0.20)\n",
    ").astype(int)\n",
    "\n",
    "# Attach cycle-level diagnostics back to daily rows for tier-aware day curves.\n",
    "for col in [\n",
    "    'signal_strength', 'signal_tier', 'signal_tier_raw', 'signal_tier_effective',\n",
    "    'exit_from_tier', 'full_scale_week', 'intent_bucket'\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "df = df.merge(\n",
    "    weekly_diag[[\n",
    "        'week_id', 'symbol', 'signal_strength', 'signal_tier', 'signal_tier_raw',\n",
    "        'signal_tier_effective', 'exit_from_tier', 'full_scale_week', 'intent_bucket'\n",
    "    ]],\n",
    "    on=['week_id', 'symbol'],\n",
    "    how='left'\n",
    ")\n",
    "df['signal_tier'] = df['signal_tier'].fillna('unknown')\n",
    "\n",
    "tier_counts_raw = weekly_diag.groupby('signal_tier_raw', as_index=False).size().sort_values('size', ascending=False)\n",
    "tier_counts_clean = weekly_diag.groupby('signal_tier', as_index=False).size().sort_values('size', ascending=False)\n",
    "exit_attribution = (\n",
    "    weekly_diag[weekly_diag['signal_tier'] == 'exit']\n",
    "    .groupby('exit_from_tier', as_index=False)\n",
    "    .size()\n",
    "    .sort_values('size', ascending=False)\n",
    ")\n",
    "\n",
    "print(f'weekly rows: {len(weekly_diag):,}')\n",
    "print('tier counts (raw):')\n",
    "display(tier_counts_raw)\n",
    "print('tier counts (cleaned):')\n",
    "display(tier_counts_clean)\n",
    "print('exit attribution (prior tier):')\n",
    "display(exit_attribution)\n",
    "\n",
    "display(\n",
    "    weekly_diag[[\n",
    "        'week_id', 'symbol', 'signal_tier_raw', 'signal_tier', 'signal_tier_effective',\n",
    "        'exit_from_tier', 'signal_strength', 'intent_bucket', 'full_scale_week',\n",
    "        'start_w', 'weekly_target_w', 'gap_start_abs', 'gap_end_abs',\n",
    "        'cycle_fill_pct', 'carryover_pct', 'target_shift_abs',\n",
    "        'target_reset_with_carryover', 'week_in_episode'\n",
    "    ]].head(20)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal-Tier Carryover Dashboards\n",
    "\n",
    "These views monitor whether positions are converging to target or staying underweight due to carryover + target resets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_order = ['strong', 'moderate', 'weak']\n",
    "\n",
    "\n",
    "def summarize_cycles(frame, label):\n",
    "    rows = []\n",
    "    for tier in tier_order:\n",
    "        g = frame[frame['signal_tier'] == tier]\n",
    "        if g.empty:\n",
    "            continue\n",
    "        gap_sum = g['gap_start_abs'].sum()\n",
    "        filled_sum = g['filled_abs'].sum()\n",
    "        weighted_fill = (filled_sum / gap_sum) if gap_sum > 1e-10 else np.nan\n",
    "        rows.append({\n",
    "            'view': label,\n",
    "            'signal_tier': tier,\n",
    "            'weeks': int(len(g)),\n",
    "            'avg_start_gap_pct': 100 * g['gap_start_abs'].mean(),\n",
    "            'avg_cycle_fill_pct': 100 * g['cycle_fill_pct'].mean(),\n",
    "            'weighted_cycle_fill_pct': 100 * weighted_fill if pd.notna(weighted_fill) else np.nan,\n",
    "            'avg_carryover_pct': 100 * g['carryover_pct'].mean(),\n",
    "            'weighted_carryover_pct': 100 * (1 - weighted_fill) if pd.notna(weighted_fill) else np.nan,\n",
    "            'reset_while_unfilled_rate': 100 * g['target_reset_with_carryover'].mean()\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# View A: all non-exit tiered cycles on complete scale windows.\n",
    "view_a = weekly_diag[\n",
    "    weekly_diag['full_scale_week'] &\n",
    "    weekly_diag['signal_tier'].isin(tier_order)\n",
    "].copy()\n",
    "\n",
    "# View B: entry/add cycles only (closest to \"underweight relative to target\" concern).\n",
    "view_b = weekly_diag[\n",
    "    weekly_diag['full_scale_week'] &\n",
    "    weekly_diag['signal_tier'].isin(tier_order) &\n",
    "    weekly_diag['intent_bucket'].isin(['entry_new', 'increase'])\n",
    "].copy()\n",
    "\n",
    "cycle_summary = pd.concat([\n",
    "    summarize_cycles(view_a, 'All Non-Exit Cycles (Full Weeks)'),\n",
    "    summarize_cycles(view_b, 'Entry/Add Cycles Only (Full Weeks)')\n",
    "], ignore_index=True)\n",
    "\n",
    "display(cycle_summary)\n",
    "\n",
    "# Day-by-day cumulative fill profile by tier for entry/add cycles.\n",
    "df_plot = df[\n",
    "    df['full_scale_week'].fillna(False) &\n",
    "    df['signal_tier'].isin(tier_order) &\n",
    "    df['intent_bucket'].isin(['entry_new', 'increase'])\n",
    "].copy()\n",
    "\n",
    "scale_tier_profile = (\n",
    "    df_plot.groupby(['signal_tier', 'scale_day'], as_index=False)\n",
    "          .agg(\n",
    "              avg_incremental_fill_pct=('incremental_fill_pct', 'mean'),\n",
    "              avg_cumulative_fill_pct=('cumulative_fill_pct', 'mean'),\n",
    "              observations=('symbol', 'count')\n",
    "          )\n",
    "          .sort_values(['signal_tier', 'scale_day'])\n",
    ")\n",
    "\n",
    "display(scale_tier_profile)\n",
    "\n",
    "# Episode-level convergence profile for stable-target episodes.\n",
    "episode_len = (\n",
    "    weekly_diag.groupby(['symbol', 'episode_id'], as_index=False)\n",
    "               .size()\n",
    "               .rename(columns={'size': 'episode_len'})\n",
    ")\n",
    "weekly_episode = weekly_diag.merge(episode_len, on=['symbol', 'episode_id'], how='left')\n",
    "weekly_episode = weekly_episode[\n",
    "    (weekly_episode['episode_len'] >= 2) &\n",
    "    weekly_episode['signal_tier'].isin(tier_order) &\n",
    "    weekly_episode['intent_bucket'].isin(['entry_new', 'increase'])\n",
    "].copy()\n",
    "\n",
    "episode_profile = (\n",
    "    weekly_episode.groupby(['signal_tier', 'week_in_episode'], as_index=False)\n",
    "                 .agg(\n",
    "                     avg_gap_start_abs=('gap_start_abs', 'mean'),\n",
    "                     avg_gap_end_abs=('gap_end_abs', 'mean'),\n",
    "                     avg_cycle_fill_pct=('cycle_fill_pct', 'mean'),\n",
    "                     observations=('week_id', 'count')\n",
    "                 )\n",
    "                 .sort_values(['signal_tier', 'week_in_episode'])\n",
    ")\n",
    "\n",
    "episode_profile['avg_gap_start_pct'] = 100 * episode_profile['avg_gap_start_abs']\n",
    "episode_profile['avg_gap_end_pct'] = 100 * episode_profile['avg_gap_end_abs']\n",
    "episode_profile['avg_carryover_pct'] = np.where(\n",
    "    episode_profile['avg_gap_start_abs'] > 1e-10,\n",
    "    100 * episode_profile['avg_gap_end_abs'] / episode_profile['avg_gap_start_abs'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "display(episode_profile)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "\n",
    "# 1) Cycle completion by tier (entry/add view).\n",
    "bar_data = cycle_summary[cycle_summary['view'] == 'Entry/Add Cycles Only (Full Weeks)'].copy()\n",
    "bar_data['signal_tier'] = pd.Categorical(bar_data['signal_tier'], categories=tier_order, ordered=True)\n",
    "bar_data = bar_data.sort_values('signal_tier')\n",
    "\n",
    "x = np.arange(len(bar_data))\n",
    "width = 0.38\n",
    "axes[0, 0].bar(x - width/2, bar_data['avg_cycle_fill_pct'], width=width, color='#2ca02c', label='Unweighted avg')\n",
    "axes[0, 0].bar(x + width/2, bar_data['weighted_cycle_fill_pct'], width=width, color='#1f77b4', label='Gap-weighted')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(bar_data['signal_tier'].astype(str))\n",
    "axes[0, 0].set_title('Average 5-Day Cycle Fill % by Tier (Entry/Add)')\n",
    "axes[0, 0].set_ylabel('Cycle fill %')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2) Cumulative fill by scale day and tier.\n",
    "for tier in tier_order:\n",
    "    sub = scale_tier_profile[scale_tier_profile['signal_tier'] == tier]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    axes[0, 1].plot(sub['scale_day'], 100 * sub['avg_cumulative_fill_pct'], marker='o', label=tier)\n",
    "axes[0, 1].set_title('Cumulative Fill % by Scale Day and Tier (Entry/Add)')\n",
    "axes[0, 1].set_xlabel('Scale day')\n",
    "axes[0, 1].set_ylabel('Cumulative fill %')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].legend(loc='best')\n",
    "\n",
    "# 3) Start-gap decay across stable-target episodes.\n",
    "for tier in tier_order:\n",
    "    sub = episode_profile[episode_profile['signal_tier'] == tier]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    axes[1, 0].plot(sub['week_in_episode'], sub['avg_gap_start_pct'], marker='o', label=tier)\n",
    "axes[1, 0].set_title('Start Gap % by Week-in-Episode (Stable Target)')\n",
    "axes[1, 0].set_xlabel('Week in episode')\n",
    "axes[1, 0].set_ylabel('Start gap (% NAV)')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4) Carryover % across stable-target episodes.\n",
    "for tier in tier_order:\n",
    "    sub = episode_profile[episode_profile['signal_tier'] == tier]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    axes[1, 1].plot(sub['week_in_episode'], sub['avg_carryover_pct'], marker='o', label=tier)\n",
    "axes[1, 1].set_title('Carryover % by Week-in-Episode (Stable Target)')\n",
    "axes[1, 1].set_xlabel('Week in episode')\n",
    "axes[1, 1].set_ylabel('Carryover %')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Convergence Table (Closest to Your Weak-Signal Scenario)\n",
    "\n",
    "This auto-selects a long stable-target episode (weak tier preferred) and prints week-by-week convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_episode = weekly_diag.copy()\n",
    "\n",
    "episode_len = (\n",
    "    weekly_episode.groupby(['symbol', 'episode_id'], as_index=False)\n",
    "                 .size()\n",
    "                 .rename(columns={'size': 'episode_len'})\n",
    ")\n",
    "weekly_episode = weekly_episode.merge(episode_len, on=['symbol', 'episode_id'], how='left')\n",
    "\n",
    "candidates = weekly_episode[(weekly_episode['signal_tier'] == 'weak') & (weekly_episode['episode_len'] >= 3)].copy()\n",
    "if candidates.empty:\n",
    "    candidates = weekly_episode[weekly_episode['episode_len'] >= 3].copy()\n",
    "\n",
    "if candidates.empty:\n",
    "    print('No episode with length >= 3 found; showing longest available episode.')\n",
    "    longest = (\n",
    "        weekly_episode.groupby(['symbol', 'episode_id'], as_index=False)\n",
    "                     .size()\n",
    "                     .sort_values('size', ascending=False)\n",
    "                     .head(1)\n",
    "    )\n",
    "else:\n",
    "    longest = (\n",
    "        candidates.groupby(['symbol', 'episode_id'], as_index=False)\n",
    "                  .size()\n",
    "                  .sort_values(['size', 'symbol'], ascending=[False, True])\n",
    "                  .head(1)\n",
    "    )\n",
    "\n",
    "if longest.empty:\n",
    "    print('No weekly episodes available.')\n",
    "else:\n",
    "    sel_symbol = longest.iloc[0]['symbol']\n",
    "    sel_episode = longest.iloc[0]['episode_id']\n",
    "\n",
    "    ex = (\n",
    "        weekly_episode[\n",
    "            (weekly_episode['symbol'] == sel_symbol) &\n",
    "            (weekly_episode['episode_id'] == sel_episode)\n",
    "        ]\n",
    "        .sort_values('week_start')\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    ex['start_w_pct'] = 100 * ex['start_w']\n",
    "    ex['weekly_target_w_pct'] = 100 * ex['weekly_target_w']\n",
    "    ex['gap_start_pct'] = 100 * ex['gap_start_abs']\n",
    "    ex['filled_this_cycle_pct_of_gap'] = 100 * ex['cycle_fill_pct']\n",
    "    ex['week_end_actual_w_pct'] = 100 * ex['week_end_actual_w']\n",
    "\n",
    "    print(f'Example symbol: {sel_symbol} | episode_id: {sel_episode} | tier: {ex.iloc[0][\"signal_tier\"]}')\n",
    "\n",
    "    display(\n",
    "        ex[[\n",
    "            'week_id',\n",
    "            'week_in_episode',\n",
    "            'start_w_pct',\n",
    "            'weekly_target_w_pct',\n",
    "            'gap_start_pct',\n",
    "            'filled_this_cycle_pct_of_gap',\n",
    "            'week_end_actual_w_pct'\n",
    "        ]]\n",
    "        .rename(columns={\n",
    "            'week_id': 'Week',\n",
    "            'week_in_episode': 'Week#',\n",
    "            'start_w_pct': 'start_w (%)',\n",
    "            'weekly_target_w_pct': 'weekly_target_w (%)',\n",
    "            'gap_start_pct': 'Gap (%)',\n",
    "            'filled_this_cycle_pct_of_gap': '% gap filled this cycle',\n",
    "            'week_end_actual_w_pct': 'actual_w achieved (%)'\n",
    "        })\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}