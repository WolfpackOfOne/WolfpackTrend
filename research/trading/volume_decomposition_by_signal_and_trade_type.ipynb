{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Volume Decomposition: Signal Tier x Trade Classification\n",
    "\n",
    "This notebook decomposes execution volume by **signal strength** and **trade classification** using:\n",
    "- `{TEAM_ID}/targets.csv` for canonical classification (`NEW_ENTRY`, `RESIZE`, `FLIP`, `EXIT`)\n",
    "- `{TEAM_ID}/order_events.csv` for order lifecycle and fills\n",
    "- `{TEAM_ID}/signals.csv` for rebalance signal magnitude\n",
    "- `{TEAM_ID}/daily_snapshots.csv` for NAV normalization\n",
    "\n",
    "Charts included:\n",
    "1. Filled Notional Heatmap: Signal Tier x Classification\n",
    "2. Order Count & Fill Rate by Tier x Side\n",
    "3. Daily Volume Profile by Signal Tier over Scale Days + theoretical schedules\n",
    "4. Turnover Decomposition Waterfall (stacked) by Classification\n",
    "5. Signal Magnitude vs Filled Notional Scatter + regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 180)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def to_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_tag_value(tag, key):\n",
    "    if pd.isna(tag):\n",
    "        return np.nan\n",
    "    match = re.search(rf'{key}=([^;]+)', str(tag))\n",
    "    return match.group(1).strip() if match else np.nan\n",
    "\n",
    "\n",
    "def tier_from_strength(x):\n",
    "    if pd.isna(x):\n",
    "        return 'unknown'\n",
    "    x = abs(float(x))\n",
    "    if x >= 0.70:\n",
    "        return 'strong'\n",
    "    if x >= 0.30:\n",
    "        return 'moderate'\n",
    "    return 'weak'\n",
    "\n",
    "\n",
    "def normalize_side(direction, quantity):\n",
    "    d = str(direction).strip().lower()\n",
    "    if d in ('buy', 'sell'):\n",
    "        return d\n",
    "    q = pd.to_numeric(quantity, errors='coerce')\n",
    "    if pd.isna(q):\n",
    "        return 'unknown'\n",
    "    return 'buy' if q > 0 else ('sell' if q < 0 else 'unknown')\n",
    "\n",
    "\n",
    "def choose_first_valid(series, default=np.nan):\n",
    "    for x in series:\n",
    "        if pd.notna(x) and str(x).strip() != '':\n",
    "            return x\n",
    "    return default\n",
    "\n",
    "\n",
    "def choose_tier(series):\n",
    "    valid = [str(x).strip().lower() for x in series if pd.notna(x) and str(x).strip() != '']\n",
    "    for tier in ('strong', 'moderate', 'weak', 'exit'):\n",
    "        if tier in valid:\n",
    "            return tier\n",
    "    return 'unknown'\n",
    "\n",
    "\n",
    "def choose_classification(series):\n",
    "    valid = [str(x).strip().upper() for x in series if pd.notna(x) and str(x).strip() != '']\n",
    "    for c in ('NEW_ENTRY', 'RESIZE', 'FLIP', 'EXIT', 'HOLD'):\n",
    "        if c in valid:\n",
    "            return c\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "\n",
    "def build_scaling_schedule(scaling_days, front_load_factor):\n",
    "    n = max(1, int(scaling_days))\n",
    "    if n <= 1:\n",
    "        return [1.0]\n",
    "    exponent = 1.0 / float(front_load_factor)\n",
    "    sched = [round((i / n) ** exponent, 4) for i in range(1, n + 1)]\n",
    "    sched[-1] = 1.0\n",
    "    return sched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ObjectStore datasets\n",
    "\n",
    "df_snap = read_csv_from_store(f'{TEAM_ID}/daily_snapshots.csv')\n",
    "df_targets = read_csv_from_store(f'{TEAM_ID}/targets.csv')\n",
    "df_events = read_csv_from_store(f'{TEAM_ID}/order_events.csv')\n",
    "df_signals = read_csv_from_store(f'{TEAM_ID}/signals.csv')\n",
    "\n",
    "if df_snap is None or df_targets is None or df_events is None:\n",
    "    raise ValueError('daily_snapshots.csv, targets.csv, and order_events.csv are required.')\n",
    "\n",
    "# Snapshots -> NAV by date\n",
    "snap = df_snap.copy()\n",
    "snap['date'] = pd.to_datetime(snap['date']).dt.normalize()\n",
    "snap = to_numeric(snap, ['nav'])\n",
    "nav_by_date = snap[['date', 'nav']].drop_duplicates(['date'])\n",
    "\n",
    "# Targets -> canonical week/symbol classification and scale-day map\n",
    "tgt = df_targets.copy()\n",
    "tgt['date'] = pd.to_datetime(tgt['date']).dt.normalize()\n",
    "num_cols = ['start_w', 'weekly_target_w', 'scheduled_fraction', 'scheduled_w', 'actual_w', 'scale_day']\n",
    "tgt = to_numeric(tgt, num_cols)\n",
    "tgt['scale_day'] = tgt['scale_day'].fillna(0).astype(int)\n",
    "tgt['week_id'] = tgt['week_id'].astype(str)\n",
    "invalid_week = tgt['week_id'].isin(['', 'nan', 'None'])\n",
    "tgt.loc[invalid_week, 'week_id'] = tgt.loc[invalid_week, 'date'].dt.strftime('%Y-%m-%d')\n",
    "tgt['classification'] = tgt['classification'].astype(str).str.upper().str.strip().replace({'': 'UNKNOWN'})\n",
    "\n",
    "week_symbol = (\n",
    "    tgt.sort_values(['week_id', 'symbol', 'date'])\n",
    "       .drop_duplicates(['week_id', 'symbol'], keep='first')\n",
    "       [['week_id', 'symbol', 'date', 'classification', 'start_w', 'weekly_target_w']]\n",
    "       .rename(columns={'date': 'week_start_date'})\n",
    ")\n",
    "\n",
    "# For scale-day join on fills\n",
    "target_daily_map = (\n",
    "    tgt[['date', 'symbol', 'week_id', 'scale_day', 'classification']]\n",
    "      .sort_values(['date', 'symbol'])\n",
    "      .drop_duplicates(['date', 'symbol'], keep='last')\n",
    "      .rename(columns={\n",
    "          'week_id': 'week_id_target',\n",
    "          'scale_day': 'scale_day_target',\n",
    "          'classification': 'classification_target'\n",
    "      })\n",
    ")\n",
    "\n",
    "# Signals -> week/symbol strength + tier\n",
    "if df_signals is not None and len(df_signals):\n",
    "    sig = df_signals.copy()\n",
    "    sig['date'] = pd.to_datetime(sig['date']).dt.normalize()\n",
    "    sig = to_numeric(sig, ['magnitude'])\n",
    "    sig['week_id'] = sig['date'].dt.strftime('%Y-%m-%d')\n",
    "    sig['signal_strength_signal'] = sig['magnitude'].abs()\n",
    "    sig['signal_tier_signal'] = sig['signal_strength_signal'].apply(tier_from_strength)\n",
    "    sig_week = (\n",
    "        sig.sort_values(['week_id', 'symbol', 'date'])\n",
    "           .drop_duplicates(['week_id', 'symbol'], keep='last')\n",
    "           [['week_id', 'symbol', 'signal_strength_signal', 'signal_tier_signal']]\n",
    "    )\n",
    "else:\n",
    "    sig_week = pd.DataFrame(columns=['week_id', 'symbol', 'signal_strength_signal', 'signal_tier_signal'])\n",
    "\n",
    "# Event-tag fallback strength/tier at week/symbol level\n",
    "evt_tmp = df_events.copy()\n",
    "evt_tmp['date'] = pd.to_datetime(evt_tmp['date']).dt.normalize()\n",
    "evt_tmp['week_id_tag'] = evt_tmp['tag'].apply(lambda t: parse_tag_value(t, 'week_id'))\n",
    "evt_tmp['tier_tag'] = evt_tmp['tag'].apply(lambda t: parse_tag_value(t, 'tier'))\n",
    "evt_tmp['signal_tag'] = pd.to_numeric(evt_tmp['tag'].apply(lambda t: parse_tag_value(t, 'signal')), errors='coerce').abs()\n",
    "evt_tmp['week_id_tag'] = evt_tmp['week_id_tag'].fillna(evt_tmp['date'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "tag_week = (\n",
    "    evt_tmp.groupby(['week_id_tag', 'symbol'], as_index=False)\n",
    "           .agg(\n",
    "               signal_strength_tag=('signal_tag', 'max'),\n",
    "               signal_tier_tag=('tier_tag', choose_tier)\n",
    "           )\n",
    "           .rename(columns={'week_id_tag': 'week_id'})\n",
    ")\n",
    "\n",
    "# Build week/symbol lookup with robust signal tier assignment\n",
    "week_symbol = week_symbol.merge(sig_week, on=['week_id', 'symbol'], how='left')\n",
    "week_symbol = week_symbol.merge(tag_week, on=['week_id', 'symbol'], how='left')\n",
    "week_symbol['signal_strength'] = week_symbol['signal_strength_signal'].fillna(week_symbol['signal_strength_tag'])\n",
    "week_symbol['signal_tier'] = week_symbol['signal_tier_signal'].fillna(week_symbol['signal_tier_tag'])\n",
    "week_symbol['signal_tier'] = week_symbol['signal_tier'].fillna('unknown').str.lower().str.strip()\n",
    "\n",
    "# EXIT weeks often have no current signal; attribute to prior known tier per symbol.\n",
    "week_symbol = week_symbol.sort_values(['symbol', 'week_start_date'])\n",
    "week_symbol['prior_known_tier'] = (\n",
    "    week_symbol['signal_tier']\n",
    "      .where(week_symbol['signal_tier'].isin(['strong', 'moderate', 'weak']))\n",
    "      .groupby(week_symbol['symbol'])\n",
    "      .ffill()\n",
    ")\n",
    "mask_exit_missing = (week_symbol['classification'] == 'EXIT') & (~week_symbol['signal_tier'].isin(['strong', 'moderate', 'weak']))\n",
    "week_symbol.loc[mask_exit_missing, 'signal_tier'] = week_symbol.loc[mask_exit_missing, 'prior_known_tier']\n",
    "week_symbol['signal_tier'] = week_symbol['signal_tier'].fillna('unknown')\n",
    "\n",
    "print(f\"Rows -> snapshots: {len(snap):,}, targets: {len(tgt):,}, events: {len(df_events):,}, signals: {0 if df_signals is None else len(df_signals):,}\")\n",
    "print(f\"Week-symbol rows: {len(week_symbol):,}\")\n",
    "display(week_symbol.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich order events and fill rows with week/classification/tier and NAV-normalized notional\n",
    "\n",
    "events = df_events.copy()\n",
    "events['date'] = pd.to_datetime(events['date']).dt.normalize()\n",
    "for c in ['quantity', 'fill_quantity', 'fill_price']:\n",
    "    if c in events.columns:\n",
    "        events[c] = pd.to_numeric(events[c], errors='coerce')\n",
    "\n",
    "events['week_id_tag'] = events['tag'].apply(lambda t: parse_tag_value(t, 'week_id'))\n",
    "events['tier_tag'] = events['tag'].apply(lambda t: parse_tag_value(t, 'tier'))\n",
    "events['signal_tag'] = pd.to_numeric(events['tag'].apply(lambda t: parse_tag_value(t, 'signal')), errors='coerce').abs()\n",
    "events['side'] = [normalize_side(d, q) for d, q in zip(events.get('direction', pd.Series(index=events.index)), events.get('quantity', pd.Series(index=events.index)))]\n",
    "\n",
    "# Attach target day metadata (scale_day + fallback week/classification)\n",
    "events = events.merge(target_daily_map, on=['date', 'symbol'], how='left')\n",
    "events['week_id'] = events['week_id_tag']\n",
    "missing_week = events['week_id'].isna() | events['week_id'].astype(str).str.strip().eq('')\n",
    "events.loc[missing_week, 'week_id'] = events.loc[missing_week, 'week_id_target']\n",
    "events['week_id'] = events['week_id'].fillna(events['date'].dt.strftime('%Y-%m-%d')).astype(str)\n",
    "\n",
    "# Week-level classification + signal fields\n",
    "events = events.merge(\n",
    "    week_symbol[['week_id', 'symbol', 'classification', 'signal_tier', 'signal_strength', 'week_start_date']],\n",
    "    on=['week_id', 'symbol'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "events['classification'] = events['classification'].fillna(events['classification_target'])\n",
    "events['classification'] = events['classification'].fillna('UNKNOWN').astype(str).str.upper().str.strip()\n",
    "\n",
    "# Fallback tier from tag, then from strength\n",
    "events['signal_tier'] = events['signal_tier'].fillna(events['tier_tag'])\n",
    "events['signal_strength'] = events['signal_strength'].fillna(events['signal_tag'])\n",
    "events['signal_tier'] = events['signal_tier'].fillna(events['signal_strength'].apply(tier_from_strength))\n",
    "events['signal_tier'] = events['signal_tier'].fillna('unknown').astype(str).str.lower().str.strip()\n",
    "\n",
    "# Derive fill metrics\n",
    "fills = events.copy()\n",
    "fills['fill_quantity_abs'] = fills['fill_quantity'].abs().fillna(0.0)\n",
    "fills['fill_price_abs'] = fills['fill_price'].abs().fillna(0.0)\n",
    "fills['fill_notional'] = fills['fill_quantity_abs'] * fills['fill_price_abs']\n",
    "fills = fills[fills['fill_notional'] > 0].copy()\n",
    "\n",
    "fills = fills.merge(nav_by_date, on='date', how='left')\n",
    "fills['fill_pct_nav'] = np.where(fills['nav'] > 1e-12, fills['fill_notional'] / fills['nav'], np.nan)\n",
    "\n",
    "fills['scale_day'] = fills['scale_day_target']\n",
    "fills['scale_day'] = pd.to_numeric(fills['scale_day'], errors='coerce')\n",
    "\n",
    "print(f'Fill rows: {len(fills):,}')\n",
    "print(f'Unique order_ids: {events[\"order_id\"].nunique():,}')\n",
    "display(fills[['date', 'week_id', 'symbol', 'signal_tier', 'classification', 'side', 'fill_notional', 'fill_pct_nav', 'scale_day']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Filled Notional Heatmap: Signal Tier x Classification\n",
    "\n",
    "Cell color is **total filled notional as % NAV** aggregated over the full sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_rows = ['strong', 'moderate', 'weak']\n",
    "class_cols = ['NEW_ENTRY', 'RESIZE', 'FLIP', 'EXIT']\n",
    "\n",
    "heat = (\n",
    "    fills[fills['signal_tier'].isin(tier_rows) & fills['classification'].isin(class_cols)]\n",
    "      .groupby(['signal_tier', 'classification'], as_index=False)\n",
    "      .agg(total_fill_pct_nav=('fill_pct_nav', 'sum'))\n",
    ")\n",
    "\n",
    "heat_pivot = (\n",
    "    heat.pivot(index='signal_tier', columns='classification', values='total_fill_pct_nav')\n",
    "        .reindex(index=tier_rows, columns=class_cols)\n",
    "        .fillna(0.0)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(\n",
    "    100 * heat_pivot,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='YlGnBu',\n",
    "    cbar_kws={'label': 'Total Filled Notional (% NAV)'}\n",
    ")\n",
    "plt.title('Filled Notional Heatmap: Signal Tier x Classification')\n",
    "plt.xlabel('Trade Classification')\n",
    "plt.ylabel('Signal Tier')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display((100 * heat_pivot).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Order Count & Fill Rate by Tier x Side\n",
    "\n",
    "`fill_rate` below is the share of orders with **any executed quantity** (`abs(fill_quantity) > 0`) across their lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build order-level summary from lifecycle events\n",
    "order_events = events.copy().reset_index(drop=False).rename(columns={'index': 'event_idx'})\n",
    "order_events = order_events.sort_values(['order_id', 'event_idx'])\n",
    "\n",
    "order_summary = (\n",
    "    order_events.groupby('order_id', as_index=False)\n",
    "               .agg(\n",
    "                   side=('side', lambda s: choose_first_valid(s, default='unknown')),\n",
    "                   signal_tier=('signal_tier', choose_tier),\n",
    "                   classification=('classification', choose_classification),\n",
    "                   final_status=('status', lambda s: choose_first_valid(list(s)[::-1], default='unknown')),\n",
    "                   any_fill=('fill_quantity', lambda s: float((pd.to_numeric(s, errors='coerce').abs().fillna(0.0) > 0).any()))\n",
    "               )\n",
    ")\n",
    "\n",
    "plot_tiers = ['strong', 'moderate', 'weak', 'exit']\n",
    "plot_sides = ['buy', 'sell']\n",
    "\n",
    "metric = (\n",
    "    order_summary[order_summary['signal_tier'].isin(plot_tiers) & order_summary['side'].isin(plot_sides)]\n",
    "      .groupby(['side', 'signal_tier'], as_index=False)\n",
    "      .agg(\n",
    "          order_count=('order_id', 'count'),\n",
    "          fill_rate_pct=('any_fill', lambda x: 100.0 * np.mean(x))\n",
    "      )\n",
    ")\n",
    "\n",
    "palette = {\n",
    "    'strong': '#1f77b4',\n",
    "    'moderate': '#ff7f0e',\n",
    "    'weak': '#2ca02c',\n",
    "    'exit': '#9467bd'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
    "for j, side in enumerate(plot_sides):\n",
    "    sub = metric[metric['side'] == side].copy()\n",
    "    sub['signal_tier'] = pd.Categorical(sub['signal_tier'], categories=plot_tiers, ordered=True)\n",
    "    sub = sub.sort_values('signal_tier')\n",
    "\n",
    "    sns.barplot(\n",
    "        data=sub,\n",
    "        x='signal_tier',\n",
    "        y='order_count',\n",
    "        order=plot_tiers,\n",
    "        palette=palette,\n",
    "        ax=axes[0, j]\n",
    "    )\n",
    "    axes[0, j].set_title(f'Order Count ({side.title()})')\n",
    "    axes[0, j].set_xlabel('Signal Tier')\n",
    "    axes[0, j].set_ylabel('Orders')\n",
    "    axes[0, j].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    sns.barplot(\n",
    "        data=sub,\n",
    "        x='signal_tier',\n",
    "        y='fill_rate_pct',\n",
    "        order=plot_tiers,\n",
    "        palette=palette,\n",
    "        ax=axes[1, j]\n",
    "    )\n",
    "    axes[1, j].set_title(f'Fill Rate ({side.title()})')\n",
    "    axes[1, j].set_xlabel('Signal Tier')\n",
    "    axes[1, j].set_ylabel('Fill Rate (%)')\n",
    "    axes[1, j].set_ylim(0, 100)\n",
    "    axes[1, j].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Order Count & Fill Rate by Signal Tier x Side', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(metric.sort_values(['side', 'signal_tier']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Daily Volume Profile by Signal Tier (Scale Day 0-4)\n",
    "\n",
    "Stacked area shows actual filled notional (% NAV) by scale day and tier.\n",
    "Dashed overlays show each tier's theoretical daily profile from front-load factors:\n",
    "- strong: `2.0`\n",
    "- moderate: `1.3`\n",
    "- weak: `1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_days = [0, 1, 2, 3, 4]\n",
    "tiers = ['strong', 'moderate', 'weak']\n",
    "\n",
    "profile = fills.copy()\n",
    "profile = profile[\n",
    "    profile['signal_tier'].isin(tiers) &\n",
    "    profile['scale_day'].notna() &\n",
    "    (profile['scale_day'] >= 0) &\n",
    "    (profile['scale_day'] <= 4)\n",
    "].copy()\n",
    "profile['scale_day'] = profile['scale_day'].astype(int)\n",
    "\n",
    "actual = (\n",
    "    profile.groupby(['scale_day', 'signal_tier'], as_index=False)\n",
    "           .agg(fill_pct_nav=('fill_pct_nav', 'sum'))\n",
    ")\n",
    "actual_pivot = (\n",
    "    actual.pivot(index='scale_day', columns='signal_tier', values='fill_pct_nav')\n",
    "          .reindex(index=scale_days, columns=tiers)\n",
    "          .fillna(0.0)\n",
    ")\n",
    "\n",
    "x = np.array(scale_days)\n",
    "stack_values = [100 * actual_pivot[t].to_numpy() for t in tiers]\n",
    "colors = {'strong': '#1f77b4', 'moderate': '#ff7f0e', 'weak': '#2ca02c'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.stackplot(\n",
    "    x,\n",
    "    stack_values,\n",
    "    labels=[f'{t} actual' for t in tiers],\n",
    "    colors=[colors[t] for t in tiers],\n",
    "    alpha=0.65\n",
    ")\n",
    "\n",
    "# Overlay theoretical daily schedule, scaled by each tier's total realized notional.\n",
    "schedule_cfg = {'strong': 2.0, 'moderate': 1.3, 'weak': 1.0}\n",
    "for tier in tiers:\n",
    "    total = float(actual_pivot[tier].sum())\n",
    "    if total <= 0:\n",
    "        continue\n",
    "    cumulative = build_scaling_schedule(5, schedule_cfg[tier])\n",
    "    increments = np.diff([0.0] + cumulative)\n",
    "    expected_daily = 100 * total * increments\n",
    "    ax.plot(\n",
    "        x,\n",
    "        expected_daily,\n",
    "        linestyle='--',\n",
    "        linewidth=2,\n",
    "        color=colors[tier],\n",
    "        label=f'{tier} theoretical'\n",
    "    )\n",
    "\n",
    "ax.set_title('Daily Volume Profile by Signal Tier over Scale Days')\n",
    "ax.set_xlabel('Scale Day')\n",
    "ax.set_ylabel('Filled Notional (% NAV)')\n",
    "ax.set_xticks(scale_days)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display((100 * actual_pivot).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Turnover Decomposition Waterfall by Classification\n",
    "\n",
    "Stacked bars by rebalance week show total filled turnover (% NAV) decomposed into:\n",
    "`NEW_ENTRY`, `RESIZE`, `FLIP`, `EXIT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_order = ['NEW_ENTRY', 'RESIZE', 'FLIP', 'EXIT']\n",
    "\n",
    "week_class = (\n",
    "    fills[fills['classification'].isin(class_order)]\n",
    "      .groupby(['week_id', 'classification'], as_index=False)\n",
    "      .agg(fill_pct_nav=('fill_pct_nav', 'sum'))\n",
    ")\n",
    "\n",
    "week_dates = (\n",
    "    week_symbol[['week_id', 'week_start_date']]\n",
    "      .drop_duplicates('week_id')\n",
    ")\n",
    "week_class = week_class.merge(week_dates, on='week_id', how='left')\n",
    "week_class['week_start_date'] = pd.to_datetime(week_class['week_start_date'])\n",
    "\n",
    "pivot = (\n",
    "    week_class.pivot_table(\n",
    "        index='week_start_date',\n",
    "        columns='classification',\n",
    "        values='fill_pct_nav',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0.0\n",
    "    )\n",
    "    .reindex(columns=class_order)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "bottom = np.zeros(len(pivot))\n",
    "x = np.arange(len(pivot))\n",
    "color_map = {\n",
    "    'NEW_ENTRY': '#1f77b4',\n",
    "    'RESIZE': '#ff7f0e',\n",
    "    'FLIP': '#2ca02c',\n",
    "    'EXIT': '#d62728'\n",
    "}\n",
    "\n",
    "for cls in class_order:\n",
    "    vals = 100 * pivot[cls].to_numpy()\n",
    "    ax.bar(x, vals, bottom=bottom, label=cls, color=color_map[cls], width=0.8)\n",
    "    bottom += vals\n",
    "\n",
    "labels = [d.strftime('%Y-%m-%d') for d in pivot.index]\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('Filled Turnover (% NAV)')\n",
    "ax.set_xlabel('Rebalance Week Start')\n",
    "ax.set_title('Turnover Decomposition by Classification (Waterfall-Style Stacked Bars)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend(title='Classification')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display((100 * pivot).round(4).tail(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Signal Magnitude vs Filled Notional (Symbol-Week)\n",
    "\n",
    "Each point is one symbol-week. X = `|signal magnitude|`, Y = total filled notional (% NAV), color = classification.\n",
    "Black line is an overall OLS trend line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_order = ['NEW_ENTRY', 'RESIZE', 'FLIP', 'EXIT']\n",
    "\n",
    "symbol_week = (\n",
    "    fills[fills['classification'].isin(class_order)]\n",
    "      .groupby(['week_id', 'symbol', 'classification'], as_index=False)\n",
    "      .agg(filled_pct_nav=('fill_pct_nav', 'sum'))\n",
    ")\n",
    "\n",
    "symbol_week = symbol_week.merge(\n",
    "    week_symbol[['week_id', 'symbol', 'signal_strength']],\n",
    "    on=['week_id', 'symbol'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "symbol_week['signal_strength'] = pd.to_numeric(symbol_week['signal_strength'], errors='coerce')\n",
    "symbol_week = symbol_week[symbol_week['signal_strength'].notna() & (symbol_week['signal_strength'] > 0)].copy()\n",
    "symbol_week['filled_pct_nav_pct'] = 100 * symbol_week['filled_pct_nav']\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=symbol_week,\n",
    "    x='signal_strength',\n",
    "    y='filled_pct_nav_pct',\n",
    "    hue='classification',\n",
    "    hue_order=class_order,\n",
    "    palette={'NEW_ENTRY': '#1f77b4', 'RESIZE': '#ff7f0e', 'FLIP': '#2ca02c', 'EXIT': '#d62728'},\n",
    "    alpha=0.75,\n",
    "    s=55\n",
    ")\n",
    "\n",
    "if len(symbol_week) >= 2:\n",
    "    sns.regplot(\n",
    "        data=symbol_week,\n",
    "        x='signal_strength',\n",
    "        y='filled_pct_nav_pct',\n",
    "        scatter=False,\n",
    "        color='black',\n",
    "        line_kws={'linewidth': 2, 'alpha': 0.9}\n",
    "    )\n",
    "\n",
    "plt.title('Signal Magnitude vs Filled Notional (% NAV) by Classification')\n",
    "plt.xlabel('|Signal Magnitude|')\n",
    "plt.ylabel('Filled Notional (% NAV) per Symbol-Week')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(title='Classification')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "corr = symbol_week[['signal_strength', 'filled_pct_nav_pct']].corr().iloc[0, 1] if len(symbol_week) >= 2 else np.nan\n",
    "print(f'Pearson correlation(|signal|, filled %NAV): {corr:.4f}' if pd.notna(corr) else 'Not enough points for correlation.')\n",
    "display(symbol_week.sort_values('filled_pct_nav', ascending=False).head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Summary: Top Trading Drivers\n",
    "\n",
    "This section prints compact diagnostics from the computed tables above so you can quickly see where execution is concentrated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto summary of key drivers from the decomposition analysis\n",
    "\n",
    "if 'fills' not in globals() or len(fills) == 0:\n",
    "    print('No fill data available for summary.')\n",
    "else:\n",
    "    class_order = ['NEW_ENTRY', 'RESIZE', 'FLIP', 'EXIT']\n",
    "    tier_order = ['strong', 'moderate', 'weak', 'exit', 'unknown']\n",
    "\n",
    "    # 1) Top tier x classification buckets by total filled % NAV\n",
    "    bucket = (\n",
    "        fills.groupby(['signal_tier', 'classification'], as_index=False)\n",
    "             .agg(filled_pct_nav=('fill_pct_nav', 'sum'))\n",
    "    )\n",
    "    bucket = bucket.sort_values('filled_pct_nav', ascending=False)\n",
    "    total_pct_nav = bucket['filled_pct_nav'].sum()\n",
    "    bucket['share_pct'] = np.where(total_pct_nav > 1e-12, 100.0 * bucket['filled_pct_nav'] / total_pct_nav, np.nan)\n",
    "\n",
    "    print('Top Tier x Classification Buckets (by filled %NAV):')\n",
    "    display(bucket.head(12))\n",
    "\n",
    "    # 2) Top rebalance weeks by turnover and their composition\n",
    "    week_mix = (\n",
    "        fills[fills['classification'].isin(class_order)]\n",
    "            .groupby(['week_id', 'classification'], as_index=False)\n",
    "            .agg(filled_pct_nav=('fill_pct_nav', 'sum'))\n",
    "    )\n",
    "    week_pivot = (\n",
    "        week_mix.pivot_table(index='week_id', columns='classification', values='filled_pct_nav', aggfunc='sum', fill_value=0.0)\n",
    "                .reindex(columns=class_order, fill_value=0.0)\n",
    "                .reset_index()\n",
    "    )\n",
    "    week_pivot['turnover_pct_nav'] = week_pivot[class_order].sum(axis=1)\n",
    "    for c in class_order:\n",
    "        week_pivot[f'{c}_share_pct'] = np.where(\n",
    "            week_pivot['turnover_pct_nav'] > 1e-12,\n",
    "            100.0 * week_pivot[c] / week_pivot['turnover_pct_nav'],\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "    top_weeks = week_pivot.sort_values('turnover_pct_nav', ascending=False).head(10)\n",
    "    print('Top Rebalance Weeks by Filled Turnover (%NAV):')\n",
    "    display(top_weeks[['week_id', 'turnover_pct_nav'] + class_order + [f'{c}_share_pct' for c in class_order]])\n",
    "\n",
    "    # 3) Fill-rate asymmetry by side and tier (order-level)\n",
    "    if 'order_summary' in globals() and len(order_summary):\n",
    "        asym = (\n",
    "            order_summary[order_summary['side'].isin(['buy', 'sell'])]\n",
    "                .groupby(['signal_tier', 'side'], as_index=False)\n",
    "                .agg(\n",
    "                    orders=('order_id', 'count'),\n",
    "                    fill_rate=('any_fill', 'mean')\n",
    "                )\n",
    "        )\n",
    "        asym['fill_rate_pct'] = 100.0 * asym['fill_rate']\n",
    "        asym = asym.sort_values(['signal_tier', 'side'])\n",
    "\n",
    "        asym_pivot = asym.pivot(index='signal_tier', columns='side', values='fill_rate_pct')\n",
    "        asym_pivot['buy_minus_sell_fill_rate_pct'] = asym_pivot.get('buy', np.nan) - asym_pivot.get('sell', np.nan)\n",
    "\n",
    "        print('Fill-Rate by Tier x Side (%):')\n",
    "        display(asym)\n",
    "        print('Buy - Sell Fill-Rate Spread (% points):')\n",
    "        display(asym_pivot[['buy_minus_sell_fill_rate_pct']].sort_values('buy_minus_sell_fill_rate_pct', ascending=False))\n",
    "\n",
    "    # 4) Signal strength vs filled notional relationship\n",
    "    if 'symbol_week' in globals() and len(symbol_week) >= 2:\n",
    "        corr = symbol_week[['signal_strength', 'filled_pct_nav']].corr().iloc[0, 1]\n",
    "        slope, intercept = np.polyfit(symbol_week['signal_strength'], symbol_week['filled_pct_nav'], 1)\n",
    "        print(\n",
    "            f\"Signal-strength linkage: corr={corr:.4f}, \"\n",
    "            f\"slope={slope:.6f} (filled %NAV per 1.0 signal-magnitude unit), \"\n",
    "            f\"intercept={intercept:.6f}\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}