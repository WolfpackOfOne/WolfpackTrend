{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebalance Week Health\n",
    "\n",
    "Weekly health dashboard that combines:\n",
    "- Exposure path (`daily_snapshots.csv`)\n",
    "- Outstanding execution (`targets.csv`)\n",
    "- Turnover/slippage (`order_events.csv`, `slippage.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uxuyup1st6r",
   "source": "## Data Loading & Panel Assembly\n\nLoads daily snapshots, weekly targets, order events, and slippage data, then constructs a unified daily panel by joining portfolio-level outstanding order percentage, daily filled notional, and daily slippage onto the snapshot timeline. The tail preview confirms the join succeeded and all four data sources contributed correctly. This panel is the single input for all charts and the weekly health summary below.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "lf6wny1c8pp",
   "source": "## Rebalance Week Health â€” 3-Panel Chart\n\nThis 3-panel chart provides an integrated view of each rebalance week's health over the backtest. The top panel shows gross and net exposure evolving day by day; the middle panel shows what fraction of the week's orders remain outstanding each day (should trend toward zero by Friday); and the bottom panel shows daily filled notional and slippage in dollars, revealing the cost of execution on active trading days. Weeks where outstanding exposure stays elevated through Friday indicate incomplete fills that carry into the next cycle.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "5agubyypmw",
   "source": "## Weekly Health Summary Table\n\nThis table collapses the daily panel to one row per rebalance week, reporting total order size, final outstanding percentage at end of week, and total slippage in dollars. High `final_outstanding_pct` values flag weeks where the 5-day scaling schedule failed to fully execute, which may coincide with market volatility or aggressive limit offsets causing fill failures. Use this table to identify specific problematic weeks for deeper investigation in the other trading notebooks.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snap = read_csv_from_store('wolfpack/daily_snapshots.csv')\n",
    "df_targets = read_csv_from_store('wolfpack/targets.csv')\n",
    "df_events = read_csv_from_store('wolfpack/order_events.csv')\n",
    "df_slippage = read_csv_from_store('wolfpack/slippage.csv')\n",
    "\n",
    "if df_snap is None or df_targets is None:\n",
    "    raise ValueError('daily_snapshots.csv and targets.csv are required.')\n",
    "\n",
    "df_snap['date'] = pd.to_datetime(df_snap['date'])\n",
    "df_targets['date'] = pd.to_datetime(df_targets['date'])\n",
    "for col in ['start_w', 'weekly_target_w', 'actual_w']:\n",
    "    df_targets[col] = pd.to_numeric(df_targets[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "if df_events is not None:\n",
    "    df_events['date'] = pd.to_datetime(df_events['date'])\n",
    "    for col in ['fill_quantity', 'fill_price']:\n",
    "        df_events[col] = pd.to_numeric(df_events[col], errors='coerce').fillna(0.0)\n",
    "if df_slippage is not None:\n",
    "    df_slippage['date'] = pd.to_datetime(df_slippage['date'])\n",
    "    df_slippage['slippage_dollars'] = pd.to_numeric(df_slippage['slippage_dollars'], errors='coerce').fillna(0.0)\n",
    "\n",
    "# Portfolio outstanding\n",
    "dx = df_targets.copy()\n",
    "dx['total_order_abs'] = (dx['weekly_target_w'] - dx['start_w']).abs()\n",
    "dx['remaining_abs'] = (dx['weekly_target_w'] - dx['actual_w']).abs()\n",
    "dx['remaining_abs'] = np.minimum(dx['remaining_abs'], dx['total_order_abs'])\n",
    "\n",
    "outstanding_daily = (\n",
    "    dx.groupby(['date', 'week_id'], as_index=False)\n",
    "      .agg(total_order_abs=('total_order_abs', 'sum'),\n",
    "           remaining_abs=('remaining_abs', 'sum'))\n",
    ")\n",
    "outstanding_daily['outstanding_pct'] = np.where(\n",
    "    outstanding_daily['total_order_abs'] > 1e-10,\n",
    "    outstanding_daily['remaining_abs'] / outstanding_daily['total_order_abs'],\n",
    "    0.0\n",
    ")\n",
    "\n",
    "if df_events is not None:\n",
    "    turnover_daily = (\n",
    "        df_events.assign(filled_notional=(df_events['fill_quantity'].abs() * df_events['fill_price'].abs()))\n",
    "                 .groupby('date', as_index=False)['filled_notional']\n",
    "                 .sum()\n",
    "    )\n",
    "else:\n",
    "    turnover_daily = pd.DataFrame(columns=['date', 'filled_notional'])\n",
    "\n",
    "if df_slippage is not None:\n",
    "    slippage_daily = (\n",
    "        df_slippage.groupby('date', as_index=False)['slippage_dollars']\n",
    "                   .sum()\n",
    "                   .rename(columns={'slippage_dollars': 'daily_slippage_abs'})\n",
    "    )\n",
    "else:\n",
    "    slippage_daily = pd.DataFrame(columns=['date', 'daily_slippage_abs'])\n",
    "\n",
    "panel = df_snap.merge(outstanding_daily[['date', 'outstanding_pct']], on='date', how='left')\n",
    "panel = panel.merge(turnover_daily, on='date', how='left')\n",
    "panel = panel.merge(slippage_daily, on='date', how='left')\n",
    "panel = panel.sort_values('date')\n",
    "\n",
    "display(panel.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "axes[0].plot(panel['date'], 100 * pd.to_numeric(panel['gross_exposure'], errors='coerce'), label='Gross %', color='#1f77b4')\n",
    "axes[0].plot(panel['date'], 100 * pd.to_numeric(panel['net_exposure'], errors='coerce'), label='Net %', color='#ff7f0e')\n",
    "axes[0].set_title('Exposure Path')\n",
    "axes[0].set_ylabel('% exposure')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(panel['date'], 100 * panel['outstanding_pct'], color='#d62728')\n",
    "axes[1].set_title('Portfolio Outstanding Weekly Order %')\n",
    "axes[1].set_ylabel('% outstanding')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].bar(panel['date'], panel['filled_notional'].fillna(0.0), color='#2ca02c', alpha=0.6, label='Filled notional')\n",
    "axes[2].plot(panel['date'], panel['daily_slippage_abs'].fillna(0.0), color='#9467bd', linewidth=1.5, label='Daily slippage $')\n",
    "axes[2].set_title('Turnover and Slippage')\n",
    "axes[2].set_ylabel('$')\n",
    "axes[2].legend()\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_health = (\n",
    "    outstanding_daily.sort_values(['week_id', 'date'])\n",
    "                   .groupby('week_id', as_index=False)\n",
    "                   .agg(\n",
    "                       start_date=('date', 'min'),\n",
    "                       end_date=('date', 'max'),\n",
    "                       week_total_order=('total_order_abs', 'max'),\n",
    "                       final_outstanding_pct=('outstanding_pct', 'last')\n",
    "                   )\n",
    "                   .sort_values('start_date')\n",
    ")\n",
    "\n",
    "if df_slippage is not None and len(df_slippage):\n",
    "    slip_week = (\n",
    "        df_slippage.assign(week_id=df_slippage['date'].dt.strftime('%Y-%m-%d'))\n",
    "                   .groupby('week_id', as_index=False)['slippage_dollars']\n",
    "                   .sum()\n",
    "                   .rename(columns={'slippage_dollars': 'week_slippage_dollars'})\n",
    "    )\n",
    "    weekly_health = weekly_health.merge(slip_week, on='week_id', how='left')\n",
    "\n",
    "display(weekly_health.tail(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}