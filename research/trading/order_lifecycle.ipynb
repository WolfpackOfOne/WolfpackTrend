{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Lifecycle\n",
    "\n",
    "Lifecycle analytics using `{TEAM_ID}/order_events.csv`.\n",
    "\n",
    "This tracks:\n",
    "- Submitted, partially filled, filled, canceled counts\n",
    "- Fill ratio and time-to-final-status\n",
    "- Final status by execution tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tdtxlm9r5lm",
   "source": [
    "## Data Loading \u2014 Order Events Log\n",
    "\n",
    "Loads the order events log from ObjectStore, parses tier and week-ID tags from the order annotation string, and displays the first few rows to confirm the schema. Each row represents one status update (submitted, partially filled, filled, cancelled) for a single order. The `tier` column inferred from tags is the key dimension used to segment all lifecycle statistics that follow."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "rjmgx8yg64f",
   "source": [
    "## Order-Level Summary \u2014 Fill Ratio and Days to Final\n",
    "\n",
    "This cell pivots from status-event rows to one row per order, computing fill ratio, days-to-final-status, and final outcome for each order ID. The resulting `order_summary` DataFrame is the primary analysis unit \u2014 one order with its full lifecycle condensed into a single record. The head preview confirms that `fill_ratio` and `days_to_final` are populated before charts are generated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "r8edo90h94l",
   "source": [
    "## Order Lifecycle \u2014 4-Panel Chart\n",
    "\n",
    "These four panels summarize the order lifecycle from submission to final resolution. Top-left shows counts by final status (filled, cancelled, etc.); top-right shows the fill-ratio distribution to reveal how often orders are only partially filled; bottom-left shows the final status mix as a stacked proportion by tier, testing whether high-tier orders fill more reliably; and bottom-right shows time-to-final-status by tier, checking whether strong-tier market-price limits resolve faster than wide weak-tier limits."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "axmmic7mfj5",
   "source": [
    "## Cancel Rate and Fill Ratio Scorecard by Tier\n",
    "\n",
    "This summary table reports cancel rate and average fill ratio for each signal tier, combining lifecycle performance into a compact scorecard. A higher cancel rate for weak-tier orders is expected given their 1.5% limit offset, but if strong-tier orders also show elevated cancels it may indicate the stale-limit cancellation threshold is too aggressive. Compare average fill ratios across tiers to confirm the tiered limit-offset design achieves meaningfully different fill outcomes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qtx95cqmlln",
   "source": [
    "# DIAGNOSTIC: Check what's happening with order_events.csv\n",
    "print(\"=== Checking ObjectStore ===\")\n",
    "print(f\"Key exists: {qb.ObjectStore.ContainsKey(f'{TEAM_ID}/order_events.csv')}\")\n",
    "\n",
    "if qb.ObjectStore.ContainsKey(f'{TEAM_ID}/order_events.csv'):\n",
    "    print(\"\\nAttempting to read...\")\n",
    "    try:\n",
    "        content = qb.ObjectStore.Read(f'{TEAM_ID}/order_events.csv')\n",
    "        print(f\"Content type: {type(content)}\")\n",
    "        print(f\"Content length: {len(content) if content else 0}\")\n",
    "        if content:\n",
    "            print(f\"First 200 chars: {content[:200]}\")\n",
    "            # Try parsing\n",
    "            df_test = pd.read_csv(StringIO(content))\n",
    "            print(f\"\\nSuccessfully parsed! Rows: {len(df_test)}, Columns: {list(df_test.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n=== Available keys in ObjectStore: ===\")\n",
    "    # List all keys to see what's actually available\n",
    "    try:\n",
    "        all_keys = list(qb.ObjectStore.GetEnumerator())\n",
    "        print(f\"Total keys: {len(all_keys)}\")\n",
    "        wolfpack_keys = [k for k in all_keys if 'wolfpack' in k.lower()]\n",
    "        print(f\"Wolfpack-related keys: {wolfpack_keys}\")\n",
    "    except:\n",
    "        print(\"Could not enumerate ObjectStore keys\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_events = read_csv_from_store(f'{TEAM_ID}/order_events.csv')\n",
    "if df_events is None:\n",
    "    raise ValueError('order_events.csv is required. Run a backtest with order-event logging enabled.')\n",
    "\n",
    "df_events['date'] = pd.to_datetime(df_events['date'])\n",
    "for col in ['quantity', 'fill_quantity', 'fill_price', 'limit_price']:\n",
    "    if col in df_events.columns:\n",
    "        df_events[col] = pd.to_numeric(df_events[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "def parse_tag_value(tag, key):\n",
    "    if pd.isna(tag):\n",
    "        return np.nan\n",
    "    m = re.search(rf'{key}=([^;]+)', str(tag))\n",
    "    return m.group(1) if m else np.nan\n",
    "\n",
    "df_events['tier'] = df_events['tag'].apply(lambda t: parse_tag_value(t, 'tier')).fillna('unknown')\n",
    "df_events['week_id'] = df_events['tag'].apply(lambda t: parse_tag_value(t, 'week_id')).fillna('')\n",
    "\n",
    "print(f'order events: {len(df_events):,}')\n",
    "display(df_events.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate each order lifecycle\n",
    "grp = df_events.sort_values('date').groupby('order_id', as_index=False)\n",
    "\n",
    "order_summary = grp.agg(\n",
    "    symbol=('symbol', 'first'),\n",
    "    tier=('tier', 'first'),\n",
    "    order_type=('order_type', 'first'),\n",
    "    quantity=('quantity', 'first'),\n",
    "    submitted_at=('date', 'min'),\n",
    "    final_at=('date', 'max')\n",
    ")\n",
    "\n",
    "final_status = (\n",
    "    df_events.sort_values('date')\n",
    "             .groupby('order_id')\n",
    "             .tail(1)[['order_id', 'status']]\n",
    "             .rename(columns={'status': 'final_status'})\n",
    ")\n",
    "\n",
    "fills = (\n",
    "    df_events.groupby('order_id', as_index=False)['fill_quantity']\n",
    "             .sum()\n",
    "             .rename(columns={'fill_quantity': 'filled_qty'})\n",
    ")\n",
    "\n",
    "order_summary = order_summary.merge(final_status, on='order_id', how='left')\n",
    "order_summary = order_summary.merge(fills, on='order_id', how='left')\n",
    "\n",
    "order_summary['abs_qty'] = order_summary['quantity'].abs().replace(0, np.nan)\n",
    "order_summary['fill_ratio'] = (order_summary['filled_qty'].abs() / order_summary['abs_qty']).fillna(0.0).clip(0, 1)\n",
    "order_summary['days_to_final'] = (order_summary['final_at'] - order_summary['submitted_at']).dt.days.fillna(0)\n",
    "\n",
    "display(order_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "status_counts = order_summary['final_status'].value_counts().sort_values(ascending=False)\n",
    "status_counts.plot(kind='bar', ax=axes[0, 0], color='#1f77b4')\n",
    "axes[0, 0].set_title('Final Order Status Counts')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.histplot(order_summary['fill_ratio'], bins=20, ax=axes[0, 1], color='#2ca02c')\n",
    "axes[0, 1].set_title('Fill Ratio Distribution')\n",
    "axes[0, 1].set_xlabel('Fill ratio')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "tier_status = pd.crosstab(order_summary['tier'], order_summary['final_status'], normalize='index')\n",
    "tier_status = tier_status.reindex(['strong', 'moderate', 'weak', 'exit', 'unknown']).dropna(how='all')\n",
    "tier_status.plot(kind='bar', stacked=True, ax=axes[1, 0], colormap='tab20')\n",
    "axes[1, 0].set_title('Final Status Mix by Tier')\n",
    "axes[1, 0].set_ylabel('Share')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=order_summary, x='tier', y='days_to_final', order=['strong', 'moderate', 'weak', 'exit', 'unknown'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Days to Final Status by Tier')\n",
    "axes[1, 1].set_xlabel('Tier')\n",
    "axes[1, 1].set_ylabel('Days')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancel_rate = (\n",
    "    order_summary.assign(is_canceled=order_summary['final_status'].astype(str).str.contains('Canceled', case=False, na=False))\n",
    "                .groupby('tier', as_index=False)\n",
    "                .agg(orders=('order_id', 'count'),\n",
    "                     cancel_rate=('is_canceled', 'mean'),\n",
    "                     avg_fill_ratio=('fill_ratio', 'mean'))\n",
    "                .sort_values('orders', ascending=False)\n",
    ")\n",
    "display(cancel_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opeq5qz2ulb",
   "source": [
    "## Investigation: Days-to-Cancellation Analysis\n",
    "\n",
    "Hypothesis: Cancelled orders are being killed at exactly 2 days due to the 2-open-check rule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dt5uv68vyod",
   "source": [
    "# Filter to cancelled orders only\n",
    "cancelled_orders = order_summary[\n",
    "    order_summary['final_status'].str.contains('Canceled', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "# Calculate days to cancellation\n",
    "cancelled_orders['days_to_cancel'] = cancelled_orders['days_to_final']\n",
    "\n",
    "# Plot distribution by tier\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for tier in ['strong', 'moderate', 'weak']:\n",
    "    tier_data = cancelled_orders[cancelled_orders['tier'] == tier]['days_to_cancel']\n",
    "    if len(tier_data) > 0:\n",
    "        ax.hist(tier_data, bins=20, alpha=0.5, label=f'{tier} (n={len(tier_data)})')\n",
    "ax.set_xlabel('Days to Cancellation')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Days-to-Cancellation Distribution by Tier')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print median/mean by tier\n",
    "print(\"\\nDays to Cancellation by Tier:\")\n",
    "display(cancelled_orders.groupby('tier')['days_to_cancel'].describe())"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "92gi3y1hsp",
   "source": [
    "## Fill Progression by Order Age\n",
    "\n",
    "Shows whether fill ratios drop sharply after 2 days (orders cancelled vs naturally unfilled)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zcrjgpkbfim",
   "source": [
    "# Group orders by days_to_final and tier\n",
    "fill_by_age = order_summary.groupby(['tier', 'days_to_final'], as_index=False).agg(\n",
    "    count=('order_id', 'count'),\n",
    "    avg_fill_ratio=('fill_ratio', 'mean'),\n",
    "    cancel_rate=('final_status', lambda x: x.str.contains('Canceled', case=False, na=False).mean())\n",
    ")\n",
    "\n",
    "# Plot for moderate tier\n",
    "moderate_data = fill_by_age[fill_by_age['tier'] == 'moderate'].sort_values('days_to_final')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Fill ratio by age\n",
    "axes[0].plot(moderate_data['days_to_final'], moderate_data['avg_fill_ratio'],\n",
    "             marker='o', linewidth=2, markersize=6)\n",
    "axes[0].set_xlabel('Days to Final Status')\n",
    "axes[0].set_ylabel('Average Fill Ratio')\n",
    "axes[0].set_title('Moderate Tier: Fill Ratio by Order Age')\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].axvline(x=2, color='red', linestyle='--', label='2-check threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cancellation rate by age\n",
    "axes[1].plot(moderate_data['days_to_final'], moderate_data['cancel_rate'],\n",
    "             marker='o', linewidth=2, markersize=6, color='red')\n",
    "axes[1].set_xlabel('Days to Final Status')\n",
    "axes[1].set_ylabel('Cancellation Rate')\n",
    "axes[1].set_title('Moderate Tier: Cancellation Rate by Order Age')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axvline(x=2, color='red', linestyle='--', label='2-check threshold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModerate Tier Order Age Statistics:\")\n",
    "display(moderate_data[['days_to_final', 'count', 'avg_fill_ratio', 'cancel_rate']].head(10))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pgoaknnss8k",
   "source": [
    "## Price Movement at Cancellation\n",
    "\n",
    "Shows whether cancelled orders had favorable price moves but were still cancelled (mechanism issue vs market issue)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a1r1c41kcwv",
   "source": [
    "# FIXED: Use actual market_price_at_submit and positions.csv for accurate price tracking\n",
    "\n",
    "# 1. Get actual market price at submission (already logged in order_events)\n",
    "# Note: Don't select 'symbol' here since cancelled_orders already has it\n",
    "submit_prices = df_events.groupby('order_id', as_index=False).first()[\n",
    "    ['order_id', 'date', 'quantity', 'market_price_at_submit']\n",
    "].rename(columns={'date': 'submit_date', 'quantity': 'submit_qty'})\n",
    "\n",
    "# 2. Get cancellation dates for cancelled orders\n",
    "cancel_dates = df_events.groupby('order_id', as_index=False).last()[\n",
    "    ['order_id', 'date']\n",
    "].rename(columns={'date': 'cancel_date'})\n",
    "\n",
    "# 3. Merge with cancelled_orders (which already has symbol from order_summary)\n",
    "cancelled_with_events = (\n",
    "    cancelled_orders\n",
    "    .merge(submit_prices, on='order_id', how='left')\n",
    "    .merge(cancel_dates, on='order_id', how='left')\n",
    ")\n",
    "\n",
    "# 4. Load positions.csv to get market prices at cancellation\n",
    "df_positions = read_csv_from_store(f'{TEAM_ID}/positions.csv')\n",
    "if df_positions is not None:\n",
    "    df_positions['date'] = pd.to_datetime(df_positions['date'])\n",
    "\n",
    "    # Get price at cancellation by matching (symbol, cancel_date) with positions\n",
    "    cancel_prices = (\n",
    "        cancelled_with_events[['order_id', 'symbol', 'cancel_date']]\n",
    "        .merge(\n",
    "            df_positions[['symbol', 'date', 'price']],\n",
    "            left_on=['symbol', 'cancel_date'],\n",
    "            right_on=['symbol', 'date'],\n",
    "            how='left'\n",
    "        )\n",
    "        .rename(columns={'price': 'cancel_market_price'})\n",
    "        [['order_id', 'cancel_market_price']]\n",
    "    )\n",
    "\n",
    "    cancelled_with_events = cancelled_with_events.merge(cancel_prices, on='order_id', how='left')\n",
    "\n",
    "    # 5. Calculate price movement in favorable direction\n",
    "    cancelled_with_events['price_change_pct'] = (\n",
    "        (cancelled_with_events['cancel_market_price'] - cancelled_with_events['market_price_at_submit'])\n",
    "        / cancelled_with_events['market_price_at_submit']\n",
    "        * 100\n",
    "    )\n",
    "\n",
    "    # Adjust for direction: for buys, negative is favorable (price dropped)\n",
    "    # For sells, positive is favorable (price rose)\n",
    "    cancelled_with_events.loc[cancelled_with_events['submit_qty'] > 0, 'price_change_pct'] *= -1\n",
    "\n",
    "    # 6. Plot distribution for moderate tier\n",
    "    moderate_cancelled = cancelled_with_events[cancelled_with_events['tier'] == 'moderate']\n",
    "    valid_data = moderate_cancelled['price_change_pct'].dropna()\n",
    "\n",
    "    if len(valid_data) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.hist(valid_data, bins=30, alpha=0.7, color='orange')\n",
    "        ax.axvline(x=0.5, color='green', linestyle='--', linewidth=2, label='0.5% favorable (would fill)')\n",
    "        ax.axvline(x=-0.5, color='red', linestyle='--', linewidth=2, label='0.5% adverse (would not fill)')\n",
    "        ax.axvline(x=0, color='gray', linestyle='-', alpha=0.3, label='No movement')\n",
    "        ax.set_xlabel('Price Change % (Favorable Direction)')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Moderate Tier: Price Movement at Cancellation (Fixed - Using Actual Market Prices)')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        # Print stats\n",
    "        print(\"\\nModerate Tier Cancelled Orders - Price Movement Stats:\")\n",
    "        print(f\"Total cancelled orders analyzed: {len(valid_data):,}\")\n",
    "        print(f\"Orders with >0.5% favorable move: {(valid_data > 0.5).sum():,} ({(valid_data > 0.5).mean()*100:.1f}%)\")\n",
    "        print(f\"Orders with <-0.5% adverse move: {(valid_data < -0.5).sum():,} ({(valid_data < -0.5).mean()*100:.1f}%)\")\n",
    "        print(f\"Orders in neutral range [-0.5%, 0.5%]: {((valid_data >= -0.5) & (valid_data <= 0.5)).sum():,} ({((valid_data >= -0.5) & (valid_data <= 0.5)).mean()*100:.1f}%)\")\n",
    "        print(f\"\\nMean price movement: {valid_data.mean():.3f}%\")\n",
    "        print(f\"Median price movement: {valid_data.median():.3f}%\")\n",
    "    else:\n",
    "        print(\"No valid price data available\")\n",
    "else:\n",
    "    print(\"positions.csv not found - cannot calculate price movement\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2tco1fya5nn",
   "source": [
    "## Scaling Day Analysis\n",
    "\n",
    "Determine which days of the 5-day scaling window see the most cancellations (requires week_id in order tags)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "a611gpb851u",
   "source": [
    "# Parse week_id and calculate scaling day (requires week_id in tags)\n",
    "# Check if week_id is available\n",
    "if 'week_id' in df_events.columns and df_events['week_id'].notna().any():\n",
    "    print(\"week_id found in order events - proceeding with analysis\")\n",
    "    \n",
    "    # Group by week_id and symbol, then rank dates to get day within week\n",
    "    scaling_analysis = df_events.sort_values('date').copy()\n",
    "    scaling_analysis['week_day'] = (\n",
    "        scaling_analysis.groupby(['symbol', 'week_id'])['date']\n",
    "        .rank(method='dense')\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    # Merge with cancelled_orders to get week_day for each cancelled order\n",
    "    cancelled_by_day = (\n",
    "        cancelled_orders.merge(\n",
    "            scaling_analysis[['order_id', 'week_day']].drop_duplicates(),\n",
    "            on='order_id',\n",
    "            how='left'\n",
    "        )\n",
    "        .dropna(subset=['week_day'])\n",
    "        .groupby(['tier', 'week_day'], as_index=False)\n",
    "        .agg(cancelled_count=('order_id', 'count'))\n",
    "    )\n",
    "    \n",
    "    # Plot by tier\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # All tiers together\n",
    "    total_by_day = cancelled_by_day.groupby('week_day', as_index=False)['cancelled_count'].sum()\n",
    "    axes[0].bar(total_by_day['week_day'], total_by_day['cancelled_count'], color='steelblue')\n",
    "    axes[0].set_xlabel('Day in Scaling Window')\n",
    "    axes[0].set_ylabel('Cancellation Count')\n",
    "    axes[0].set_title('All Tiers: Cancellations by Day in 5-Day Scaling Window')\n",
    "    axes[0].set_xticks(range(1, 6))\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Moderate tier specifically\n",
    "    moderate_by_day = cancelled_by_day[cancelled_by_day['tier'] == 'moderate']\n",
    "    if len(moderate_by_day) > 0:\n",
    "        axes[1].bar(moderate_by_day['week_day'], moderate_by_day['cancelled_count'], color='orange')\n",
    "        axes[1].set_xlabel('Day in Scaling Window')\n",
    "        axes[1].set_ylabel('Cancellation Count')\n",
    "        axes[1].set_title('Moderate Tier: Cancellations by Day in 5-Day Scaling Window')\n",
    "        axes[1].set_xticks(range(1, 6))\n",
    "        axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nCancellations by Scaling Day and Tier:\")\n",
    "    pivot = cancelled_by_day.pivot(index='week_day', columns='tier', values='cancelled_count').fillna(0).astype(int)\n",
    "    display(pivot)\n",
    "    \n",
    "else:\n",
    "    print(\"NOTE: week_id not available in order events\")\n",
    "    print(\"This analysis requires backtest re-run with week_id logging in order tags\")\n",
    "    print(\"\\nCurrent order event columns:\")\n",
    "    print(df_events.columns.tolist())\n",
    "    print(\"\\nSample tag values:\")\n",
    "    display(df_events[['order_id', 'tag']].head(10))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cjsbsixusik",
   "source": [
    "## Summary: Validating the 2-Check Cancellation Hypothesis\n",
    "\n",
    "### Expected Results if Hypothesis is **Confirmed**:\n",
    "\n",
    "1. **Days-to-Cancellation**: Peak at 2 days for moderate tier\n",
    "2. **Fill Progression**: Sharp cliff at 2 days where cancellation rate spikes\n",
    "3. **Price Movement**: Some cancelled orders had favorable moves >0.5% (mechanism issue, not market)\n",
    "4. **Scaling Days**: Early-week orders (days 1-2) cancelled before completion\n",
    "\n",
    "### If Hypothesis is **Rejected**:\n",
    "- Different patterns suggest alternative root causes\n",
    "- May need to investigate: limit pricing, liquidity issues, or other factors\n",
    "\n",
    "### Next Steps Based on Results:\n",
    "- **Hypothesis confirmed** \u2192 Implement Option 2: signal-aware cancellation (only cancel previous rebalance cycles)\n",
    "- **Hypothesis rejected** \u2192 Investigate alternative approaches (adjust offsets, increase checks, etc.)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}