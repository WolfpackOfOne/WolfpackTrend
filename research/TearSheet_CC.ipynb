{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy Tear Sheet\n",
    "\n",
    "Comprehensive tear sheet generated from ObjectStore CSVs logged by the backtest.\n",
    "\n",
    "**Data Sources:**\n",
    "- `{TEAM_ID}/daily_snapshots.csv` — Daily NAV, exposure, P&L, volatility\n",
    "- `{TEAM_ID}/positions.csv` — Per-symbol daily positions and P&L\n",
    "- `{TEAM_ID}/signals.csv` — Signal scores on rebalance days\n",
    "- `{TEAM_ID}/slippage.csv` — Per-fill slippage data\n",
    "- `{TEAM_ID}/trades.csv` — Completed round-trip trades\n",
    "- `{TEAM_ID}/targets.csv` — Scaling schedule targets\n",
    "- `{TEAM_ID}/order_events.csv` — Full order lifecycle events\n",
    "- SPY and SGOV fetched live via QuantBook\n",
    "\n",
    "**Prerequisites:** Run a cloud backtest first to populate ObjectStore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sps\n",
    "import re\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "qb = QuantBook()\n",
    "print(\"QuantBook initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_store(key):\n",
    "    \"\"\"Read a CSV from ObjectStore with existence check and error handling.\"\"\"\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADING_DAYS = 252\n",
    "\n",
    "# --- Load CSVs ---\n",
    "df_snapshots = read_csv_from_store(f\"{TEAM_ID}/daily_snapshots.csv\")\n",
    "df_positions = read_csv_from_store(f\"{TEAM_ID}/positions.csv\")\n",
    "df_signals   = read_csv_from_store(f\"{TEAM_ID}/signals.csv\")\n",
    "df_slippage  = read_csv_from_store(f\"{TEAM_ID}/slippage.csv\")\n",
    "df_trades    = read_csv_from_store(f\"{TEAM_ID}/trades.csv\")\n",
    "df_targets   = read_csv_from_store(f\"{TEAM_ID}/targets.csv\")\n",
    "df_orders    = read_csv_from_store(f\"{TEAM_ID}/order_events.csv\")\n",
    "\n",
    "# --- Validate the minimum requirement ---\n",
    "if df_snapshots is None:\n",
    "    raise ValueError(\"daily_snapshots.csv is required. Run a backtest first.\")\n",
    "\n",
    "# --- Parse dates and sort ---\n",
    "for name, frame in [('df_snapshots', df_snapshots),\n",
    "                    ('df_positions', df_positions),\n",
    "                    ('df_signals',   df_signals),\n",
    "                    ('df_slippage',  df_slippage),\n",
    "                    ('df_trades',    df_trades),\n",
    "                    ('df_targets',   df_targets),\n",
    "                    ('df_orders',    df_orders)]:\n",
    "    if frame is not None and 'date' in frame.columns:\n",
    "        frame['date'] = pd.to_datetime(frame['date'])\n",
    "        frame.sort_values('date', inplace=True)\n",
    "        frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_snapshots['daily_return'] = df_snapshots['nav'].pct_change()\n",
    "\n",
    "print(f\"Snapshots: {len(df_snapshots)} rows, \"\n",
    "      f\"{df_snapshots['date'].min():%Y-%m-%d} to {df_snapshots['date'].max():%Y-%m-%d}\")\n",
    "for label, frame in [('Positions', df_positions), ('Signals', df_signals),\n",
    "                     ('Slippage', df_slippage), ('Trades', df_trades),\n",
    "                     ('Targets', df_targets), ('Orders', df_orders)]:\n",
    "    print(f\"{label}: {len(frame) if frame is not None else 'NOT FOUND'} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SGOV Risk-Free Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgov_symbol = qb.AddEquity(\"SGOV\", Resolution.Daily).Symbol\n",
    "\n",
    "sgov_start = df_snapshots['date'].min() - pd.Timedelta(days=10)\n",
    "sgov_end   = df_snapshots['date'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "sgov_history = qb.History(sgov_symbol, sgov_start, sgov_end, Resolution.Daily)\n",
    "sgov_prices  = sgov_history['close'].reset_index()\n",
    "\n",
    "# Normalize tz-aware timestamps from QuantBook\n",
    "date_col = 'time' if 'time' in sgov_prices.columns else sgov_prices.columns[0]\n",
    "sgov_prices = sgov_prices.rename(columns={date_col: 'date', 'close': 'sgov_close'})\n",
    "sgov_prices['date'] = pd.to_datetime(sgov_prices['date']).dt.tz_localize(None).dt.normalize()\n",
    "sgov_prices['sgov_daily_rf'] = sgov_prices['sgov_close'].pct_change()\n",
    "\n",
    "df_snapshots = df_snapshots.merge(sgov_prices[['date', 'sgov_daily_rf']], on='date', how='left')\n",
    "df_snapshots['sgov_daily_rf'] = df_snapshots['sgov_daily_rf'].ffill().fillna(0.0)\n",
    "\n",
    "RISK_FREE_RATE = df_snapshots['sgov_daily_rf'].mean() * TRADING_DAYS\n",
    "df_snapshots['excess_return'] = df_snapshots['daily_return'] - df_snapshots['sgov_daily_rf']\n",
    "\n",
    "returns = df_snapshots['daily_return'].dropna()\n",
    "excess_returns = df_snapshots['excess_return'].dropna()\n",
    "\n",
    "print(f\"Effective annualized risk-free rate (SGOV): {RISK_FREE_RATE * 100:.2f}%\")\n",
    "print(f\"Return observations: {len(returns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SPY Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "spy_symbol = qb.AddEquity('SPY', Resolution.Daily).Symbol\n\nspy_start = df_snapshots['date'].min()\nspy_end   = df_snapshots['date'].max() + pd.Timedelta(days=1)\n\nspy_hist = qb.History(spy_symbol, spy_start, spy_end, Resolution.Daily)\n\n# --- Handle QuantBook's MultiIndex return format ---\nspy_df = spy_hist.copy()\nif isinstance(spy_df.index, pd.MultiIndex):\n    for level in range(spy_df.index.nlevels):\n        try:\n            spy_df = spy_df.xs(spy_symbol, level=level)\n            break\n        except Exception:\n            continue\n\nif 'close' in spy_df.columns:\n    spy_close = spy_df['close']\nelif 'value' in spy_df.columns:\n    spy_close = spy_df['value']\nelse:\n    raise ValueError('SPY history does not include close/value price column.')\n\nspy_close = pd.Series(spy_close)\nspy_close.index = pd.to_datetime(spy_close.index)\nif getattr(spy_close.index, 'tz', None) is not None:\n    spy_close.index = spy_close.index.tz_localize(None)\nspy_close.index = spy_close.index.normalize()\nspy_close = spy_close.groupby(spy_close.index).last().sort_index()\n\ndf_spy = pd.DataFrame({'date': spy_close.index, 'spy_close': spy_close.values})\ndf_spy['spy_return'] = df_spy['spy_close'].pct_change()\ndf_spy = df_spy.dropna(subset=['spy_return']).copy()\n\n# --- Merge onto snapshots ---\nmerged = df_snapshots[['date', 'daily_return']].merge(\n    df_spy[['date', 'spy_return']],\n    on='date', how='inner'\n).dropna().sort_values('date').reset_index(drop=True)\n\nprint(f\"SPY overlap: {len(merged)} trading days\")\nif len(merged) == 0:\n    print(\"WARNING: No date overlap between snapshots and SPY.\")\n    print(f\"  Snapshot dates sample: {df_snapshots['date'].head(3).tolist()}\")\n    print(f\"  SPY dates sample:     {df_spy['date'].head(3).tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(rets, risk_free_rate=0.0, periods_per_year=252):\n",
    "    daily_rf = risk_free_rate / periods_per_year\n",
    "    excess = rets - daily_rf\n",
    "    if excess.std() == 0:\n",
    "        return np.nan\n",
    "    return (excess.mean() / excess.std()) * np.sqrt(periods_per_year)\n",
    "\n",
    "\n",
    "def sortino_ratio(rets, risk_free_rate=0.0, periods_per_year=252):\n",
    "    daily_rf = risk_free_rate / periods_per_year\n",
    "    excess = rets - daily_rf\n",
    "    downside = excess[excess < 0]\n",
    "    downside_std = np.sqrt((downside ** 2).mean())\n",
    "    if downside_std == 0:\n",
    "        return np.nan\n",
    "    return (excess.mean() / downside_std) * np.sqrt(periods_per_year)\n",
    "\n",
    "\n",
    "def calmar_ratio(rets, periods_per_year=252):\n",
    "    cumulative = (1 + rets).cumprod()\n",
    "    total_return = cumulative.iloc[-1] - 1\n",
    "    years = len(rets) / periods_per_year\n",
    "    ann_return = (1 + total_return) ** (1 / years) - 1\n",
    "    running_max = cumulative.cummax()\n",
    "    max_dd = abs(((cumulative / running_max) - 1).min())\n",
    "    if max_dd == 0:\n",
    "        return np.nan\n",
    "    return ann_return / max_dd\n",
    "\n",
    "\n",
    "def max_drawdown(rets):\n",
    "    cumulative = (1 + rets).cumprod()\n",
    "    running_max = cumulative.cummax()\n",
    "    return abs(((cumulative / running_max) - 1).min())\n",
    "\n",
    "\n",
    "def probabilistic_sharpe_ratio(rets, sr, benchmark_sr=0.0):\n",
    "    n = len(rets)\n",
    "    skew = sps.skew(rets)\n",
    "    kurt = sps.kurtosis(rets)\n",
    "    variance = (1 - skew * sr + ((kurt - 1) / 4) * sr ** 2) / (n - 1)\n",
    "    if variance <= 0:\n",
    "        return np.nan\n",
    "    return sps.norm.cdf((sr - benchmark_sr) / np.sqrt(variance))\n",
    "\n",
    "\n",
    "def tier_from_mag(m):\n",
    "    \"\"\"Classify signal magnitude into tier.\"\"\"\n",
    "    if pd.isna(m):  return 'unknown'\n",
    "    if m >= 0.7:    return 'strong'\n",
    "    if m >= 0.3:    return 'moderate'\n",
    "    return 'weak'\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Performance Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Performance Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Monthly returns for monthly stats ---\n",
    "df_snapshots['year']  = df_snapshots['date'].dt.year\n",
    "df_snapshots['month'] = df_snapshots['date'].dt.month\n",
    "\n",
    "monthly = df_snapshots.groupby(['year', 'month']).agg(\n",
    "    nav_start=('nav', 'first'),\n",
    "    nav_end=('nav', 'last')\n",
    ")\n",
    "monthly['monthly_return'] = (monthly['nav_end'] / monthly['nav_start']) - 1\n",
    "monthly = monthly.reset_index()\n",
    "\n",
    "# --- Compute all metrics ---\n",
    "total_return    = (df_snapshots['nav'].iloc[-1] / df_snapshots['nav'].iloc[0]) - 1\n",
    "years           = len(returns) / TRADING_DAYS\n",
    "ann_return      = (1 + total_return) ** (1 / years) - 1\n",
    "ann_vol         = returns.std() * np.sqrt(TRADING_DAYS)\n",
    "sr              = sharpe_ratio(returns, RISK_FREE_RATE, TRADING_DAYS)\n",
    "sort            = sortino_ratio(returns, RISK_FREE_RATE, TRADING_DAYS)\n",
    "cal             = calmar_ratio(returns, TRADING_DAYS)\n",
    "mdd             = max_drawdown(returns)\n",
    "psr_zero        = probabilistic_sharpe_ratio(returns, sr, benchmark_sr=0.0)\n",
    "psr_one         = probabilistic_sharpe_ratio(returns, sr, benchmark_sr=1.0)\n",
    "\n",
    "win_loss_daily  = abs(returns[returns > 0].mean() / returns[returns < 0].mean())\n",
    "\n",
    "metrics = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Period',\n",
    "        'Total Return', 'CAGR', 'Annualized Volatility',\n",
    "        'Sharpe Ratio', 'Sortino Ratio', 'Calmar Ratio',\n",
    "        'PSR (SR* = 0)', 'PSR (SR* = 1)',\n",
    "        'Maximum Drawdown',\n",
    "        'Best Day', 'Worst Day', 'Best Month', 'Worst Month',\n",
    "        'Win Rate (Daily)', 'Win Rate (Monthly)',\n",
    "        'Avg Win / Avg Loss (Daily)',\n",
    "        'Skewness (Daily)', 'Kurtosis (Daily)',\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{df_snapshots['date'].min():%Y-%m-%d} to {df_snapshots['date'].max():%Y-%m-%d}\",\n",
    "        f\"{total_return * 100:.2f}%\",\n",
    "        f\"{ann_return * 100:.2f}%\",\n",
    "        f\"{ann_vol * 100:.2f}%\",\n",
    "        f\"{sr:.4f}\", f\"{sort:.4f}\", f\"{cal:.4f}\",\n",
    "        f\"{psr_zero:.4f}\", f\"{psr_one:.4f}\",\n",
    "        f\"{mdd * 100:.2f}%\",\n",
    "        f\"{returns.max() * 100:.4f}%\", f\"{returns.min() * 100:.4f}%\",\n",
    "        f\"{monthly['monthly_return'].max() * 100:.2f}%\",\n",
    "        f\"{monthly['monthly_return'].min() * 100:.2f}%\",\n",
    "        f\"{(returns > 0).mean() * 100:.1f}%\",\n",
    "        f\"{(monthly['monthly_return'] > 0).mean() * 100:.1f}%\",\n",
    "        f\"{win_loss_daily:.2f}\",\n",
    "        f\"{returns.skew():.4f}\", f\"{returns.kurtosis():.4f}\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Cumulative Return + Underwater Drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Strategy cumulative return ---\n",
    "df_snapshots['cumulative_return'] = (1 + df_snapshots['daily_return'].fillna(0)).cumprod()\n",
    "df_snapshots['running_max'] = df_snapshots['cumulative_return'].cummax()\n",
    "df_snapshots['drawdown'] = (df_snapshots['cumulative_return'] / df_snapshots['running_max']) - 1\n",
    "\n",
    "# --- SPY cumulative return (aligned to same start date) ---\n",
    "spy_aligned = df_spy[df_spy['date'] >= df_snapshots['date'].min()].copy()\n",
    "spy_aligned['spy_cumulative'] = (1 + spy_aligned['spy_return'].fillna(0)).cumprod()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# -- Top: cumulative return --\n",
    "axes[0].plot(df_snapshots['date'],\n",
    "             (df_snapshots['cumulative_return'] - 1) * 100,\n",
    "             linewidth=2, color='steelblue', label='Strategy')\n",
    "axes[0].plot(spy_aligned['date'],\n",
    "             (spy_aligned['spy_cumulative'] - 1) * 100,\n",
    "             linewidth=1.5, color='grey', alpha=0.7, label='SPY')\n",
    "axes[0].fill_between(df_snapshots['date'], 0,\n",
    "                     (df_snapshots['cumulative_return'] - 1) * 100,\n",
    "                     alpha=0.15, color='steelblue')\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0].set_title('Cumulative Return', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Cumulative Return (%)')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# -- Bottom: underwater --\n",
    "axes[1].fill_between(df_snapshots['date'], 0,\n",
    "                     df_snapshots['drawdown'] * 100,\n",
    "                     color='red', alpha=0.5)\n",
    "axes[1].plot(df_snapshots['date'], df_snapshots['drawdown'] * 100,\n",
    "             linewidth=1, color='darkred')\n",
    "axes[1].set_title('Underwater Plot (Drawdown)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Drawdown (%)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# -- Annotate max drawdown --\n",
    "worst_idx = df_snapshots['drawdown'].idxmin()\n",
    "worst_date = df_snapshots.loc[worst_idx, 'date']\n",
    "worst_dd   = df_snapshots.loc[worst_idx, 'drawdown'] * 100\n",
    "axes[1].annotate(f'Max DD: {worst_dd:.1f}%',\n",
    "                 xy=(worst_date, worst_dd),\n",
    "                 xytext=(worst_date + pd.Timedelta(days=30), worst_dd - 1),\n",
    "                 fontsize=10, color='darkred',\n",
    "                 arrowprops=dict(arrowstyle='->', color='darkred'))\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3: Monthly Return Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = monthly.pivot(index='year', columns='month', values='monthly_return') * 100\n",
    "\n",
    "# Dynamically label only the months that appear\n",
    "month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "heatmap_data.columns = [month_labels[m - 1] for m in heatmap_data.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, max(4, len(heatmap_data) * 0.8)))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', center=0,\n",
    "            linewidths=1, ax=ax, cbar_kws={'label': 'Return (%)'})\n",
    "ax.set_title('Monthly Returns (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Year')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 4: Rolling Sharpe, Sortino, Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = TRADING_DAYS  # 252\n",
    "\n",
    "sgov_rf = df_snapshots['sgov_daily_rf'].iloc[1:].reset_index(drop=True)\n",
    "ret_vals = returns.reset_index(drop=True)\n",
    "excess_ser = pd.Series(ret_vals.values - sgov_rf.values, index=ret_vals.index)\n",
    "plot_dates = df_snapshots['date'].iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# -- Rolling Sharpe --\n",
    "rolling_sharpe = (excess_ser.rolling(window).mean()\n",
    "                  / ret_vals.rolling(window).std()) * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "# -- Rolling Sortino --\n",
    "sgov_rf_arr = sgov_rf.values\n",
    "\n",
    "def _rolling_sortino(x):\n",
    "    idx = x.index\n",
    "    excess = x.values - sgov_rf_arr[idx]\n",
    "    downside = excess[excess < 0]\n",
    "    if len(downside) == 0:\n",
    "        return np.nan\n",
    "    ds_std = np.sqrt((downside ** 2).mean())\n",
    "    if ds_std == 0:\n",
    "        return np.nan\n",
    "    return (excess.mean() / ds_std) * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "rolling_sortino = ret_vals.rolling(window).apply(_rolling_sortino, raw=False)\n",
    "\n",
    "# -- Rolling Volatility --\n",
    "rolling_vol = ret_vals.rolling(window).std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 14), sharex=True)\n",
    "\n",
    "axes[0].plot(plot_dates, rolling_sharpe, linewidth=2, color='steelblue')\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=1, color='green', linestyle='--', alpha=0.5, label='Sharpe = 1')\n",
    "axes[0].set_title(f'Rolling {window}-Day Sharpe Ratio (Risk-Free: SGOV)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Sharpe Ratio')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(plot_dates, rolling_sortino, linewidth=2, color='coral')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=1, color='green', linestyle='--', alpha=0.5, label='Sortino = 1')\n",
    "axes[1].set_title(f'Rolling {window}-Day Sortino Ratio (Risk-Free: SGOV)',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Sortino Ratio')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(plot_dates, rolling_vol * 100, linewidth=2, color='#1f77b4', label='Realized Vol')\n",
    "axes[2].axhline(y=10, color='green', linestyle='--', alpha=0.5, label='10% Target')\n",
    "axes[2].set_title(f'Rolling {window}-Day Annualized Volatility',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Volatility (%)')\n",
    "axes[2].legend(loc='upper left')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 5: Rolling Beta + Scatter vs SPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(merged) < 10:\n    print(f\"Insufficient SPY overlap ({len(merged)} days) — skipping beta/scatter plot.\")\nelse:\n    # --- Rolling beta ---\n    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n\n    for win, color, label in [(60, 'coral', '60-Day'), (TRADING_DAYS, 'steelblue', '252-Day')]:\n        cov  = merged['daily_return'].rolling(win).cov(merged['spy_return'])\n        var  = merged['spy_return'].rolling(win).var()\n        beta = cov / var\n        axes[0].plot(merged['date'], beta, linewidth=1.5, color=color, label=f'{label} Beta')\n\n    axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    axes[0].axhline(y=1, color='red', linestyle='--', alpha=0.3, label='Beta = 1')\n    axes[0].set_title('Rolling Beta to SPY', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('Beta')\n    axes[0].legend(loc='upper left')\n    axes[0].grid(True, alpha=0.3)\n\n    # --- Full-period regression stats ---\n    x = merged['spy_return'].values\n    y = merged['daily_return'].values\n    slope, intercept, r_value, p_value, std_err = sps.linregress(x, y)\n    ann_alpha = intercept * TRADING_DAYS\n    corr = np.corrcoef(x, y)[0, 1]\n\n    # Up/down market beta\n    up_mask   = merged['spy_return'] > 0\n    down_mask = merged['spy_return'] < 0\n    up_beta   = np.cov(y[up_mask], x[up_mask])[0, 1] / np.var(x[up_mask]) if up_mask.sum() > 2 else np.nan\n    down_beta = np.cov(y[down_mask], x[down_mask])[0, 1] / np.var(x[down_mask]) if down_mask.sum() > 2 else np.nan\n\n    # --- Scatter ---\n    axes[1].scatter(x * 100, y * 100, alpha=0.3, s=10, color='steelblue')\n    fit_x = np.linspace(x.min(), x.max(), 100)\n    axes[1].plot(fit_x * 100, (slope * fit_x + intercept) * 100,\n                 color='crimson', linewidth=2, label='OLS Fit')\n    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    axes[1].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n    axes[1].set_title('Daily Returns: Strategy vs SPY', fontsize=14, fontweight='bold')\n    axes[1].set_xlabel('SPY Daily Return (%)')\n    axes[1].set_ylabel('Strategy Daily Return (%)')\n    axes[1].legend(loc='upper left')\n    axes[1].grid(True, alpha=0.3)\n\n    # Annotation box\n    stats_text = (f\"Beta: {slope:.3f}\\n\"\n                  f\"Alpha (ann): {ann_alpha * 100:.2f}%\\n\"\n                  f\"Correlation: {corr:.3f}\\n\"\n                  f\"R\\u00b2: {r_value**2:.3f}\\n\"\n                  f\"Up Beta: {up_beta:.3f}\\n\"\n                  f\"Down Beta: {down_beta:.3f}\")\n    axes[1].text(0.02, 0.97, stats_text, transform=axes[1].transAxes,\n                 fontsize=10, verticalalignment='top',\n                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exposure & Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 6: Exposure Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(df_snapshots['date'], df_snapshots['gross_exposure'],\n",
    "        linewidth=1.5, color='#1f77b4', label='Gross')\n",
    "ax.plot(df_snapshots['date'], df_snapshots['net_exposure'],\n",
    "        linewidth=1.5, color='#d62728', label='Net')\n",
    "ax.plot(df_snapshots['date'], df_snapshots['long_exposure'],\n",
    "        linewidth=1.5, color='#2ca02c', label='Long')\n",
    "ax.plot(df_snapshots['date'], df_snapshots['short_exposure'],\n",
    "        linewidth=1.5, color='#9467bd', label='Short')\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_title('Portfolio Exposure Over Time', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Exposure (fraction of NAV)')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- Summary stats --\n",
    "print(\"\\nExposure Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for col in ['gross_exposure', 'net_exposure', 'long_exposure', 'short_exposure']:\n",
    "    vals = df_snapshots[col]\n",
    "    print(f\"  {col:20s}  mean={vals.mean():.3f}  min={vals.min():.3f}  max={vals.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 7: VaR / CVaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Static VaR / CVaR ---\n",
    "confidence_levels = [0.95, 0.99]\n",
    "var_cvar_rows = []\n",
    "for cl in confidence_levels:\n",
    "    var = np.percentile(returns, (1 - cl) * 100)\n",
    "    cvar = returns[returns <= var].mean()\n",
    "    var_cvar_rows.append({\n",
    "        'Confidence': f'{cl * 100:.0f}%',\n",
    "        'VaR (daily)':  f'{var * 100:.4f}%',\n",
    "        'CVaR (daily)': f'{cvar * 100:.4f}%',\n",
    "    })\n",
    "display(pd.DataFrame(var_cvar_rows))\n",
    "\n",
    "# --- Rolling VaR (95%) ---\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "for win, color in [(20, 'coral'), (60, '#1f77b4'), (TRADING_DAYS, 'steelblue')]:\n",
    "    rolling_var = returns.rolling(win).quantile(0.05)\n",
    "    ax.plot(df_snapshots['date'].iloc[1:].reset_index(drop=True),\n",
    "            rolling_var * 100, linewidth=1.5, color=color, label=f'{win}d VaR (95%)')\n",
    "\n",
    "ax.set_title('Rolling 95% VaR (Daily)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('VaR (%)')\n",
    "ax.legend(loc='lower left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 8: Return Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Daily\n",
    "axes[0].hist(returns * 100, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].axvline(x=returns.mean() * 100, color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {returns.mean() * 100:.4f}%')\n",
    "axes[0].set_title('Daily Return Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Daily Return (%)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly\n",
    "axes[1].hist(monthly['monthly_return'] * 100, bins=20, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(x=monthly['monthly_return'].mean() * 100, color='green', linestyle='--',\n",
    "                linewidth=2, label=f'Mean: {monthly[\"monthly_return\"].mean() * 100:.2f}%')\n",
    "axes[1].set_title('Monthly Return Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Monthly Return (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 9: Concentration Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is not None:\n",
    "    pos = df_positions[df_positions['invested'] == True].copy()\n",
    "    pos['abs_weight'] = pos['weight'].abs()\n",
    "\n",
    "    daily_conc = []\n",
    "    for date, group in pos.groupby('date'):\n",
    "        total = group['abs_weight'].sum()\n",
    "        if total == 0:\n",
    "            continue\n",
    "        shares = (group['abs_weight'] / total).sort_values(ascending=False)\n",
    "        hhi = (shares ** 2).sum()\n",
    "        daily_conc.append({\n",
    "            'date': date,\n",
    "            'top_1': shares.iloc[0] if len(shares) >= 1 else np.nan,\n",
    "            'top_3': shares.iloc[:3].sum() if len(shares) >= 3 else shares.sum(),\n",
    "            'top_5': shares.iloc[:5].sum() if len(shares) >= 5 else shares.sum(),\n",
    "            'hhi': hhi,\n",
    "            'eff_n': 1.0 / hhi if hhi > 0 else np.nan,\n",
    "            'n_positions': len(group),\n",
    "        })\n",
    "    conc = pd.DataFrame(daily_conc)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 14), sharex=True)\n",
    "\n",
    "    for col, label, color in [('top_1', 'Top 1', '#d62728'),\n",
    "                               ('top_3', 'Top 3', '#1f77b4'),\n",
    "                               ('top_5', 'Top 5', '#2ca02c')]:\n",
    "        axes[0].plot(conc['date'], conc[col] * 100, linewidth=1.5, color=color, label=label)\n",
    "    axes[0].set_title('Concentration: Top-N Share of Gross Exposure',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Share (%)')\n",
    "    axes[0].legend(loc='upper right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(conc['date'], conc['hhi'], linewidth=1.5, color='steelblue')\n",
    "    axes[1].axhline(y=0.12, color='red', linestyle='--', alpha=0.5, label='HHI = 0.12 threshold')\n",
    "    axes[1].set_title('Herfindahl-Hirschman Index (HHI)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('HHI')\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].plot(conc['date'], conc['eff_n'], linewidth=1.5, color='#2ca02c')\n",
    "    axes[2].set_title('Effective Number of Bets (1 / HHI)', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].set_ylabel('Effective N')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"positions.csv not available — skipping concentration plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 14: Position Count Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(df_snapshots['date'], df_snapshots['num_positions'],\n",
    "        linewidth=1.5, color='steelblue')\n",
    "ax.fill_between(df_snapshots['date'], 0, df_snapshots['num_positions'],\n",
    "                alpha=0.15, color='steelblue')\n",
    "ax.set_title('Number of Open Positions', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Position Count')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 15: Estimated vs Realized Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ser = df_snapshots['daily_return'].dropna()\n",
    "realized_vol_60 = ret_ser.rolling(60).std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(df_snapshots['date'], df_snapshots['estimated_vol'] * 100,\n",
    "        linewidth=1.5, color='coral', label='Estimated Vol (PCM)')\n",
    "ax.plot(df_snapshots['date'].iloc[1:].reset_index(drop=True),\n",
    "        realized_vol_60 * 100,\n",
    "        linewidth=1.5, color='steelblue', label='Realized Vol (60d)')\n",
    "ax.axhline(y=10, color='green', linestyle='--', linewidth=1.5, alpha=0.7, label='10% Target')\n",
    "\n",
    "ax.set_title('Estimated vs Realized Volatility', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Annualized Volatility (%)')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 16: Drawdown Episode Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'drawdown' not in df_snapshots.columns:\n",
    "    df_snapshots['cumulative_return'] = (1 + df_snapshots['daily_return'].fillna(0)).cumprod()\n",
    "    df_snapshots['running_max'] = df_snapshots['cumulative_return'].cummax()\n",
    "    df_snapshots['drawdown'] = (df_snapshots['cumulative_return'] / df_snapshots['running_max']) - 1\n",
    "\n",
    "df_snapshots['in_drawdown'] = df_snapshots['drawdown'] < 0\n",
    "df_snapshots['drawdown_start'] = (\n",
    "    df_snapshots['in_drawdown'] & ~df_snapshots['in_drawdown'].shift(1).fillna(False)\n",
    ")\n",
    "df_snapshots['drawdown_end'] = (\n",
    "    ~df_snapshots['in_drawdown'] & df_snapshots['in_drawdown'].shift(1).fillna(False)\n",
    ")\n",
    "\n",
    "drawdown_periods = []\n",
    "start_date = None\n",
    "\n",
    "for idx, row in df_snapshots.iterrows():\n",
    "    if row['drawdown_start']:\n",
    "        start_date = row['date']\n",
    "    if row['drawdown_end'] and start_date is not None:\n",
    "        episode = df_snapshots[\n",
    "            (df_snapshots['date'] >= start_date) & (df_snapshots['date'] <= row['date'])\n",
    "        ]\n",
    "        trough_idx = episode['drawdown'].idxmin()\n",
    "        drawdown_periods.append({\n",
    "            'start': start_date,\n",
    "            'trough': df_snapshots.loc[trough_idx, 'date'],\n",
    "            'recovery': row['date'],\n",
    "            'duration_days': (row['date'] - start_date).days,\n",
    "            'max_drawdown_pct': episode['drawdown'].min() * 100,\n",
    "        })\n",
    "        start_date = None\n",
    "\n",
    "if drawdown_periods:\n",
    "    dd_df = pd.DataFrame(drawdown_periods).sort_values('max_drawdown_pct')\n",
    "    print(\"Top Drawdown Episodes (sorted by severity):\")\n",
    "    print(\"=\" * 80)\n",
    "    display(dd_df.head(10))\n",
    "    print(f\"\\nAverage drawdown duration: {dd_df['duration_days'].mean():.1f} days\")\n",
    "    print(f\"Maximum drawdown duration: {dd_df['duration_days'].max()} days\")\n",
    "    print(f\"Total completed drawdown episodes: {len(dd_df)}\")\n",
    "else:\n",
    "    print(\"No completed drawdown periods found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 17: Exposure Regime Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROSS_HIGH = 1.20\n",
    "GROSS_LOW  = 0.80\n",
    "NET_BULL   = 0.30\n",
    "NET_BEAR   = -0.30\n",
    "\n",
    "for c in ['gross_exposure', 'net_exposure', 'long_exposure', 'short_exposure', 'nav']:\n",
    "    df_snapshots[c] = pd.to_numeric(df_snapshots[c], errors='coerce')\n",
    "\n",
    "def classify_regime(row):\n",
    "    gross = row['gross_exposure']\n",
    "    net   = row['net_exposure']\n",
    "    gross_bucket = 'high_gross' if gross >= GROSS_HIGH else ('low_gross' if gross <= GROSS_LOW else 'mid_gross')\n",
    "    net_bucket   = 'net_long'   if net >= NET_BULL    else ('net_short'  if net <= NET_BEAR   else 'market_neutral')\n",
    "    return f'{gross_bucket}__{net_bucket}'\n",
    "\n",
    "df_snapshots['regime'] = df_snapshots.apply(classify_regime, axis=1)\n",
    "df_snapshots['risk_state'] = np.select(\n",
    "    [df_snapshots['gross_exposure'] >= GROSS_HIGH,\n",
    "     df_snapshots['gross_exposure'] <= GROSS_LOW],\n",
    "    ['risk_on', 'risk_off'],\n",
    "    default='balanced'\n",
    ")\n",
    "\n",
    "# --- Scorecard ---\n",
    "gross_breach = (df_snapshots['gross_exposure'] > GROSS_HIGH).mean()\n",
    "gross_under  = (df_snapshots['gross_exposure'] < GROSS_LOW).mean()\n",
    "net_long_pct = (df_snapshots['net_exposure'] > NET_BULL).mean()\n",
    "net_short_pct = (df_snapshots['net_exposure'] < NET_BEAR).mean()\n",
    "\n",
    "print('Exposure Regime Scorecard:')\n",
    "print('=' * 60)\n",
    "print(f'  Gross > {GROSS_HIGH * 100:.0f}% NAV: {gross_breach * 100:.2f}% of days')\n",
    "print(f'  Gross < {GROSS_LOW * 100:.0f}% NAV: {gross_under * 100:.2f}% of days')\n",
    "print(f'  Net > +{NET_BULL * 100:.0f}% NAV: {net_long_pct * 100:.2f}% of days')\n",
    "print(f'  Net < -{abs(NET_BEAR) * 100:.0f}% NAV: {net_short_pct * 100:.2f}% of days')\n",
    "\n",
    "risk_state_counts = (\n",
    "    df_snapshots.groupby('risk_state', as_index=False)\n",
    "      .agg(days=('risk_state', 'size'))\n",
    "      .sort_values('days', ascending=False)\n",
    ")\n",
    "risk_state_counts['pct_days'] = risk_state_counts['days'] / len(df_snapshots)\n",
    "print('\\nRisk State Distribution:')\n",
    "display(risk_state_counts)\n",
    "\n",
    "# --- Return / Vol / Sharpe by risk state ---\n",
    "if df_snapshots['daily_return'].notna().sum() > 30:\n",
    "    by_state = (\n",
    "        df_snapshots.groupby('risk_state', as_index=False)\n",
    "          .agg(\n",
    "              observations=('daily_return', lambda s: s.notna().sum()),\n",
    "              avg_daily_return=('daily_return', 'mean'),\n",
    "              annualized_return=('daily_return',\n",
    "                  lambda s: (1 + s.dropna()).prod() ** (TRADING_DAYS / max(len(s.dropna()), 1)) - 1),\n",
    "              annualized_vol=('daily_return',\n",
    "                  lambda s: s.dropna().std() * np.sqrt(TRADING_DAYS)),\n",
    "          )\n",
    "    )\n",
    "    by_state['sharpe'] = np.where(\n",
    "        by_state['annualized_vol'].abs() > 1e-12,\n",
    "        by_state['annualized_return'] / by_state['annualized_vol'],\n",
    "        np.nan\n",
    "    )\n",
    "    print('\\nPerformance by Risk State:')\n",
    "    display(by_state)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    colors = {'risk_on': '#d62728', 'balanced': '#1f77b4', 'risk_off': '#2ca02c'}\n",
    "    bar_colors = [colors.get(s, '#999999') for s in by_state['risk_state']]\n",
    "    ax.bar(by_state['risk_state'], by_state['annualized_vol'] * 100, color=bar_colors)\n",
    "    ax.set_title('Annualized Volatility by Risk State', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Annualized Vol (%)')\n",
    "    ax.set_xlabel('Risk State')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Execution & Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 10: Slippage Distribution + Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_slippage is not None and len(df_slippage) > 0:\n",
    "    df_slippage['slippage_bps'] = (\n",
    "        df_slippage['slippage_dollars']\n",
    "        / (df_slippage['expected_price'] * df_slippage['quantity'].abs())\n",
    "        * 10000\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].hist(df_slippage['slippage_bps'].dropna(), bins=50,\n",
    "                 color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "    med_bps = df_slippage['slippage_bps'].median()\n",
    "    axes[0].axvline(x=med_bps, color='green', linestyle='--', linewidth=2,\n",
    "                    label=f'Median: {med_bps:.1f} bps')\n",
    "    axes[0].set_title('Slippage Distribution (per fill)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Slippage (bps)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    daily_slip = df_slippage.groupby('date')['slippage_dollars'].sum().reset_index()\n",
    "    daily_slip = daily_slip.sort_values('date')\n",
    "    daily_slip['cumulative_slippage'] = daily_slip['slippage_dollars'].cumsum()\n",
    "\n",
    "    axes[1].plot(daily_slip['date'], daily_slip['cumulative_slippage'],\n",
    "                 linewidth=2, color='coral')\n",
    "    axes[1].fill_between(daily_slip['date'], 0, daily_slip['cumulative_slippage'],\n",
    "                         alpha=0.2, color='coral')\n",
    "    axes[1].set_title('Cumulative Slippage ($)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Cumulative Slippage ($)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nSlippage Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Total fills: {len(df_slippage)}\")\n",
    "    print(f\"  Total slippage: ${df_slippage['slippage_dollars'].sum():,.2f}\")\n",
    "    print(f\"  Mean slippage: {df_slippage['slippage_bps'].mean():.1f} bps\")\n",
    "    print(f\"  Median slippage: {df_slippage['slippage_bps'].median():.1f} bps\")\n",
    "else:\n",
    "    print(\"slippage.csv not available — skipping slippage plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 13: Turnover Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is not None:\n",
    "    weights = df_positions.pivot_table(index='date', columns='symbol',\n",
    "                                       values='weight', fill_value=0)\n",
    "    daily_turnover = weights.diff().abs().sum(axis=1) / 2\n",
    "    daily_turnover = daily_turnover.reset_index()\n",
    "    daily_turnover.columns = ['date', 'half_turn']\n",
    "\n",
    "    daily_turnover['weekly_turn'] = daily_turnover['half_turn'].rolling(5).sum()\n",
    "\n",
    "    trading_days_total = len(daily_turnover)\n",
    "    years_total = trading_days_total / TRADING_DAYS\n",
    "    ann_turnover = daily_turnover['half_turn'].sum() / years_total if years_total > 0 else 0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(daily_turnover['date'], daily_turnover['weekly_turn'] * 100,\n",
    "            linewidth=1.5, color='steelblue')\n",
    "    ax.fill_between(daily_turnover['date'], 0,\n",
    "                    daily_turnover['weekly_turn'] * 100,\n",
    "                    alpha=0.15, color='steelblue')\n",
    "    ax.set_title('Rolling 5-Day Turnover', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Turnover (% NAV)')\n",
    "    ax.text(0.02, 0.95, f'Annualized turnover: {ann_turnover * 100:.0f}%',\n",
    "            transform=ax.transAxes, fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"positions.csv not available — skipping turnover chart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 18: Order Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_orders is not None:\n",
    "    df_events = df_orders.copy()\n",
    "    for col in ['quantity', 'fill_quantity', 'fill_price', 'limit_price']:\n",
    "        if col in df_events.columns:\n",
    "            df_events[col] = pd.to_numeric(df_events[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "    def parse_tag_value(tag, key):\n",
    "        if pd.isna(tag):\n",
    "            return np.nan\n",
    "        m = re.search(rf'{key}=([^;]+)', str(tag))\n",
    "        return m.group(1) if m else np.nan\n",
    "\n",
    "    df_events['tier'] = df_events['tag'].apply(\n",
    "        lambda t: parse_tag_value(t, 'tier')).fillna('unknown')\n",
    "\n",
    "    grp = df_events.sort_values('date').groupby('order_id', as_index=False)\n",
    "    order_summary = grp.agg(\n",
    "        symbol=('symbol', 'first'),\n",
    "        tier=('tier', 'first'),\n",
    "        order_type=('order_type', 'first'),\n",
    "        quantity=('quantity', 'first'),\n",
    "        submitted_at=('date', 'min'),\n",
    "        final_at=('date', 'max')\n",
    "    )\n",
    "\n",
    "    final_status = (\n",
    "        df_events.sort_values('date')\n",
    "                 .groupby('order_id').tail(1)[['order_id', 'status']]\n",
    "                 .rename(columns={'status': 'final_status'})\n",
    "    )\n",
    "    fills = (\n",
    "        df_events.groupby('order_id', as_index=False)['fill_quantity']\n",
    "                 .sum().rename(columns={'fill_quantity': 'filled_qty'})\n",
    "    )\n",
    "\n",
    "    order_summary = order_summary.merge(final_status, on='order_id', how='left')\n",
    "    order_summary = order_summary.merge(fills, on='order_id', how='left')\n",
    "    order_summary['abs_qty'] = order_summary['quantity'].abs().replace(0, np.nan)\n",
    "    order_summary['fill_ratio'] = (\n",
    "        order_summary['filled_qty'].abs() / order_summary['abs_qty']\n",
    "    ).fillna(0.0).clip(0, 1)\n",
    "    order_summary['days_to_final'] = (\n",
    "        order_summary['final_at'] - order_summary['submitted_at']\n",
    "    ).dt.days.fillna(0)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "    status_counts = order_summary['final_status'].value_counts().sort_values(ascending=False)\n",
    "    status_counts.plot(kind='bar', ax=axes[0, 0], color='#1f77b4')\n",
    "    axes[0, 0].set_title('Final Order Status Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    sns.histplot(order_summary['fill_ratio'], bins=20, ax=axes[0, 1], color='#2ca02c')\n",
    "    axes[0, 1].set_title('Fill Ratio Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Fill Ratio')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    tier_order = ['strong', 'moderate', 'weak', 'exit', 'unknown']\n",
    "    tier_status = pd.crosstab(order_summary['tier'], order_summary['final_status'], normalize='index')\n",
    "    tier_status = tier_status.reindex([t for t in tier_order if t in tier_status.index]).dropna(how='all')\n",
    "    tier_status.plot(kind='bar', stacked=True, ax=axes[1, 0], colormap='tab20')\n",
    "    axes[1, 0].set_title('Final Status Mix by Tier', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Share')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    sns.boxplot(data=order_summary, x='tier', y='days_to_final',\n",
    "                order=[t for t in tier_order if t in order_summary['tier'].values],\n",
    "                ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Days to Final Status by Tier', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Tier')\n",
    "    axes[1, 1].set_ylabel('Days')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    cancel_rate = (\n",
    "        order_summary\n",
    "            .assign(is_canceled=order_summary['final_status']\n",
    "                                 .astype(str)\n",
    "                                 .str.contains('Canceled', case=False, na=False))\n",
    "            .groupby('tier', as_index=False)\n",
    "            .agg(orders=('order_id', 'count'),\n",
    "                 cancel_rate=('is_canceled', 'mean'),\n",
    "                 avg_fill_ratio=('fill_ratio', 'mean'))\n",
    "            .sort_values('orders', ascending=False)\n",
    "    )\n",
    "    print('\\nCancel Rate and Fill Ratio by Tier:')\n",
    "    display(cancel_rate)\n",
    "else:\n",
    "    print(\"order_events.csv not available — skipping order lifecycle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 19: Scaling Adherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_targets is not None:\n",
    "    tgt = df_targets.copy()\n",
    "    for col in ['start_w', 'weekly_target_w', 'scheduled_w', 'actual_w', 'scale_day']:\n",
    "        tgt[col] = pd.to_numeric(tgt[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "    if df_signals is not None:\n",
    "        sig = df_signals.copy()\n",
    "        sig['week_id'] = sig['date'].dt.strftime('%Y-%m-%d')\n",
    "        sig['magnitude'] = pd.to_numeric(sig['magnitude'], errors='coerce').fillna(0.0).abs()\n",
    "        sig = sig[['week_id', 'symbol', 'magnitude']].drop_duplicates(['week_id', 'symbol'])\n",
    "        tgt = tgt.merge(sig, on=['week_id', 'symbol'], how='left')\n",
    "    else:\n",
    "        tgt['magnitude'] = np.nan\n",
    "\n",
    "    tgt['tier'] = tgt['magnitude'].apply(tier_from_mag)\n",
    "\n",
    "    tgt['total_week_order_abs'] = (tgt['weekly_target_w'] - tgt['start_w']).abs()\n",
    "    tgt['planned_progress'] = np.where(\n",
    "        tgt['total_week_order_abs'] > 1e-10,\n",
    "        (tgt['scheduled_w'] - tgt['start_w']).abs() / tgt['total_week_order_abs'],\n",
    "        1.0\n",
    "    ).clip(0, 1)\n",
    "    tgt['actual_progress'] = np.where(\n",
    "        tgt['total_week_order_abs'] > 1e-10,\n",
    "        (tgt['actual_w'] - tgt['start_w']).abs() / tgt['total_week_order_abs'],\n",
    "        1.0\n",
    "    ).clip(0, 1)\n",
    "    tgt['progress_gap'] = tgt['actual_progress'] - tgt['planned_progress']\n",
    "\n",
    "    tgt = tgt.sort_values(['week_id', 'symbol', 'date'])\n",
    "    tgt['day_in_week'] = tgt.groupby(['week_id', 'symbol']).cumcount()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    tier_order = ['strong', 'moderate', 'weak', 'unknown']\n",
    "    sns.boxplot(data=tgt, x='tier', y='progress_gap',\n",
    "                order=[t for t in tier_order if t in tgt['tier'].values], ax=ax)\n",
    "    ax.axhline(0, color='black', linewidth=1)\n",
    "    ax.set_title('Progress Gap by Signal Tier (Actual - Planned)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Progress Gap')\n",
    "    ax.set_xlabel('Tier')\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    profile = (\n",
    "        tgt.groupby(['tier', 'day_in_week'], as_index=False)\n",
    "           .agg(planned=('planned_progress', 'mean'), actual=('actual_progress', 'mean'))\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharey=True)\n",
    "    sns.lineplot(data=profile, x='day_in_week', y='planned', hue='tier', marker='o', ax=axes[0])\n",
    "    axes[0].set_title('Planned Progress by Tier', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Day-in-Week')\n",
    "    axes[0].set_ylabel('Progress')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    sns.lineplot(data=profile, x='day_in_week', y='actual', hue='tier', marker='o', ax=axes[1])\n",
    "    axes[1].set_title('Actual Progress by Tier', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Day-in-Week')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    worst_lag = (\n",
    "        tgt.groupby('symbol', as_index=False)['progress_gap']\n",
    "           .mean().sort_values('progress_gap')\n",
    "    )\n",
    "    print('Most Lagging Symbols (most negative average gap):')\n",
    "    display(worst_lag.head(15))\n",
    "else:\n",
    "    print(\"targets.csv not available — skipping scaling adherence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 20: Stale Signal Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_targets is not None and df_positions is not None:\n",
    "    px = (\n",
    "        df_positions[['date', 'symbol', 'price', 'daily_total_net_pnl']]\n",
    "        .dropna(subset=['price'])\n",
    "        .drop_duplicates(['date', 'symbol'])\n",
    "        .sort_values(['symbol', 'date'])\n",
    "    )\n",
    "    for col in ['price', 'daily_total_net_pnl']:\n",
    "        px[col] = pd.to_numeric(px[col], errors='coerce')\n",
    "\n",
    "    tgt_stale = df_targets[['date', 'week_id', 'symbol',\n",
    "                             'start_w', 'weekly_target_w', 'actual_w']].drop_duplicates()\n",
    "    for col in ['start_w', 'weekly_target_w', 'actual_w']:\n",
    "        tgt_stale[col] = pd.to_numeric(tgt_stale[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "    m = tgt_stale.merge(px, on=['date', 'symbol'], how='left')\n",
    "\n",
    "    rebalance_px = (\n",
    "        m.sort_values('date')\n",
    "         .groupby(['week_id', 'symbol'], as_index=False)\n",
    "         .first()[['week_id', 'symbol', 'price']]\n",
    "         .rename(columns={'price': 'rebalance_price'})\n",
    "    )\n",
    "    m = m.merge(rebalance_px, on=['week_id', 'symbol'], how='left')\n",
    "\n",
    "    m['day_in_week'] = m.sort_values('date').groupby(['week_id', 'symbol']).cumcount()\n",
    "    m['signal_direction'] = np.sign(m['weekly_target_w']).replace(0, np.nan)\n",
    "    m['return_since_rebalance'] = (m['price'] / m['rebalance_price']) - 1.0\n",
    "    m['adverse_move'] = -m['signal_direction'] * m['return_since_rebalance']\n",
    "\n",
    "    if df_signals is not None:\n",
    "        sig = df_signals.copy()\n",
    "        sig['week_id'] = sig['date'].dt.strftime('%Y-%m-%d')\n",
    "        sig['mag_abs'] = pd.to_numeric(sig['magnitude'], errors='coerce').fillna(0.0).abs()\n",
    "        sig = sig[['week_id', 'symbol', 'mag_abs']].drop_duplicates(['week_id', 'symbol'])\n",
    "        m = m.merge(sig, on=['week_id', 'symbol'], how='left')\n",
    "    else:\n",
    "        m['mag_abs'] = np.nan\n",
    "\n",
    "    m['tier'] = m['mag_abs'].apply(tier_from_mag)\n",
    "\n",
    "    profile = (\n",
    "        m.groupby(['tier', 'day_in_week'], as_index=False)\n",
    "         .agg(\n",
    "             mean_adverse_move=('adverse_move', 'mean'),\n",
    "             mean_daily_net_pnl=('daily_total_net_pnl', 'mean')\n",
    "         )\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    sns.lineplot(data=profile, x='day_in_week', y='mean_adverse_move',\n",
    "                 hue='tier', marker='o', ax=axes[0])\n",
    "    axes[0].axhline(0, color='black', linewidth=1)\n",
    "    axes[0].set_title('Mean Adverse Move Since Rebalance', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Adverse Move (fraction)')\n",
    "    axes[0].set_xlabel('Day-in-Week')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    sns.lineplot(data=profile, x='day_in_week', y='mean_daily_net_pnl',\n",
    "                 hue='tier', marker='o', ax=axes[1])\n",
    "    axes[1].axhline(0, color='black', linewidth=1)\n",
    "    axes[1].set_title('Mean Daily Net P&L During Scaling', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Daily Net P&L ($)')\n",
    "    axes[1].set_xlabel('Day-in-Week')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    week_risk = (\n",
    "        m.groupby('week_id', as_index=False)\n",
    "         .agg(\n",
    "             avg_adverse_move=('adverse_move', 'mean'),\n",
    "             max_adverse_move=('adverse_move', 'max'),\n",
    "             week_net_pnl=('daily_total_net_pnl', 'sum')\n",
    "         )\n",
    "         .sort_values('week_id')\n",
    "    )\n",
    "    print('\\nWeek-Level Stale Signal Risk (last 15 weeks):')\n",
    "    display(week_risk.tail(15))\n",
    "else:\n",
    "    print(\"targets.csv and/or positions.csv not available — skipping stale signal risk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# P&L Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 11: P&L by Long vs Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if df_positions is not None:\n    pnl_col = 'daily_total_net_pnl' if 'daily_total_net_pnl' in df_positions.columns else 'daily_pnl'\n\n    pos = df_positions.copy()\n    pos['side'] = np.where(pos['weight'] > 0, 'Long', np.where(pos['weight'] < 0, 'Short', 'Flat'))\n    pos = pos[pos['side'] != 'Flat']\n\n    pnl_by_side = pos.groupby(['date', 'side'])[pnl_col].sum().unstack(fill_value=0)\n    for col in ['Long', 'Short']:\n        if col not in pnl_by_side.columns:\n            pnl_by_side[col] = 0.0\n    pnl_by_side = pnl_by_side.sort_index()\n\n    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n    axes[0].bar(pnl_by_side.index, pnl_by_side['Long'],\n                color='#2ca02c', alpha=0.7, label='Long')\n    axes[0].bar(pnl_by_side.index, pnl_by_side['Short'],\n                bottom=pnl_by_side['Long'], color='#9467bd', alpha=0.7, label='Short')\n    axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    axes[0].set_title('Daily P&L by Side', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('P&L ($)')\n    axes[0].legend(loc='upper left')\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(pnl_by_side.index, pnl_by_side['Long'].cumsum(),\n                 linewidth=2, color='#2ca02c', label='Long (cumulative)')\n    axes[1].plot(pnl_by_side.index, pnl_by_side['Short'].cumsum(),\n                 linewidth=2, color='#9467bd', label='Short (cumulative)')\n    axes[1].plot(pnl_by_side.index, (pnl_by_side['Long'] + pnl_by_side['Short']).cumsum(),\n                 linewidth=2, color='steelblue', linestyle='--', label='Total')\n    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    axes[1].set_title('Cumulative P&L by Side', fontsize=14, fontweight='bold')\n    axes[1].set_xlabel('Date')\n    axes[1].set_ylabel('Cumulative P&L ($)')\n    axes[1].legend(loc='upper left')\n    axes[1].grid(True, alpha=0.3)\n\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"\\nUsing P&L column: {pnl_col}\")\n    print(f\"  Long total:  ${pnl_by_side['Long'].sum():,.2f}\")\n    print(f\"  Short total: ${pnl_by_side['Short'].sum():,.2f}\")\nelse:\n    print(\"positions.csv not available — skipping P&L by side.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 12: Top / Bottom Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if df_positions is not None:\n    pnl_col = 'daily_total_net_pnl' if 'daily_total_net_pnl' in df_positions.columns else 'daily_pnl'\n    symbol_pnl = df_positions.groupby('symbol')[pnl_col].sum().sort_values()\n\n    n = min(10, len(symbol_pnl))\n    bottom = symbol_pnl.head(n)\n    top    = symbol_pnl.tail(n)\n    combined = pd.concat([bottom, top]).drop_duplicates()\n\n    colors = ['#d62728' if v < 0 else '#2ca02c' for v in combined.values]\n\n    fig, ax = plt.subplots(figsize=(10, max(6, len(combined) * 0.4)))\n    ax.barh(combined.index.astype(str), combined.values, color=colors, alpha=0.8)\n    ax.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n    ax.set_title(f'Top {n} and Bottom {n} Contributors by Total P&L',\n                 fontsize=14, fontweight='bold')\n    ax.set_xlabel('Total P&L ($)')\n    ax.grid(True, alpha=0.3, axis='x')\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"\\nUsing P&L column: {pnl_col}\")\n    print(f\"Total positive contributors: {(symbol_pnl > 0).sum()}\")\n    print(f\"Total negative contributors: {(symbol_pnl < 0).sum()}\")\nelse:\n    print(\"positions.csv not available — skipping contributor chart.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 21: P&L Reconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if df_positions is not None:\n    pnl_col = 'daily_total_net_pnl' if 'daily_total_net_pnl' in df_positions.columns else 'daily_pnl'\n    daily_position_pnl = (\n        df_positions.groupby('date')[pnl_col].sum()\n        .reset_index()\n        .rename(columns={pnl_col: 'position_pnl'})\n    )\n\n    if 'daily_fees' in df_positions.columns:\n        daily_fees = df_positions.groupby('date')['daily_fees'].sum().reset_index()\n        daily_fees.columns = ['date', 'fees']\n        daily_position_pnl = daily_position_pnl.merge(daily_fees, on='date', how='left')\n    if 'fees' not in daily_position_pnl.columns:\n        daily_position_pnl['fees'] = 0.0\n    daily_position_pnl['fees'] = daily_position_pnl['fees'].fillna(0.0)\n\n    if 'daily_dividends' in df_positions.columns:\n        daily_divs = df_positions.groupby('date')['daily_dividends'].sum().reset_index()\n        daily_divs.columns = ['date', 'dividends']\n        daily_position_pnl = daily_position_pnl.merge(daily_divs, on='date', how='left')\n    if 'dividends' not in daily_position_pnl.columns:\n        daily_position_pnl['dividends'] = 0.0\n    daily_position_pnl['dividends'] = daily_position_pnl['dividends'].fillna(0.0)\n\n    daily_position_pnl['attributed_net'] = daily_position_pnl['position_pnl']\n\n    snap = df_snapshots[['date', 'nav']].copy()\n    if 'daily_pnl' in df_snapshots.columns:\n        snap['nav_change'] = df_snapshots['daily_pnl']\n    else:\n        snap['nav_change'] = snap['nav'].diff()\n\n    reconcile = snap[['date', 'nav_change']].merge(daily_position_pnl, on='date', how='left')\n    for col in ['position_pnl', 'attributed_net', 'fees', 'dividends']:\n        reconcile[col] = reconcile[col].fillna(0)\n    reconcile['unexplained'] = reconcile['nav_change'] - reconcile['attributed_net']\n\n    print('=' * 80)\n    print(f'P&L RECONCILIATION: Attributed ({pnl_col}) vs NAV Change')\n    print('=' * 80)\n    print(f\"Total NAV Change:         ${reconcile['nav_change'].sum():,.2f}\")\n    print(f\"Total Attributed Net:     ${reconcile['attributed_net'].sum():,.2f}\")\n    print(f\"Total Unexplained:        ${reconcile['unexplained'].sum():,.2f}\")\n    nav_total = reconcile['nav_change'].sum()\n    if nav_total != 0:\n        print(f\"Unexplained as % of NAV:  {abs(reconcile['unexplained'].sum()) / abs(nav_total) * 100:.2f}%\")\n\n    fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n\n    axes[0].plot(reconcile['date'], reconcile['nav_change'],\n                 'b-', linewidth=2, label='NAV Change', alpha=0.8)\n    axes[0].plot(reconcile['date'], reconcile['attributed_net'],\n                 'g--', linewidth=2, label='Attributed Net P&L', alpha=0.8)\n    axes[0].set_title('Daily P&L Reconciliation: NAV Change vs Attributed Net',\n                      fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('P&L ($)')\n    axes[0].legend(loc='upper left')\n    axes[0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].bar(reconcile['date'], reconcile['unexplained'],\n                color=np.where(reconcile['unexplained'] >= 0, 'orange', 'purple'), alpha=0.7)\n    axes[1].set_title('Daily Unexplained P&L', fontsize=14, fontweight='bold')\n    axes[1].set_ylabel('Unexplained P&L ($)')\n    axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    axes[1].grid(True, alpha=0.3)\n\n    reconcile['cum_nav']     = reconcile['nav_change'].cumsum()\n    reconcile['cum_attrib']  = reconcile['attributed_net'].cumsum()\n\n    axes[2].plot(reconcile['date'], reconcile['cum_nav'],\n                 'b-', linewidth=2, label='Cumulative NAV Change')\n    axes[2].plot(reconcile['date'], reconcile['cum_attrib'],\n                 'g--', linewidth=2, label='Cumulative Attributed Net')\n    axes[2].fill_between(reconcile['date'], reconcile['cum_attrib'], reconcile['cum_nav'],\n                         alpha=0.3, color='red', label='Cumulative Gap')\n    axes[2].set_title('Cumulative P&L: NAV Change vs Attributed Net',\n                      fontsize=14, fontweight='bold')\n    axes[2].set_xlabel('Date')\n    axes[2].set_ylabel('Cumulative P&L ($)')\n    axes[2].legend(loc='upper left')\n    axes[2].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n    axes[2].grid(True, alpha=0.3)\n\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"positions.csv not available — skipping P&L reconciliation.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 22: P&L by Signal Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if df_positions is not None and df_signals is not None:\n    sig = df_signals.copy()\n\n    required_cols = ['price', 'sma_short', 'sma_medium', 'sma_long', 'atr']\n    for col in required_cols:\n        sig[col] = pd.to_numeric(sig[col], errors='coerce')\n    sig = sig.dropna(subset=required_cols).copy()\n    sig = sig.sort_values('date').groupby(['date', 'symbol']).tail(1)\n\n    atr_safe = sig['atr'].replace(0, np.nan)\n    sig['dist_short']  = (sig['price'] - sig['sma_short'])  / atr_safe\n    sig['dist_medium'] = (sig['price'] - sig['sma_medium']) / atr_safe\n    sig['dist_long']   = (sig['price'] - sig['sma_long'])   / atr_safe\n\n    weights = {'short': 0.5, 'medium': 0.3, 'long': 0.2}\n    sig['comp_short']  = weights['short']  * sig['dist_short']\n    sig['comp_medium'] = weights['medium'] * sig['dist_medium']\n    sig['comp_long']   = weights['long']   * sig['dist_long']\n    sig['abs_total']   = sig[['comp_short', 'comp_medium', 'comp_long']].abs().sum(axis=1)\n    sig = sig[sig['abs_total'] > 0].copy()\n\n    pnl_col = 'daily_total_net_pnl' if 'daily_total_net_pnl' in df_positions.columns else 'daily_pnl'\n    pos_pnl = (\n        df_positions.groupby(['date', 'symbol'])[pnl_col]\n                    .sum().reset_index()\n                    .rename(columns={pnl_col: 'position_pnl'})\n    )\n\n    merged_h = pos_pnl.merge(\n        sig[['date', 'symbol', 'comp_short', 'comp_medium', 'comp_long', 'abs_total']],\n        on=['symbol', 'date'], how='left'\n    ).sort_values(['symbol', 'date'])\n\n    merged_h[['comp_short', 'comp_medium', 'comp_long', 'abs_total']] = (\n        merged_h.groupby('symbol')[['comp_short', 'comp_medium', 'comp_long', 'abs_total']].ffill()\n    )\n    merged_h = merged_h[merged_h['abs_total'].notna() & (merged_h['abs_total'] > 0)].copy()\n\n    merged_h['short_pnl']  = merged_h['position_pnl'] * (merged_h['comp_short'].abs()  / merged_h['abs_total'])\n    merged_h['medium_pnl'] = merged_h['position_pnl'] * (merged_h['comp_medium'].abs() / merged_h['abs_total'])\n    merged_h['long_pnl']   = merged_h['position_pnl'] * (merged_h['comp_long'].abs()   / merged_h['abs_total'])\n\n    daily_horizon = merged_h.groupby('date')[['short_pnl', 'medium_pnl', 'long_pnl']].sum().reset_index()\n\n    totals = daily_horizon[['short_pnl', 'medium_pnl', 'long_pnl']].sum()\n    totals = totals.rename({'short_pnl': 'Short (SMA-20)', 'medium_pnl': 'Medium (SMA-63)', 'long_pnl': 'Long (SMA-252)'})\n    totals_df = totals.to_frame('Total P&L')\n    total_attributed = totals_df['Total P&L'].sum()\n    totals_df['Pct of Attributed'] = totals_df['Total P&L'] / total_attributed if total_attributed != 0 else 0.0\n    print('\\nHorizon Attribution Summary:')\n    print('=' * 60)\n    display(totals_df)\n\n    cum_h = daily_horizon.set_index('date').cumsum()\n    fig, ax = plt.subplots(figsize=(14, 6))\n    ax.plot(cum_h.index, cum_h['short_pnl'],  label='Short (SMA-20)',  color='#1f77b4', linewidth=2)\n    ax.plot(cum_h.index, cum_h['medium_pnl'], label='Medium (SMA-63)', color='#ff7f0e', linewidth=2)\n    ax.plot(cum_h.index, cum_h['long_pnl'],   label='Long (SMA-252)',  color='#2ca02c', linewidth=2)\n    ax.set_title('Cumulative P&L by Signal Horizon', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Date')\n    ax.set_ylabel('Cumulative P&L ($)')\n    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='upper left')\n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()\n\n    monthly_h = daily_horizon.copy()\n    monthly_h['year_month'] = monthly_h['date'].dt.to_period('M')\n    monthly_h = monthly_h.groupby('year_month')[['short_pnl', 'medium_pnl', 'long_pnl']].sum()\n\n    fig, ax = plt.subplots(figsize=(14, 6))\n    monthly_h.plot(kind='bar', stacked=True, ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n    ax.set_title('Monthly P&L by Signal Horizon (Stacked)', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Month')\n    ax.set_ylabel('P&L ($)')\n    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    ax.grid(True, alpha=0.3, axis='y')\n    ax.set_xticklabels([str(p) for p in monthly_h.index], rotation=45, ha='right')\n    ax.legend(loc='upper left')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"positions.csv and/or signals.csv not available — skipping horizon attribution.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 23: Slippage Cost Overlay (Gross vs Net P&L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_slippage is not None:\n",
    "    slippage_col = None\n",
    "    for col in ['slippage_dollars', 'slippage', 'cost']:\n",
    "        if col in df_slippage.columns:\n",
    "            slippage_col = col\n",
    "            break\n",
    "\n",
    "    if slippage_col is None and 'expected_price' in df_slippage.columns and 'fill_price' in df_slippage.columns:\n",
    "        df_slippage['slippage_dollars'] = df_slippage['fill_price'] - df_slippage['expected_price']\n",
    "        slippage_col = 'slippage_dollars'\n",
    "\n",
    "    if slippage_col:\n",
    "        daily_slip = df_slippage.groupby('date')[slippage_col].sum().reset_index()\n",
    "        daily_slip.columns = ['date', 'daily_slippage']\n",
    "\n",
    "        overlay = df_snapshots[['date', 'daily_pnl']].merge(daily_slip, on='date', how='left')\n",
    "        overlay['daily_slippage'] = overlay['daily_slippage'].fillna(0)\n",
    "        overlay['gross_pnl'] = overlay['daily_pnl'] + overlay['daily_slippage']\n",
    "\n",
    "        print('\\nSlippage Impact:')\n",
    "        print('=' * 60)\n",
    "        print(f\"Total slippage cost:         ${overlay['daily_slippage'].sum():,.2f}\")\n",
    "        print(f\"Net P&L:                     ${overlay['daily_pnl'].sum():,.2f}\")\n",
    "        print(f\"Gross P&L (before slippage): ${overlay['gross_pnl'].sum():,.2f}\")\n",
    "\n",
    "        overlay['cum_net']      = overlay['daily_pnl'].cumsum()\n",
    "        overlay['cum_gross']    = overlay['gross_pnl'].cumsum()\n",
    "        overlay['cum_slippage'] = overlay['daily_slippage'].cumsum()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        ax.plot(overlay['date'], overlay['cum_gross'],\n",
    "                linewidth=2, label='Gross P&L', color='blue', alpha=0.7)\n",
    "        ax.plot(overlay['date'], overlay['cum_net'],\n",
    "                linewidth=2, label='Net P&L', color='green', alpha=0.7)\n",
    "        ax.fill_between(overlay['date'], overlay['cum_gross'], overlay['cum_net'],\n",
    "                        alpha=0.3, color='red', label='Slippage Cost')\n",
    "        ax.set_title('Cumulative P&L: Gross vs Net (Slippage Impact)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Cumulative P&L ($)')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Could not identify slippage column — skipping overlay.\")\n",
    "else:\n",
    "    print(\"slippage.csv not available — skipping slippage overlay.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}