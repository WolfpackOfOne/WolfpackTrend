{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbol Stickiness and Selection Drivers\n",
    "\n",
    "This notebook quantifies concentration and persistence to explain why similar names keep appearing.\n",
    "\n",
    "Primary outputs:\n",
    "- Per-symbol stickiness metrics\n",
    "- Week-to-week retention diagnostics\n",
    "- Concentration trends (top-name share / HHI proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIVERSE_SYMBOLS = [\n",
    "    'AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM',\n",
    "    'CSCO', 'CVX', 'DIS', 'DOW', 'GS', 'HD',\n",
    "    'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO',\n",
    "    'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG',\n",
    "    'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT'\n",
    "]\n",
    "\n",
    "# k controls how much tanh saturation is reduced (larger k => more separation)\n",
    "score_scale_k = 2.0\n",
    "\n",
    "df_signals = read_csv_from_store('wolfpack/signals.csv')\n",
    "df_targets = read_csv_from_store('wolfpack/targets.csv')\n",
    "\n",
    "if df_signals is None or df_targets is None:\n",
    "    raise ValueError('signals.csv and targets.csv are required.')\n",
    "\n",
    "df_signals['date'] = pd.to_datetime(df_signals['date'])\n",
    "for col in ['magnitude', 'price', 'sma_short', 'sma_medium', 'sma_long', 'atr']:\n",
    "    if col in df_signals.columns:\n",
    "        df_signals[col] = pd.to_numeric(df_signals[col], errors='coerce')\n",
    "\n",
    "df_signals['abs_magnitude'] = df_signals['magnitude'].abs()\n",
    "df_signals['signal_sign'] = np.sign(df_signals['magnitude'])\n",
    "df_signals['week_id'] = df_signals['date']\n",
    "\n",
    "needed_for_composite = {'price', 'sma_short', 'sma_medium', 'sma_long', 'atr'}\n",
    "if needed_for_composite.issubset(df_signals.columns):\n",
    "    safe_atr = df_signals['atr'].replace(0, np.nan)\n",
    "    df_signals['dist_short'] = (df_signals['price'] - df_signals['sma_short']) / safe_atr\n",
    "    df_signals['dist_medium'] = (df_signals['price'] - df_signals['sma_medium']) / safe_atr\n",
    "    df_signals['dist_long'] = (df_signals['price'] - df_signals['sma_long']) / safe_atr\n",
    "    df_signals['composite_score'] = (\n",
    "        0.5 * df_signals['dist_short'] +\n",
    "        0.3 * df_signals['dist_medium'] +\n",
    "        0.2 * df_signals['dist_long']\n",
    "    )\n",
    "else:\n",
    "    clipped_mag = df_signals['magnitude'].clip(-0.999999, 0.999999)\n",
    "    df_signals['composite_score'] = np.arctanh(clipped_mag)\n",
    "    print('Warning: price/SMA/ATR columns missing; composite_score recovered from arctanh(magnitude).')\n",
    "\n",
    "df_signals['abs_composite_score'] = df_signals['composite_score'].abs()\n",
    "df_signals['scaled_magnitude_k'] = np.tanh(df_signals['composite_score'] / score_scale_k)\n",
    "df_signals['abs_scaled_magnitude_k'] = df_signals['scaled_magnitude_k'].abs()\n",
    "\n",
    "if 'week_id' in df_targets.columns:\n",
    "    df_targets['week_id'] = pd.to_datetime(df_targets['week_id'], errors='coerce')\n",
    "else:\n",
    "    df_targets['week_id'] = pd.to_datetime(df_targets['date'])\n",
    "\n",
    "for col in ['weekly_target_w', 'actual_w']:\n",
    "    if col in df_targets.columns:\n",
    "        df_targets[col] = pd.to_numeric(df_targets[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "weekly_targets = (\n",
    "    df_targets[df_targets['week_id'].notna()]\n",
    "      .sort_values('date')\n",
    "      .groupby(['week_id', 'symbol'], as_index=False)\n",
    "      .agg(weekly_target_w=('weekly_target_w', 'last'))\n",
    ")\n",
    "weekly_targets['selected'] = weekly_targets['weekly_target_w'].abs() > 1e-6\n",
    "\n",
    "weekly_signals = (\n",
    "    df_signals[\n",
    "        [\n",
    "            'week_id', 'symbol', 'magnitude', 'abs_magnitude', 'signal_sign',\n",
    "            'composite_score', 'abs_composite_score',\n",
    "            'scaled_magnitude_k', 'abs_scaled_magnitude_k'\n",
    "        ]\n",
    "    ]\n",
    "    .dropna(subset=['week_id'])\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "week_ids = sorted(set(weekly_targets['week_id']).union(set(weekly_signals['week_id'])))\n",
    "panel = pd.MultiIndex.from_product([week_ids, UNIVERSE_SYMBOLS], names=['week_id', 'symbol']).to_frame(index=False)\n",
    "panel = panel.merge(weekly_signals, on=['week_id', 'symbol'], how='left')\n",
    "panel = panel.merge(weekly_targets[['week_id', 'symbol', 'weekly_target_w', 'selected']], on=['week_id', 'symbol'], how='left')\n",
    "\n",
    "panel['has_signal'] = panel['abs_magnitude'].notna()\n",
    "panel['selected'] = panel['selected'].fillna(False)\n",
    "panel['weekly_target_w'] = panel['weekly_target_w'].fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_metrics = (\n",
    "    panel.groupby('symbol', as_index=False)\n",
    "         .agg(\n",
    "             weeks_observed=('week_id', 'count'),\n",
    "             weeks_signaled=('has_signal', 'sum'),\n",
    "             weeks_selected=('selected', 'sum'),\n",
    "             mean_abs_magnitude=('abs_magnitude', 'mean'),\n",
    "             median_abs_magnitude=('abs_magnitude', 'median'),\n",
    "             mean_abs_scaled_magnitude_k=('abs_scaled_magnitude_k', 'mean'),\n",
    "             median_abs_scaled_magnitude_k=('abs_scaled_magnitude_k', 'median'),\n",
    "             mean_abs_composite_score=('abs_composite_score', 'mean'),\n",
    "             median_abs_composite_score=('abs_composite_score', 'median'),\n",
    "             avg_abs_target_when_selected=('weekly_target_w', lambda s: s[s.abs() > 1e-6].abs().mean())\n",
    "         )\n",
    ")\n",
    "\n",
    "# Selection rate conditional on signal existing\n",
    "def conditional_selection_rate(g):\n",
    "    dg = g[g['has_signal']]\n",
    "    if len(dg) == 0:\n",
    "        return np.nan\n",
    "    return dg['selected'].mean()\n",
    "\n",
    "rates = panel.groupby('symbol').apply(conditional_selection_rate).rename('selection_rate_when_signaled').reset_index()\n",
    "direction_consistency = (\n",
    "    weekly_signals.groupby('symbol', as_index=False)['signal_sign']\n",
    "                  .mean()\n",
    "                  .rename(columns={'signal_sign': 'mean_signal_sign'})\n",
    ")\n",
    "direction_consistency['direction_consistency_abs'] = direction_consistency['mean_signal_sign'].abs()\n",
    "\n",
    "symbol_metrics = symbol_metrics.merge(rates, on='symbol', how='left')\n",
    "symbol_metrics = symbol_metrics.merge(direction_consistency[['symbol', 'direction_consistency_abs']], on='symbol', how='left')\n",
    "\n",
    "# Max consecutive selected-week streak\n",
    "panel_sorted = panel.sort_values(['symbol', 'week_id']).copy()\n",
    "\n",
    "def max_streak(selected_values):\n",
    "    max_run = 0\n",
    "    run = 0\n",
    "    for val in selected_values:\n",
    "        if bool(val):\n",
    "            run += 1\n",
    "            max_run = max(max_run, run)\n",
    "        else:\n",
    "            run = 0\n",
    "    return max_run\n",
    "\n",
    "streaks = (\n",
    "    panel_sorted.groupby('symbol')['selected']\n",
    "               .apply(max_streak)\n",
    "               .rename('max_selected_streak')\n",
    "               .reset_index()\n",
    ")\n",
    "\n",
    "symbol_metrics = symbol_metrics.merge(streaks, on='symbol', how='left')\n",
    "symbol_metrics = symbol_metrics.sort_values(['weeks_selected', 'selection_rate_when_signaled'], ascending=[False, False])\n",
    "\n",
    "display(symbol_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = symbol_metrics.head(15)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(top['symbol'], top['weeks_selected'], color='#1f77b4')\n",
    "plt.title('Top Symbols by Weeks Selected')\n",
    "plt.ylabel('Weeks selected')\n",
    "plt.xticks(rotation=35)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "y = 100 * symbol_metrics['selection_rate_when_signaled']\n",
    "size = 25 + 18 * symbol_metrics['weeks_selected'].fillna(0)\n",
    "color = symbol_metrics['direction_consistency_abs'].fillna(0)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6), sharey=True)\n",
    "\n",
    "sc0 = axes[0].scatter(\n",
    "    symbol_metrics['median_abs_magnitude'],\n",
    "    y,\n",
    "    s=size,\n",
    "    c=color,\n",
    "    cmap='viridis',\n",
    "    alpha=0.85\n",
    ")\n",
    "axes[0].set_xlabel('Median abs tanh(score)')\n",
    "axes[0].set_title('Saturated')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].scatter(\n",
    "    symbol_metrics['median_abs_scaled_magnitude_k'],\n",
    "    y,\n",
    "    s=size,\n",
    "    c=color,\n",
    "    cmap='viridis',\n",
    "    alpha=0.85\n",
    ")\n",
    "axes[1].set_xlabel(f'Median abs tanh(score/{score_scale_k:g})')\n",
    "axes[1].set_title('Reduced Saturation')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "axes[2].scatter(\n",
    "    symbol_metrics['median_abs_composite_score'],\n",
    "    y,\n",
    "    s=size,\n",
    "    c=color,\n",
    "    cmap='viridis',\n",
    "    alpha=0.85\n",
    ")\n",
    "for _, row in symbol_metrics.head(12).iterrows():\n",
    "    axes[2].annotate(\n",
    "        row['symbol'],\n",
    "        (row['median_abs_composite_score'], 100 * row['selection_rate_when_signaled']),\n",
    "        fontsize=8\n",
    "    )\n",
    "axes[2].set_xlabel('Median abs composite_score (unsaturated)')\n",
    "axes[2].set_title('Unsaturated')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel('Selection rate when signaled (%)')\n",
    "fig.suptitle('Signal Strength vs Selection Probability (bubble size = weeks selected)', y=1.02)\n",
    "fig.colorbar(sc0, ax=axes, label='Direction consistency (abs mean sign)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(\n",
    "    symbol_metrics[\n",
    "        [\n",
    "            'symbol',\n",
    "            'median_abs_magnitude',\n",
    "            'median_abs_scaled_magnitude_k',\n",
    "            'median_abs_composite_score',\n",
    "            'selection_rate_when_signaled',\n",
    "            'weeks_selected'\n",
    "        ]\n",
    "    ]\n",
    "    .sort_values('median_abs_composite_score', ascending=False)\n",
    "    .head(20)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly concentration diagnostics\n",
    "weekly_abs_weights = (\n",
    "    panel.groupby(['week_id', 'symbol'], as_index=False)['weekly_target_w']\n",
    "         .last()\n",
    ")\n",
    "weekly_abs_weights['abs_w'] = weekly_abs_weights['weekly_target_w'].abs()\n",
    "\n",
    "concentration_rows = []\n",
    "for week, g in weekly_abs_weights.groupby('week_id'):\n",
    "    total = g['abs_w'].sum()\n",
    "    if total <= 0:\n",
    "        concentration_rows.append({\n",
    "            'week_id': week,\n",
    "            'gross_target': 0.0,\n",
    "            'top_5_share': np.nan,\n",
    "            'top_10_share': np.nan,\n",
    "            'hhi_proxy': np.nan\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    shares = g['abs_w'] / total\n",
    "    concentration_rows.append({\n",
    "        'week_id': week,\n",
    "        'gross_target': total,\n",
    "        'top_5_share': g['abs_w'].nlargest(5).sum() / total,\n",
    "        'top_10_share': g['abs_w'].nlargest(10).sum() / total,\n",
    "        'hhi_proxy': (shares ** 2).sum()\n",
    "    })\n",
    "\n",
    "concentration = pd.DataFrame(concentration_rows).sort_values('week_id')\n",
    "\n",
    "display(concentration.tail(20))\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 9), sharex=True)\n",
    "axes[0].plot(concentration['week_id'], 100 * concentration['top_5_share'], label='Top 5 share %', color='#ff7f0e')\n",
    "axes[0].plot(concentration['week_id'], 100 * concentration['top_10_share'], label='Top 10 share %', color='#1f77b4')\n",
    "axes[0].set_ylabel('Share of gross target (%)')\n",
    "axes[0].set_title('Target Concentration Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(concentration['week_id'], concentration['hhi_proxy'], color='#2ca02c')\n",
    "axes[1].set_ylabel('HHI proxy')\n",
    "axes[1].set_xlabel('Week')\n",
    "axes[1].set_title('Concentration HHI Proxy (higher = stickier universe)')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week-over-week retention diagnostics\n",
    "selected_sets = (\n",
    "    panel[panel['selected']]\n",
    "      .groupby('week_id')['symbol']\n",
    "      .apply(set)\n",
    "      .sort_index()\n",
    ")\n",
    "\n",
    "rows = []\n",
    "weeks = list(selected_sets.index)\n",
    "for i, week in enumerate(weeks):\n",
    "    current_set = selected_sets.loc[week]\n",
    "    prev_set = selected_sets.loc[weeks[i - 1]] if i > 0 else set()\n",
    "    stayed = sorted(current_set & prev_set)\n",
    "    entered = sorted(current_set - prev_set)\n",
    "    exited = sorted(prev_set - current_set)\n",
    "    rows.append({\n",
    "        'week_id': week,\n",
    "        'selected_count': len(current_set),\n",
    "        'stayed_count': len(stayed),\n",
    "        'entered_count': len(entered),\n",
    "        'exited_count': len(exited),\n",
    "        'retention_from_prev_week': len(stayed) / len(prev_set) if prev_set else np.nan,\n",
    "        'entered_symbols': ', '.join(entered),\n",
    "        'exited_symbols': ', '.join(exited)\n",
    "    })\n",
    "\n",
    "retention_report = pd.DataFrame(rows)\n",
    "display(retention_report.tail(20))\n",
    "\n",
    "latest_week = panel['week_id'].max()\n",
    "latest = panel[panel['week_id'] == latest_week].copy()\n",
    "latest['abs_magnitude'] = latest['abs_magnitude'].fillna(0.0)\n",
    "latest_non_selected = latest[~latest['selected']].sort_values('abs_magnitude', ascending=False)\n",
    "\n",
    "print(f'Latest week: {latest_week.date()}')\n",
    "print('Top non-selected symbols by abs signal magnitude:')\n",
    "display(latest_non_selected.head(10)[['symbol', 'abs_magnitude', 'weekly_target_w', 'has_signal']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}