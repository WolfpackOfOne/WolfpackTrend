{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TearSheet GPT\n",
    "\n",
    "Comprehensive tear sheet notebook built from existing WolfpackTrend ObjectStore logs.\n",
    "\n",
    "Design goals:\n",
    "- No strategy values hardcoded in this notebook.\n",
    "- Strategy settings are extracted dynamically from `main.py`.\n",
    "- All plots degrade gracefully when optional datasets are missing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sps\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Strategy Metadata Extraction (Dynamic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADING_DAYS = 252\n",
    "\n",
    "\n",
    "def _literal_value(node):\n",
    "    if isinstance(node, ast.Constant):\n",
    "        return node.value\n",
    "    if isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.USub) and isinstance(node.operand, ast.Constant):\n",
    "        if isinstance(node.operand.value, (int, float)):\n",
    "            return -node.operand.value\n",
    "    if isinstance(node, ast.Tuple):\n",
    "        vals = []\n",
    "        for elt in node.elts:\n",
    "            val = _literal_value(elt)\n",
    "            if val is None:\n",
    "                return None\n",
    "            vals.append(val)\n",
    "        return tuple(vals)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_strategy_settings(main_path='main.py'):\n",
    "    settings = {\n",
    "        'benchmark_symbol': None,\n",
    "        'target_vol_annual': None,\n",
    "        'scaling_days': None,\n",
    "        'rebalance_interval_trading_days': None,\n",
    "        'strong_threshold': None,\n",
    "        'moderate_threshold': None,\n",
    "    }\n",
    "\n",
    "    p = Path(main_path)\n",
    "    if not p.exists():\n",
    "        print(f'WARNING: {main_path} not found. Settings fallback will be used only where necessary.')\n",
    "        return settings\n",
    "\n",
    "    tree = ast.parse(p.read_text())\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Call):\n",
    "            fn_name = None\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                fn_name = node.func.id\n",
    "            elif isinstance(node.func, ast.Attribute):\n",
    "                fn_name = node.func.attr\n",
    "\n",
    "            if fn_name == 'SetBenchmark' and node.args:\n",
    "                val = _literal_value(node.args[0])\n",
    "                if isinstance(val, str):\n",
    "                    settings['benchmark_symbol'] = val\n",
    "\n",
    "            if fn_name == 'TargetVolPortfolioConstructionModel':\n",
    "                for kw in node.keywords:\n",
    "                    if kw.arg in settings:\n",
    "                        val = _literal_value(kw.value)\n",
    "                        if val is not None:\n",
    "                            settings[kw.arg] = val\n",
    "\n",
    "            if fn_name == 'SignalStrengthExecutionModel':\n",
    "                for kw in node.keywords:\n",
    "                    if kw.arg in settings:\n",
    "                        val = _literal_value(kw.value)\n",
    "                        if val is not None:\n",
    "                            settings[kw.arg] = val\n",
    "\n",
    "    return settings\n",
    "\n",
    "\n",
    "STRATEGY = extract_strategy_settings('main.py')\n",
    "\n",
    "print('Extracted strategy settings:')\n",
    "for k, v in STRATEGY.items():\n",
    "    print(f'  {k}: {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data Loading Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def to_numeric(df, cols):\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_tag_value(tag, key):\n",
    "    if pd.isna(tag):\n",
    "        return np.nan\n",
    "    m = re.search(rf'{key}=([^;]+)', str(tag))\n",
    "    return m.group(1) if m else np.nan\n",
    "\n",
    "\n",
    "def infer_cycle_days(df_targets, df_snapshots):\n",
    "    # Prefer explicit scaling index from targets.\n",
    "    if df_targets is not None and 'scale_day' in df_targets.columns:\n",
    "        s = pd.to_numeric(df_targets['scale_day'], errors='coerce').dropna()\n",
    "        if len(s):\n",
    "            mx = int(s.max())\n",
    "            if mx >= 0:\n",
    "                return mx + 1\n",
    "\n",
    "    # Fallback: distinct dates per week_id.\n",
    "    if df_targets is not None and {'week_id', 'date'}.issubset(df_targets.columns):\n",
    "        tmp = df_targets[['week_id', 'date']].dropna().copy()\n",
    "        if len(tmp):\n",
    "            tmp['date'] = pd.to_datetime(tmp['date'])\n",
    "            d = tmp.groupby('week_id')['date'].nunique()\n",
    "            d = d[d > 0]\n",
    "            if len(d):\n",
    "                return int(round(float(d.median())))\n",
    "\n",
    "    # Last fallback: median trading days per ISO week from snapshots.\n",
    "    if df_snapshots is not None and 'date' in df_snapshots.columns:\n",
    "        s = pd.to_datetime(df_snapshots['date']).dropna().dt.to_period('W')\n",
    "        if len(s):\n",
    "            d = s.value_counts()\n",
    "            if len(d):\n",
    "                return int(round(float(d.median())))\n",
    "\n",
    "    return max(1, int(round(TRADING_DAYS / 52)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Load ObjectStore Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snapshots = read_csv_from_store(f'{TEAM_ID}/daily_snapshots.csv')\n",
    "df_positions = read_csv_from_store(f'{TEAM_ID}/positions.csv')\n",
    "df_signals = read_csv_from_store(f'{TEAM_ID}/signals.csv')\n",
    "df_slippage = read_csv_from_store(f'{TEAM_ID}/slippage.csv')\n",
    "df_trades = read_csv_from_store(f'{TEAM_ID}/trades.csv')\n",
    "df_targets = read_csv_from_store(f'{TEAM_ID}/targets.csv')\n",
    "df_orders = read_csv_from_store(f'{TEAM_ID}/order_events.csv')\n",
    "\n",
    "if df_snapshots is None:\n",
    "    raise ValueError('daily_snapshots.csv is required. Run a backtest first.')\n",
    "\n",
    "for frame in [df_snapshots, df_positions, df_signals, df_slippage, df_trades, df_targets, df_orders]:\n",
    "    if frame is not None and 'date' in frame.columns:\n",
    "        frame['date'] = pd.to_datetime(frame['date'], errors='coerce')\n",
    "        frame.dropna(subset=['date'], inplace=True)\n",
    "        frame.sort_values('date', inplace=True)\n",
    "        frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Snapshots core\n",
    "if 'nav' not in df_snapshots.columns:\n",
    "    raise ValueError('daily_snapshots.csv is missing nav column')\n",
    "\n",
    "df_snapshots = to_numeric(df_snapshots, [\n",
    "    'nav', 'cash', 'gross_exposure', 'net_exposure', 'long_exposure', 'short_exposure',\n",
    "    'daily_pnl', 'cumulative_pnl', 'daily_slippage', 'num_positions', 'estimated_vol'\n",
    "])\n",
    "df_snapshots['daily_return'] = df_snapshots['nav'].pct_change()\n",
    "\n",
    "# Positions core\n",
    "if df_positions is not None:\n",
    "    df_positions = to_numeric(df_positions, [\n",
    "        'invested', 'quantity', 'price', 'market_value', 'weight', 'unrealized_pnl',\n",
    "        'daily_pnl', 'daily_unrealized_pnl', 'daily_realized_pnl', 'daily_fees',\n",
    "        'daily_dividends', 'daily_total_net_pnl', 'avg_price'\n",
    "    ])\n",
    "\n",
    "# Slippage core\n",
    "if df_slippage is not None:\n",
    "    df_slippage = to_numeric(df_slippage, ['quantity', 'expected_price', 'fill_price', 'slippage_dollars'])\n",
    "\n",
    "# Targets core\n",
    "if df_targets is not None:\n",
    "    df_targets = to_numeric(df_targets, [\n",
    "        'start_w', 'weekly_target_w', 'scheduled_fraction', 'scheduled_w', 'actual_w', 'scale_day'\n",
    "    ])\n",
    "\n",
    "# Orders core\n",
    "if df_orders is not None:\n",
    "    df_orders = to_numeric(df_orders, [\n",
    "        'quantity', 'fill_quantity', 'fill_price', 'limit_price', 'market_price_at_submit'\n",
    "    ])\n",
    "    if 'tag' in df_orders.columns:\n",
    "        df_orders['tier_tag'] = df_orders['tag'].apply(lambda x: parse_tag_value(x, 'tier')).fillna('unknown')\n",
    "\n",
    "print(f\"Snapshots: {len(df_snapshots):,} rows | {df_snapshots['date'].min():%Y-%m-%d} -> {df_snapshots['date'].max():%Y-%m-%d}\")\n",
    "for label, frame in [\n",
    "    ('Positions', df_positions), ('Signals', df_signals), ('Slippage', df_slippage),\n",
    "    ('Trades', df_trades), ('Targets', df_targets), ('Orders', df_orders)\n",
    "]:\n",
    "    print(f'{label}: {0 if frame is None else len(frame):,} rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Benchmark and Risk-Free Series (Dynamic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark ticker from strategy metadata.\n",
    "benchmark_symbol = STRATEGY.get('benchmark_symbol')\n",
    "if not benchmark_symbol:\n",
    "    raise ValueError('Could not extract benchmark symbol from main.py. Please set benchmark explicitly in strategy code.')\n",
    "\n",
    "bench_symbol = qb.AddEquity(benchmark_symbol, Resolution.Daily).Symbol\n",
    "start = df_snapshots['date'].min()\n",
    "end = df_snapshots['date'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "bench_hist = qb.History(bench_symbol, start, end, Resolution.Daily)\n",
    "if bench_hist is None or len(bench_hist) == 0:\n",
    "    raise ValueError(f'Unable to load benchmark history for {benchmark_symbol}')\n",
    "\n",
    "bench_df = bench_hist.copy()\n",
    "if isinstance(bench_df.index, pd.MultiIndex):\n",
    "    extracted = None\n",
    "    for level in range(bench_df.index.nlevels):\n",
    "        try:\n",
    "            extracted = bench_df.xs(bench_symbol, level=level)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if extracted is not None:\n",
    "        bench_df = extracted\n",
    "\n",
    "if 'close' in bench_df.columns:\n",
    "    bench_close = bench_df['close']\n",
    "elif 'value' in bench_df.columns:\n",
    "    bench_close = bench_df['value']\n",
    "else:\n",
    "    raise ValueError('Benchmark history does not include close/value column')\n",
    "\n",
    "bench_close = pd.Series(bench_close)\n",
    "bench_close.index = pd.to_datetime(bench_close.index)\n",
    "if getattr(bench_close.index, 'tz', None) is not None:\n",
    "    bench_close.index = bench_close.index.tz_localize(None)\n",
    "bench_close = bench_close.groupby(bench_close.index).last().sort_index()\n",
    "\n",
    "df_bench = pd.DataFrame({'date': bench_close.index, 'bench_close': bench_close.values})\n",
    "df_bench['bench_return'] = df_bench['bench_close'].pct_change()\n",
    "df_bench = df_bench.dropna(subset=['bench_return']).copy()\n",
    "\n",
    "# Risk-free proxy (not strategy-specific).\n",
    "rf_symbol = qb.AddEquity('SGOV', Resolution.Daily).Symbol\n",
    "rf_hist = qb.History(rf_symbol, start - pd.Timedelta(days=10), end, Resolution.Daily)\n",
    "\n",
    "df_snapshots['rf_daily'] = 0.0\n",
    "if rf_hist is not None and len(rf_hist) > 0:\n",
    "    rf_prices = rf_hist['close'].reset_index()\n",
    "    date_col = 'time' if 'time' in rf_prices.columns else rf_prices.columns[0]\n",
    "    rf_prices = rf_prices.rename(columns={date_col: 'date', 'close': 'rf_close'})\n",
    "    rf_prices['date'] = pd.to_datetime(rf_prices['date']).dt.tz_localize(None).dt.normalize()\n",
    "    rf_prices['rf_daily'] = rf_prices['rf_close'].pct_change()\n",
    "\n",
    "    df_snapshots = df_snapshots.merge(rf_prices[['date', 'rf_daily']], on='date', how='left', suffixes=('', '_new'))\n",
    "    if 'rf_daily_new' in df_snapshots.columns:\n",
    "        df_snapshots['rf_daily'] = df_snapshots['rf_daily_new'].combine_first(df_snapshots['rf_daily'])\n",
    "        df_snapshots.drop(columns=['rf_daily_new'], inplace=True)\n",
    "\n",
    "    df_snapshots['rf_daily'] = df_snapshots['rf_daily'].ffill().fillna(0.0)\n",
    "\n",
    "df_snapshots['excess_return'] = df_snapshots['daily_return'] - df_snapshots['rf_daily']\n",
    "\n",
    "merged = df_snapshots[['date', 'daily_return']].merge(\n",
    "    df_bench[['date', 'bench_return']], on='date', how='inner'\n",
    ").dropna().sort_values('date').reset_index(drop=True)\n",
    "\n",
    "returns = df_snapshots['daily_return'].dropna()\n",
    "excess_returns = df_snapshots['excess_return'].dropna()\n",
    "rf_annual = float(df_snapshots['rf_daily'].mean() * TRADING_DAYS)\n",
    "\n",
    "print(f'Benchmark: {benchmark_symbol} | overlap rows: {len(merged):,}')\n",
    "print(f'Risk-free proxy: SGOV | effective annualized RF: {rf_annual * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Metric Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(rets, risk_free_rate=0.0, periods_per_year=252):\n",
    "    daily_rf = risk_free_rate / periods_per_year\n",
    "    excess = rets - daily_rf\n",
    "    if excess.std() == 0:\n",
    "        return np.nan\n",
    "    return (excess.mean() / excess.std()) * np.sqrt(periods_per_year)\n",
    "\n",
    "\n",
    "def sortino_ratio(rets, risk_free_rate=0.0, periods_per_year=252):\n",
    "    daily_rf = risk_free_rate / periods_per_year\n",
    "    excess = rets - daily_rf\n",
    "    downside = excess[excess < 0]\n",
    "    downside_std = np.sqrt((downside ** 2).mean())\n",
    "    if downside_std == 0:\n",
    "        return np.nan\n",
    "    return (excess.mean() / downside_std) * np.sqrt(periods_per_year)\n",
    "\n",
    "\n",
    "def calmar_ratio(rets, periods_per_year=252):\n",
    "    cumulative = (1 + rets).cumprod()\n",
    "    total_return = cumulative.iloc[-1] - 1\n",
    "    years = len(rets) / periods_per_year\n",
    "    if years <= 0:\n",
    "        return np.nan\n",
    "    ann_return = (1 + total_return) ** (1 / years) - 1\n",
    "    running_max = cumulative.cummax()\n",
    "    max_dd = abs(((cumulative / running_max) - 1).min())\n",
    "    if max_dd == 0:\n",
    "        return np.nan\n",
    "    return ann_return / max_dd\n",
    "\n",
    "\n",
    "def max_drawdown(rets):\n",
    "    cumulative = (1 + rets).cumprod()\n",
    "    running_max = cumulative.cummax()\n",
    "    return abs(((cumulative / running_max) - 1).min())\n",
    "\n",
    "\n",
    "def probabilistic_sharpe_ratio(rets, sr, benchmark_sr=0.0):\n",
    "    n = len(rets)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    skew = sps.skew(rets)\n",
    "    kurt = sps.kurtosis(rets)\n",
    "    variance = (1 - skew * sr + ((kurt - 1) / 4) * sr ** 2) / (n - 1)\n",
    "    if variance <= 0:\n",
    "        return np.nan\n",
    "    return sps.norm.cdf((sr - benchmark_sr) / np.sqrt(variance))\n",
    "\n",
    "\n",
    "def historical_var(rets, confidence=0.95):\n",
    "    return -np.percentile(rets, (1 - confidence) * 100)\n",
    "\n",
    "\n",
    "def historical_cvar(rets, confidence=0.95):\n",
    "    var = historical_var(rets, confidence)\n",
    "    tail = rets[rets <= -var]\n",
    "    return -tail.mean() if len(tail) else np.nan\n",
    "\n",
    "print('Helper functions defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Performance Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) == 0:\n",
    "    print('No return series available')\n",
    "else:\n",
    "    df_tmp = df_snapshots[['date', 'nav', 'daily_return']].dropna().copy()\n",
    "    df_tmp['year'] = df_tmp['date'].dt.year\n",
    "    df_tmp['month'] = df_tmp['date'].dt.month\n",
    "\n",
    "    monthly = df_tmp.groupby(['year', 'month']).agg(nav_start=('nav', 'first'), nav_end=('nav', 'last')).reset_index()\n",
    "    monthly['monthly_return'] = (monthly['nav_end'] / monthly['nav_start']) - 1\n",
    "\n",
    "    total_return = (df_tmp['nav'].iloc[-1] / df_tmp['nav'].iloc[0]) - 1\n",
    "    years = len(returns) / TRADING_DAYS\n",
    "    ann_return = (1 + total_return) ** (1 / years) - 1 if years > 0 else np.nan\n",
    "    ann_vol = returns.std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "    sr = sharpe_ratio(returns, rf_annual, TRADING_DAYS)\n",
    "    so = sortino_ratio(returns, rf_annual, TRADING_DAYS)\n",
    "    ca = calmar_ratio(returns, TRADING_DAYS)\n",
    "    mdd = max_drawdown(returns)\n",
    "    psr0 = probabilistic_sharpe_ratio(returns, sr, benchmark_sr=0.0)\n",
    "    psr1 = probabilistic_sharpe_ratio(returns, sr, benchmark_sr=1.0)\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Total Return', 'Annualized Return', 'Annualized Volatility',\n",
    "            'Sharpe Ratio', 'Sortino Ratio', 'Calmar Ratio',\n",
    "            'PSR (SR*=0)', 'PSR (SR*=1)', 'Maximum Drawdown',\n",
    "            'Best Day', 'Worst Day', 'Best Month', 'Worst Month',\n",
    "            'Win Rate (Daily)', 'Win Rate (Monthly)', 'Skewness', 'Kurtosis'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f'{total_return * 100:.2f}%', f'{ann_return * 100:.2f}%', f'{ann_vol * 100:.2f}%',\n",
    "            f'{sr:.4f}', f'{so:.4f}', f'{ca:.4f}',\n",
    "            f'{psr0:.4f}', f'{psr1:.4f}', f'{mdd * 100:.2f}%',\n",
    "            f'{returns.max() * 100:.4f}%', f'{returns.min() * 100:.4f}%',\n",
    "            f'{monthly[\"monthly_return\"].max() * 100:.2f}%', f'{monthly[\"monthly_return\"].min() * 100:.2f}%',\n",
    "            f'{(returns > 0).mean() * 100:.1f}%', f'{(monthly[\"monthly_return\"] > 0).mean() * 100:.1f}%',\n",
    "            f'{returns.skew():.4f}', f'{returns.kurtosis():.4f}'\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    print('=' * 80)\n",
    "    print('PERFORMANCE SUMMARY')\n",
    "    print('=' * 80)\n",
    "    print(f'Period: {df_tmp[\"date\"].min():%Y-%m-%d} to {df_tmp[\"date\"].max():%Y-%m-%d}')\n",
    "    print(f'Risk-free proxy annualized: {rf_annual * 100:.2f}%')\n",
    "    display(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Cumulative Return + Underwater Drawdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) == 0:\n",
    "    print('No return series available')\n",
    "else:\n",
    "    df_plot = df_snapshots[['date', 'daily_return']].dropna().copy()\n",
    "    df_plot['cum'] = (1 + df_plot['daily_return']).cumprod()\n",
    "    df_plot['run_max'] = df_plot['cum'].cummax()\n",
    "    df_plot['drawdown'] = (df_plot['cum'] / df_plot['run_max']) - 1\n",
    "\n",
    "    # Benchmark rebased to the same starting day used in strategy returns.\n",
    "    bench_aligned = df_bench.merge(df_plot[['date']], on='date', how='inner').copy()\n",
    "    bench_aligned['bench_cum'] = (1 + bench_aligned['bench_return']).cumprod()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "    axes[0].plot(df_plot['date'], (df_plot['cum'] - 1) * 100, linewidth=2, label='Strategy')\n",
    "    if len(bench_aligned):\n",
    "        axes[0].plot(bench_aligned['date'], (bench_aligned['bench_cum'] - 1) * 100, linewidth=2, alpha=0.8,\n",
    "                     label=f'{benchmark_symbol}')\n",
    "    axes[0].axhline(0, color='black', alpha=0.3)\n",
    "    axes[0].set_title('Cumulative Return', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Return (%)')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].fill_between(df_plot['date'], 0, df_plot['drawdown'] * 100, color='firebrick', alpha=0.5)\n",
    "    axes[1].plot(df_plot['date'], df_plot['drawdown'] * 100, color='darkred', linewidth=1)\n",
    "    axes[1].set_title('Underwater (Drawdown)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Drawdown (%)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    i = df_plot['drawdown'].idxmin()\n",
    "    axes[1].scatter(df_plot.loc[i, 'date'], df_plot.loc[i, 'drawdown'] * 100, color='black', zorder=5)\n",
    "    axes[1].annotate(\n",
    "        f\"Max DD: {df_plot.loc[i, 'drawdown'] * 100:.2f}%\",\n",
    "        (df_plot.loc[i, 'date'], df_plot.loc[i, 'drawdown'] * 100),\n",
    "        xytext=(10, 10), textcoords='offset points', fontsize=9,\n",
    "        bbox=dict(boxstyle='round', fc='white', ec='black', alpha=0.8)\n",
    "    )\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3: Monthly Return Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) == 0:\n",
    "    print('No return series available')\n",
    "else:\n",
    "    df_m = df_snapshots[['date', 'nav']].dropna().copy()\n",
    "    df_m['year'] = df_m['date'].dt.year\n",
    "    df_m['month'] = df_m['date'].dt.month\n",
    "\n",
    "    monthly = df_m.groupby(['year', 'month']).agg(nav_start=('nav', 'first'), nav_end=('nav', 'last')).reset_index()\n",
    "    monthly['monthly_return'] = (monthly['nav_end'] / monthly['nav_start']) - 1\n",
    "\n",
    "    heat = monthly.pivot(index='year', columns='month', values='monthly_return') * 100\n",
    "    month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    heat.columns = [month_labels[m - 1] for m in heat.columns]\n",
    "\n",
    "    plt.figure(figsize=(12, max(4, 0.8 * len(heat))))\n",
    "    sns.heatmap(heat, annot=True, fmt='.1f', cmap='RdYlGn', center=0, linewidths=1,\n",
    "                cbar_kws={'label': 'Return (%)'})\n",
    "    plt.title('Monthly Returns Heatmap (%)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Year')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 4: Rolling Sharpe, Sortino, Volatility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) < 30:\n",
    "    print('Insufficient observations for rolling metrics')\n",
    "else:\n",
    "    window = min(252, max(30, len(returns) // 3))\n",
    "\n",
    "    ret_vals = returns.reset_index(drop=True)\n",
    "    rf_vals = df_snapshots['rf_daily'].dropna().reset_index(drop=True)\n",
    "    rf_vals = rf_vals.iloc[:len(ret_vals)]\n",
    "\n",
    "    ex = pd.Series(ret_vals.values - rf_vals.values)\n",
    "\n",
    "    rolling_sharpe = (ex.rolling(window).mean() / ret_vals.rolling(window).std()) * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "    def _rolling_sortino(x):\n",
    "        idx = x.index\n",
    "        ex_i = x.values - rf_vals.iloc[idx].values\n",
    "        down = ex_i[ex_i < 0]\n",
    "        if len(down) == 0:\n",
    "            return np.nan\n",
    "        dstd = np.sqrt((down ** 2).mean())\n",
    "        if dstd == 0:\n",
    "            return np.nan\n",
    "        return (ex_i.mean() / dstd) * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "    rolling_sortino = ret_vals.rolling(window).apply(_rolling_sortino, raw=False)\n",
    "    rolling_vol = ret_vals.rolling(window).std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "    plot_dates = df_snapshots[['date']].dropna().iloc[1:].reset_index(drop=True)\n",
    "    n = min(len(plot_dates), len(rolling_sharpe), len(rolling_sortino), len(rolling_vol))\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "    axes[0].plot(plot_dates['date'].iloc[:n], rolling_sharpe.iloc[:n], linewidth=2, color='steelblue')\n",
    "    axes[0].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0].axhline(1, color='green', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_title(f'Rolling Sharpe ({window}D)', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('Sharpe')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].plot(plot_dates['date'].iloc[:n], rolling_sortino.iloc[:n], linewidth=2, color='darkorange')\n",
    "    axes[1].axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[1].axhline(1, color='green', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_title(f'Rolling Sortino ({window}D)', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_ylabel('Sortino')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[2].plot(plot_dates['date'].iloc[:n], 100 * rolling_vol.iloc[:n], linewidth=2, color='purple')\n",
    "    axes[2].set_title(f'Rolling Volatility ({window}D)', fontsize=13, fontweight='bold')\n",
    "    axes[2].set_ylabel('Vol (%)')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 5: Rolling Beta + Return Scatter vs Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(merged) < 30:\n",
    "    print('Insufficient benchmark overlap for beta analytics')\n",
    "else:\n",
    "    windows = [w for w in [20, 60, 252] if w <= len(merged)]\n",
    "\n",
    "    beta_frame = merged.copy()\n",
    "    for w in windows:\n",
    "        cov = beta_frame['daily_return'].rolling(w).cov(beta_frame['bench_return'])\n",
    "        var = beta_frame['bench_return'].rolling(w).var()\n",
    "        beta_frame[f'beta_{w}'] = cov / var\n",
    "\n",
    "    full_beta = np.cov(beta_frame['daily_return'], beta_frame['bench_return'], ddof=1)[0, 1] / np.var(beta_frame['bench_return'], ddof=1)\n",
    "    full_corr = beta_frame['daily_return'].corr(beta_frame['bench_return'])\n",
    "    full_r2 = full_corr ** 2\n",
    "    full_alpha = (1 + (beta_frame['daily_return'].mean() - full_beta * beta_frame['bench_return'].mean())) ** TRADING_DAYS - 1\n",
    "\n",
    "    up = beta_frame['bench_return'] > 0\n",
    "    down = beta_frame['bench_return'] < 0\n",
    "    up_beta = np.nan\n",
    "    down_beta = np.nan\n",
    "    if up.sum() > 10:\n",
    "        up_beta = np.cov(beta_frame.loc[up, 'daily_return'], beta_frame.loc[up, 'bench_return'], ddof=1)[0, 1] / np.var(beta_frame.loc[up, 'bench_return'], ddof=1)\n",
    "    if down.sum() > 10:\n",
    "        down_beta = np.cov(beta_frame.loc[down, 'daily_return'], beta_frame.loc[down, 'bench_return'], ddof=1)[0, 1] / np.var(beta_frame.loc[down, 'bench_return'], ddof=1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    for w in windows:\n",
    "        axes[0].plot(beta_frame['date'], beta_frame[f'beta_{w}'], linewidth=2, label=f'{w}D beta')\n",
    "    axes[0].axhline(1.0, color='black', linestyle='--', alpha=0.6)\n",
    "    axes[0].axhline(0.0, color='gray', linestyle=':', alpha=0.8)\n",
    "    axes[0].set_title(f'Rolling Beta vs {benchmark_symbol}', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('Beta')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    x = beta_frame['bench_return'].values\n",
    "    y = beta_frame['daily_return'].values\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    xline = np.linspace(x.min(), x.max(), 100)\n",
    "    yline = slope * xline + intercept\n",
    "\n",
    "    axes[1].scatter(100 * x, 100 * y, alpha=0.35, s=20, color='#1f77b4')\n",
    "    axes[1].plot(100 * xline, 100 * yline, color='crimson', linewidth=2)\n",
    "    axes[1].axhline(0, color='black', alpha=0.5)\n",
    "    axes[1].axvline(0, color='black', alpha=0.5)\n",
    "    axes[1].set_title(f'Daily Return Scatter vs {benchmark_symbol}', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel(f'{benchmark_symbol} Return (%)')\n",
    "    axes[1].set_ylabel('Strategy Return (%)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    summary_text = \"\\n\".join([\n",
    "        f'Beta: {full_beta:.3f}',\n",
    "        f'Corr: {full_corr:.3f}',\n",
    "        f'R2: {full_r2:.3f}',\n",
    "        f'Alpha (ann): {full_alpha * 100:.2f}%',\n",
    "        f'Up beta: {up_beta:.3f}',\n",
    "        f'Down beta: {down_beta:.3f}',\n",
    "    ])\n",
    "    axes[1].text(0.02, 0.98, summary_text, transform=axes[1].transAxes, va='top', fontsize=9,\n",
    "                 bbox=dict(boxstyle='round', fc='white', ec='gray', alpha=0.9))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 6: Exposure Time Series (Gross / Net / Long / Short)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needed = ['gross_exposure', 'net_exposure', 'long_exposure', 'short_exposure']\n",
    "cols = [c for c in needed if c in df_snapshots.columns]\n",
    "\n",
    "if len(cols) < 2:\n",
    "    print('Insufficient exposure columns in snapshots')\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for c in cols:\n",
    "        ax.plot(df_snapshots['date'], 100 * df_snapshots[c], linewidth=1.8, label=c)\n",
    "\n",
    "    ax.axhline(0, color='black', alpha=0.4)\n",
    "    ax.set_title('Portfolio Exposures Over Time', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Exposure (% NAV)')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('Exposure stats:')\n",
    "    display(df_snapshots[cols].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 7: VaR / CVaR (Static + Rolling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) < 30:\n",
    "    print('Insufficient data for VaR/CVaR')\n",
    "else:\n",
    "    rows = []\n",
    "    for cl in [0.95, 0.99]:\n",
    "        v = historical_var(returns, cl)\n",
    "        c = historical_cvar(returns, cl)\n",
    "        rows.append({\n",
    "            'Confidence': f'{int(cl * 100)}%',\n",
    "            'VaR (daily)': v,\n",
    "            'CVaR (daily)': c,\n",
    "            'VaR (annualized)': v * np.sqrt(TRADING_DAYS),\n",
    "            'CVaR (annualized)': c * np.sqrt(TRADING_DAYS),\n",
    "        })\n",
    "    stat = pd.DataFrame(rows)\n",
    "    print('Static VaR/CVaR')\n",
    "    display(stat)\n",
    "\n",
    "    windows = [w for w in [20, 60, 252] if w <= len(returns)]\n",
    "    plot_dates = df_snapshots[['date']].dropna().iloc[1:].reset_index(drop=True)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    ret_reset = returns.reset_index(drop=True)\n",
    "\n",
    "    for w in windows:\n",
    "        rv = ret_reset.rolling(w).apply(lambda x: historical_var(pd.Series(x), 0.95), raw=False)\n",
    "        rc = ret_reset.rolling(w).apply(lambda x: historical_cvar(pd.Series(x), 0.95), raw=False)\n",
    "        n = min(len(plot_dates), len(rv), len(rc))\n",
    "        axes[0].plot(plot_dates['date'].iloc[:n], 100 * rv.iloc[:n], linewidth=1.8, label=f'VaR {w}D')\n",
    "        axes[1].plot(plot_dates['date'].iloc[:n], 100 * rc.iloc[:n], linewidth=1.8, label=f'CVaR {w}D')\n",
    "\n",
    "    axes[0].set_title('Rolling 95% VaR', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('VaR (%)')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].set_title('Rolling 95% CVaR', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_ylabel('CVaR (%)')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].legend(loc='upper left')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 8: Return Distribution (Daily + Monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) == 0:\n",
    "    print('No returns available')\n",
    "else:\n",
    "    df_m = df_snapshots[['date', 'nav']].dropna().copy()\n",
    "    df_m['year'] = df_m['date'].dt.year\n",
    "    df_m['month'] = df_m['date'].dt.month\n",
    "    monthly = df_m.groupby(['year', 'month']).agg(nav_start=('nav', 'first'), nav_end=('nav', 'last')).reset_index()\n",
    "    monthly['monthly_return'] = (monthly['nav_end'] / monthly['nav_start']) - 1\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    axes[0].hist(100 * returns, bins=50, color='steelblue', alpha=0.75, edgecolor='black')\n",
    "    axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[0].axvline(100 * returns.mean(), color='green', linestyle='--', linewidth=2,\n",
    "                    label=f'Mean: {100 * returns.mean():.3f}%')\n",
    "    axes[0].set_title('Daily Return Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Daily Return (%)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].hist(100 * monthly['monthly_return'], bins=20, color='coral', alpha=0.75, edgecolor='black')\n",
    "    axes[1].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "    axes[1].axvline(100 * monthly['monthly_return'].mean(), color='green', linestyle='--', linewidth=2,\n",
    "                    label=f'Mean: {100 * monthly[\"monthly_return\"].mean():.2f}%')\n",
    "    axes[1].set_title('Monthly Return Distribution', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Monthly Return (%)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 9: Concentration Risk (Top-N Shares + HHI + Effective N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is None or not {'date', 'symbol', 'weight'}.issubset(df_positions.columns):\n",
    "    print('positions.csv missing required columns for concentration plot')\n",
    "else:\n",
    "    pos = df_positions.copy()\n",
    "\n",
    "    if 'invested' in pos.columns:\n",
    "        inv = pd.to_numeric(pos['invested'], errors='coerce').fillna(0)\n",
    "        pos = pos[inv > 0].copy()\n",
    "\n",
    "    pos['abs_weight'] = pos['weight'].abs()\n",
    "\n",
    "    recs = []\n",
    "    for d, g in pos.groupby('date'):\n",
    "        w = g['abs_weight'].dropna()\n",
    "        tot = w.sum()\n",
    "        if tot <= 0:\n",
    "            continue\n",
    "        s = (w / tot).sort_values(ascending=False)\n",
    "        hhi = float((s ** 2).sum())\n",
    "        recs.append({\n",
    "            'date': d,\n",
    "            'top_1_share': float(s.iloc[:1].sum()),\n",
    "            'top_3_share': float(s.iloc[:3].sum()),\n",
    "            'top_5_share': float(s.iloc[:5].sum()),\n",
    "            'top_10_share': float(s.iloc[:10].sum()),\n",
    "            'hhi': hhi,\n",
    "            'effective_n': (1.0 / hhi) if hhi > 0 else np.nan,\n",
    "            'n_positions': int(len(g))\n",
    "        })\n",
    "\n",
    "    conc = pd.DataFrame(recs).sort_values('date')\n",
    "    if len(conc) == 0:\n",
    "        print('No concentration rows computed')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "        axes[0].plot(conc['date'], 100 * conc['top_1_share'], linewidth=1.8, label='Top 1')\n",
    "        axes[0].plot(conc['date'], 100 * conc['top_3_share'], linewidth=1.8, label='Top 3')\n",
    "        axes[0].plot(conc['date'], 100 * conc['top_5_share'], linewidth=1.8, label='Top 5')\n",
    "        axes[0].plot(conc['date'], 100 * conc['top_10_share'], linewidth=1.8, label='Top 10')\n",
    "        axes[0].set_title('Top-N Share of Gross Exposure', fontsize=13, fontweight='bold')\n",
    "        axes[0].set_ylabel('Share (%)')\n",
    "        axes[0].legend(loc='upper right')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].plot(conc['date'], conc['hhi'], linewidth=1.8, color='crimson', label='HHI')\n",
    "        axes[1].plot(conc['date'], conc['effective_n'], linewidth=1.8, color='steelblue', label='Effective N')\n",
    "        axes[1].set_title('HHI and Effective Number of Bets', fontsize=13, fontweight='bold')\n",
    "        axes[1].set_ylabel('Value')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].legend(loc='upper right')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        latest = conc.iloc[-1]\n",
    "        print('Latest concentration snapshot:')\n",
    "        print(f\"Top1={100*latest['top_1_share']:.2f}% | Top3={100*latest['top_3_share']:.2f}% | \"\n",
    "              f\"Top5={100*latest['top_5_share']:.2f}% | HHI={latest['hhi']:.4f} | EffectiveN={latest['effective_n']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 10: Slippage Distribution + Cumulative Slippage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_slippage is None or len(df_slippage) == 0:\n",
    "    print('No slippage data available')\n",
    "else:\n",
    "    slp = df_slippage.copy()\n",
    "\n",
    "    if not {'slippage_dollars', 'expected_price', 'quantity'}.issubset(slp.columns):\n",
    "        print('slippage.csv missing required columns')\n",
    "    else:\n",
    "        denom = (slp['expected_price'].abs() * slp['quantity'].abs()).replace(0, np.nan)\n",
    "        slp['slippage_bps'] = 10000.0 * slp['slippage_dollars'] / denom\n",
    "\n",
    "        daily = slp.groupby('date', as_index=False)['slippage_dollars'].sum().sort_values('date')\n",
    "        daily['cumulative_slippage'] = daily['slippage_dollars'].cumsum()\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        axes[0].hist(slp['slippage_bps'].dropna(), bins=50, color='steelblue', alpha=0.75, edgecolor='black')\n",
    "        axes[0].axvline(0, color='red', linestyle='--', linewidth=2)\n",
    "        med = float(slp['slippage_bps'].median())\n",
    "        axes[0].axvline(med, color='green', linestyle='--', linewidth=2, label=f'Median: {med:.2f} bps')\n",
    "        axes[0].set_title('Per-Fill Slippage Distribution', fontsize=13, fontweight='bold')\n",
    "        axes[0].set_xlabel('Slippage (bps)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].plot(daily['date'], daily['cumulative_slippage'], linewidth=2, color='coral')\n",
    "        axes[1].fill_between(daily['date'], 0, daily['cumulative_slippage'], alpha=0.2, color='coral')\n",
    "        axes[1].set_title('Cumulative Slippage Over Time', fontsize=13, fontweight='bold')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel('Cumulative Slippage ($)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print('Slippage summary:')\n",
    "        print(f\"Fills: {len(slp):,}\")\n",
    "        print(f\"Total slippage: ${slp['slippage_dollars'].sum():,.2f}\")\n",
    "        print(f\"Mean bps: {slp['slippage_bps'].mean():.2f} | Median bps: {slp['slippage_bps'].median():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 11: P&L by Long vs Short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is None or len(df_positions) == 0:\n",
    "    print('No positions data available')\n",
    "else:\n",
    "    pos = df_positions.copy()\n",
    "\n",
    "    pnl_col = None\n",
    "    for c in ['daily_total_net_pnl', 'daily_pnl', 'daily_unrealized_pnl']:\n",
    "        if c in pos.columns:\n",
    "            pnl_col = c\n",
    "            break\n",
    "\n",
    "    if pnl_col is None or 'weight' not in pos.columns:\n",
    "        print('Required columns for side-PnL plot not available')\n",
    "    else:\n",
    "        pos['side'] = np.where(pos['weight'] > 0, 'Long', np.where(pos['weight'] < 0, 'Short', 'Flat'))\n",
    "        pos = pos[pos['side'] != 'Flat']\n",
    "\n",
    "        pnl = pos.groupby(['date', 'side'])[pnl_col].sum().unstack(fill_value=0).sort_index()\n",
    "        for c in ['Long', 'Short']:\n",
    "            if c not in pnl.columns:\n",
    "                pnl[c] = 0.0\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "        axes[0].bar(pnl.index, pnl['Long'], color='#2ca02c', alpha=0.7, label='Long')\n",
    "        axes[0].bar(pnl.index, pnl['Short'], bottom=pnl['Long'], color='#9467bd', alpha=0.7, label='Short')\n",
    "        axes[0].axhline(0, color='black', alpha=0.3)\n",
    "        axes[0].set_title('Daily P&L by Side', fontsize=13, fontweight='bold')\n",
    "        axes[0].set_ylabel('P&L ($)')\n",
    "        axes[0].legend(loc='upper left')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].plot(pnl.index, pnl['Long'].cumsum(), linewidth=2, color='#2ca02c', label='Long cumulative')\n",
    "        axes[1].plot(pnl.index, pnl['Short'].cumsum(), linewidth=2, color='#9467bd', label='Short cumulative')\n",
    "        axes[1].plot(pnl.index, (pnl['Long'] + pnl['Short']).cumsum(), linewidth=2, linestyle='--', color='steelblue',\n",
    "                     label='Total cumulative')\n",
    "        axes[1].axhline(0, color='black', alpha=0.3)\n",
    "        axes[1].set_title('Cumulative P&L by Side', fontsize=13, fontweight='bold')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].set_ylabel('Cumulative P&L ($)')\n",
    "        axes[1].legend(loc='upper left')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 12: Top / Bottom Contributors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is None or len(df_positions) == 0:\n",
    "    print('No positions data available')\n",
    "else:\n",
    "    pnl_col = None\n",
    "    for c in ['daily_total_net_pnl', 'daily_pnl', 'daily_unrealized_pnl']:\n",
    "        if c in df_positions.columns:\n",
    "            pnl_col = c\n",
    "            break\n",
    "\n",
    "    if pnl_col is None or 'symbol' not in df_positions.columns:\n",
    "        print('Required columns for contributors plot not available')\n",
    "    else:\n",
    "        sym = df_positions.groupby('symbol')[pnl_col].sum().sort_values()\n",
    "        if len(sym) == 0:\n",
    "            print('No contributor rows available')\n",
    "        else:\n",
    "            n = min(10, len(sym))\n",
    "            combined = pd.concat([sym.head(n), sym.tail(n)])\n",
    "            colors = ['#d62728' if v < 0 else '#2ca02c' for v in combined.values]\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, max(6, 0.35 * len(combined))))\n",
    "            ax.barh(combined.index.astype(str), combined.values, color=colors, alpha=0.85)\n",
    "            ax.axvline(0, color='black', alpha=0.3)\n",
    "            ax.set_title(f'Top {n} and Bottom {n} Contributors', fontsize=13, fontweight='bold')\n",
    "            ax.set_xlabel('Total P&L ($)')\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 13: Turnover Over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is None or len(df_positions) == 0 or not {'date', 'symbol', 'weight'}.issubset(df_positions.columns):\n",
    "    print('positions.csv missing required columns for turnover')\n",
    "else:\n",
    "    w = df_positions.pivot_table(index='date', columns='symbol', values='weight', aggfunc='last', fill_value=0).sort_index()\n",
    "    daily_turn = 0.5 * w.diff().abs().sum(axis=1)\n",
    "\n",
    "    cycle_days = infer_cycle_days(df_targets, df_snapshots)\n",
    "    roll = daily_turn.rolling(cycle_days, min_periods=max(2, cycle_days // 2)).sum()\n",
    "\n",
    "    years = len(daily_turn) / TRADING_DAYS if len(daily_turn) else np.nan\n",
    "    ann_turn = daily_turn.sum() / years if years and years > 0 else np.nan\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(roll.index, 100 * roll.values, linewidth=1.8, color='steelblue')\n",
    "    ax.fill_between(roll.index, 0, 100 * roll.values, alpha=0.15, color='steelblue')\n",
    "    ax.set_title(f'Rolling Turnover ({cycle_days} trading days)', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Turnover (% NAV)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if pd.notna(ann_turn):\n",
    "        ax.text(0.02, 0.95, f'Annualized turnover: {100 * ann_turn:.1f}%', transform=ax.transAxes,\n",
    "                va='top', fontsize=10, bbox=dict(boxstyle='round', fc='wheat', alpha=0.5))\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 14: Position Count Over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'num_positions' not in df_snapshots.columns:\n",
    "    print('num_positions not found in snapshots')\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    ax.plot(df_snapshots['date'], df_snapshots['num_positions'], linewidth=1.8, color='steelblue')\n",
    "    ax.fill_between(df_snapshots['date'], 0, df_snapshots['num_positions'], alpha=0.15, color='steelblue')\n",
    "    ax.set_title('Open Position Count Over Time', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Positions')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 15: Estimated Vol vs Realized Vol vs Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(returns) < 20:\n",
    "    print('Insufficient data for volatility comparison')\n",
    "else:\n",
    "    est_col = 'estimated_vol' if 'estimated_vol' in df_snapshots.columns else None\n",
    "\n",
    "    # Dynamic target from strategy metadata.\n",
    "    target_vol_annual = STRATEGY.get('target_vol_annual')\n",
    "\n",
    "    ret_series = returns.reset_index(drop=True)\n",
    "    realized_window = min(60, max(20, len(ret_series) // 4))\n",
    "    realized_vol = ret_series.rolling(realized_window).std() * np.sqrt(TRADING_DAYS)\n",
    "\n",
    "    plot_dates = df_snapshots[['date']].dropna().iloc[1:].reset_index(drop=True)\n",
    "    n = min(len(plot_dates), len(realized_vol))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    if est_col:\n",
    "        est = pd.to_numeric(df_snapshots[est_col], errors='coerce')\n",
    "        ax.plot(df_snapshots['date'], 100 * est, linewidth=1.8, color='coral', label='Estimated vol')\n",
    "\n",
    "    ax.plot(plot_dates['date'].iloc[:n], 100 * realized_vol.iloc[:n], linewidth=1.8, color='steelblue',\n",
    "            label=f'Realized vol ({realized_window}D)')\n",
    "\n",
    "    if target_vol_annual is not None:\n",
    "        ax.axhline(100 * float(target_vol_annual), color='green', linestyle='--', linewidth=1.6,\n",
    "                   label=f'Target vol ({100 * float(target_vol_annual):.1f}%)')\n",
    "\n",
    "    ax.set_title('Estimated vs Realized Volatility', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Annualized Volatility (%)')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 16: Order Lifecycle Quality (Status Mix + Fill Ratio by Tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_orders is None or len(df_orders) == 0 or 'order_id' not in df_orders.columns:\n",
    "    print('No order lifecycle data available')\n",
    "else:\n",
    "    ev = df_orders.copy().sort_values('date')\n",
    "\n",
    "    # Tier from tags when present; no hardcoded fallback mapping to strategy internals.\n",
    "    if 'tier_tag' not in ev.columns:\n",
    "        ev['tier_tag'] = 'unknown'\n",
    "\n",
    "    grp = ev.groupby('order_id', as_index=False)\n",
    "    order_summary = grp.agg(\n",
    "        symbol=('symbol', 'first') if 'symbol' in ev.columns else ('order_id', 'first'),\n",
    "        tier=('tier_tag', 'first'),\n",
    "        quantity=('quantity', 'first') if 'quantity' in ev.columns else ('order_id', 'size'),\n",
    "        submitted_at=('date', 'min'),\n",
    "        final_at=('date', 'max')\n",
    "    )\n",
    "\n",
    "    if 'status' in ev.columns:\n",
    "        final_status = ev.groupby('order_id').tail(1)[['order_id', 'status']].rename(columns={'status': 'final_status'})\n",
    "        order_summary = order_summary.merge(final_status, on='order_id', how='left')\n",
    "    else:\n",
    "        order_summary['final_status'] = 'unknown'\n",
    "\n",
    "    if 'fill_quantity' in ev.columns:\n",
    "        fills = ev.groupby('order_id', as_index=False)['fill_quantity'].sum().rename(columns={'fill_quantity': 'filled_qty'})\n",
    "        order_summary = order_summary.merge(fills, on='order_id', how='left')\n",
    "        qty = pd.to_numeric(order_summary['quantity'], errors='coerce').abs().replace(0, np.nan)\n",
    "        order_summary['fill_ratio'] = (pd.to_numeric(order_summary['filled_qty'], errors='coerce').abs() / qty).fillna(0).clip(0, 1)\n",
    "    else:\n",
    "        order_summary['fill_ratio'] = np.nan\n",
    "\n",
    "    order_summary['days_to_final'] = (order_summary['final_at'] - order_summary['submitted_at']).dt.days.fillna(0)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    order_summary['final_status'].value_counts().plot(kind='bar', ax=axes[0], color='#1f77b4')\n",
    "    axes[0].set_title('Final Order Status Counts', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "    tier_order = sorted(order_summary['tier'].dropna().unique().tolist())\n",
    "    sns.boxplot(data=order_summary, x='tier', y='fill_ratio', order=tier_order, ax=axes[1])\n",
    "    axes[1].set_title('Fill Ratio by Tier (from order tags)', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Tier')\n",
    "    axes[1].set_ylabel('Fill ratio')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 17: Scaling Adherence (Planned vs Actual Progress)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_targets is None or len(df_targets) == 0:\n",
    "    print('No targets data available')\n",
    "else:\n",
    "    needed = {'week_id', 'symbol', 'start_w', 'weekly_target_w', 'scheduled_w', 'actual_w', 'date'}\n",
    "    if not needed.issubset(df_targets.columns):\n",
    "        print(f'targets.csv missing required columns: {sorted(needed - set(df_targets.columns))}')\n",
    "    else:\n",
    "        d = df_targets.copy().sort_values(['week_id', 'symbol', 'date'])\n",
    "\n",
    "        for col in ['start_w', 'weekly_target_w', 'scheduled_w', 'actual_w', 'scale_day']:\n",
    "            if col in d.columns:\n",
    "                d[col] = pd.to_numeric(d[col], errors='coerce')\n",
    "\n",
    "        order_delta = d['weekly_target_w'] - d['start_w']\n",
    "        total = order_delta.abs()\n",
    "        d = d[total > 1e-10].copy()\n",
    "        d['order_dir'] = np.sign(order_delta.loc[d.index])\n",
    "        d['total_order_abs'] = total.loc[d.index]\n",
    "\n",
    "        # Prefer explicit trading-day index from strategy target state.\n",
    "        if 'scale_day' in d.columns and d['scale_day'].notna().any():\n",
    "            d['day_idx'] = d['scale_day'].round().astype('Int64')\n",
    "        else:\n",
    "            d['day_idx'] = d.groupby(['week_id', 'symbol']).cumcount().astype('Int64')\n",
    "\n",
    "        # If logger emits is_scaling, keep only active scaling rows.\n",
    "        if 'is_scaling' in d.columns:\n",
    "            raw = d['is_scaling']\n",
    "            is_scaling = (\n",
    "                raw.astype(str).str.strip().str.lower().isin(['true', '1', 'yes', 'y']) |\n",
    "                pd.to_numeric(raw, errors='coerce').fillna(0).astype(float).gt(0)\n",
    "            )\n",
    "            d = d[is_scaling].copy()\n",
    "\n",
    "        d = d[d['day_idx'].notna() & (d['day_idx'] >= 0)].copy()\n",
    "\n",
    "        cycle_days = STRATEGY.get('scaling_days')\n",
    "        if cycle_days is None:\n",
    "            cycle_days = infer_cycle_days(df_targets, df_snapshots)\n",
    "        cycle_days = max(1, int(cycle_days))\n",
    "        d = d[d['day_idx'] < cycle_days].copy()\n",
    "\n",
    "        if len(d) == 0:\n",
    "            print('No active scaling rows after filtering')\n",
    "        else:\n",
    "            d['planned_progress'] = ((d['scheduled_w'] - d['start_w']).abs() / d['total_order_abs']).clip(0, 1)\n",
    "            d['actual_progress'] = ((d['actual_w'] - d['start_w']).abs() / d['total_order_abs']).clip(0, 1)\n",
    "            d['progress_gap'] = d['actual_progress'] - d['planned_progress']\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "            sns.boxplot(data=d, y='progress_gap', ax=axes[0], color='steelblue')\n",
    "            axes[0].axhline(0, color='black', alpha=0.5)\n",
    "            axes[0].set_title('Progress Gap Distribution (Actual - Planned)', fontsize=13, fontweight='bold')\n",
    "            axes[0].set_ylabel('Gap')\n",
    "            axes[0].grid(alpha=0.3)\n",
    "\n",
    "            profile = d.groupby('day_idx', as_index=False).agg(\n",
    "                planned=('planned_progress', 'mean'),\n",
    "                actual=('actual_progress', 'mean'),\n",
    "                rows=('day_idx', 'size')\n",
    "            ).sort_values('day_idx')\n",
    "\n",
    "            axes[1].plot(profile['day_idx'], profile['planned'], marker='o', linewidth=2, label='Planned')\n",
    "            axes[1].plot(profile['day_idx'], profile['actual'], marker='o', linewidth=2, label='Actual')\n",
    "            axes[1].set_title('Planned vs Actual Progress by Scale Day', fontsize=13, fontweight='bold')\n",
    "            axes[1].set_xlabel('Scale day index')\n",
    "            axes[1].set_ylabel('Progress')\n",
    "            axes[1].set_xticks(sorted(profile['day_idx'].astype(int).unique().tolist()))\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f'Using cycle_days={cycle_days}; plotted day_idx range: {int(profile[\"day_idx\"].min())}..{int(profile[\"day_idx\"].max())}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 18: Stale Signal Risk Proxy (Adverse Move During Scaling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_targets is None or len(df_targets) == 0:\n",
    "    print('Need targets.csv for stale-signal risk proxy')\n",
    "else:\n",
    "    req_t = {'date', 'week_id', 'symbol', 'start_w', 'weekly_target_w'}\n",
    "    if not req_t.issubset(df_targets.columns):\n",
    "        print(f'Required target columns missing: {sorted(req_t - set(df_targets.columns))}')\n",
    "    else:\n",
    "        tgt = df_targets[list(req_t.union({'scale_day', 'is_scaling'}).intersection(df_targets.columns))].copy()\n",
    "        tgt['date'] = pd.to_datetime(tgt['date'])\n",
    "\n",
    "        for col in ['start_w', 'weekly_target_w', 'scale_day']:\n",
    "            if col in tgt.columns:\n",
    "                tgt[col] = pd.to_numeric(tgt[col], errors='coerce')\n",
    "\n",
    "        tgt['order_delta_w'] = tgt['weekly_target_w'] - tgt['start_w']\n",
    "        tgt['order_dir'] = np.sign(tgt['order_delta_w'])\n",
    "        tgt['order_abs'] = tgt['order_delta_w'].abs()\n",
    "        tgt = tgt[tgt['order_abs'] > 1e-10].copy()\n",
    "\n",
    "        if len(tgt) == 0:\n",
    "            print('No non-zero weekly orders found for stale-signal analysis')\n",
    "        else:\n",
    "            if 'scale_day' in tgt.columns and tgt['scale_day'].notna().any():\n",
    "                tgt['day_idx'] = tgt['scale_day'].round().astype('Int64')\n",
    "            else:\n",
    "                tgt = tgt.sort_values(['week_id', 'symbol', 'date'])\n",
    "                tgt['day_idx'] = tgt.groupby(['week_id', 'symbol']).cumcount().astype('Int64')\n",
    "\n",
    "            if 'is_scaling' in tgt.columns:\n",
    "                raw = tgt['is_scaling']\n",
    "                is_scaling = (\n",
    "                    raw.astype(str).str.strip().str.lower().isin(['true', '1', 'yes', 'y']) |\n",
    "                    pd.to_numeric(raw, errors='coerce').fillna(0).astype(float).gt(0)\n",
    "                )\n",
    "                tgt = tgt[is_scaling].copy()\n",
    "\n",
    "            cycle_days = STRATEGY.get('scaling_days')\n",
    "            if cycle_days is None:\n",
    "                cycle_days = infer_cycle_days(df_targets, df_snapshots)\n",
    "            cycle_days = max(1, int(cycle_days))\n",
    "\n",
    "            tgt = tgt[tgt['day_idx'].notna() & (tgt['day_idx'] >= 0) & (tgt['day_idx'] < cycle_days)].copy()\n",
    "\n",
    "            if len(tgt) == 0:\n",
    "                print('No scaling rows after day-index filtering')\n",
    "            else:\n",
    "                # Primary price source: positions log.\n",
    "                px = pd.DataFrame(columns=['date', 'symbol', 'price'])\n",
    "                if df_positions is not None and {'date', 'symbol', 'price'}.issubset(df_positions.columns):\n",
    "                    px = df_positions[['date', 'symbol', 'price']].copy()\n",
    "                    px['date'] = pd.to_datetime(px['date']).dt.normalize()\n",
    "                    px['price'] = pd.to_numeric(px['price'], errors='coerce')\n",
    "                    px = px.dropna(subset=['price']).drop_duplicates(['date', 'symbol'], keep='last')\n",
    "\n",
    "                tgt['date'] = pd.to_datetime(tgt['date']).dt.normalize()\n",
    "                m = tgt.merge(px, on=['date', 'symbol'], how='left')\n",
    "\n",
    "                # Fallback: QC price history for missing date-symbol pairs.\n",
    "                miss_rate = float(m['price'].isna().mean()) if len(m) else 1.0\n",
    "                if miss_rate > 0:\n",
    "                    syms = sorted(m.loc[m['price'].isna(), 'symbol'].dropna().unique().tolist())\n",
    "                    if len(syms):\n",
    "                        qc_map = {}\n",
    "                        for s in syms:\n",
    "                            try:\n",
    "                                qc_map[s] = qb.AddEquity(str(s), Resolution.Daily).Symbol\n",
    "                            except Exception:\n",
    "                                pass\n",
    "\n",
    "                        if len(qc_map):\n",
    "                            hist = qb.History(list(qc_map.values()), m['date'].min(), m['date'].max() + pd.Timedelta(days=1), Resolution.Daily)\n",
    "                            if hist is not None and len(hist):\n",
    "                                h = hist.reset_index()\n",
    "                                sym_col = 'symbol' if 'symbol' in h.columns else h.columns[0]\n",
    "                                if 'time' in h.columns:\n",
    "                                    tcol = 'time'\n",
    "                                elif 'end_time' in h.columns:\n",
    "                                    tcol = 'end_time'\n",
    "                                else:\n",
    "                                    tcol = h.columns[1]\n",
    "                                pcol = 'close' if 'close' in h.columns else ('Close' if 'Close' in h.columns else None)\n",
    "\n",
    "                                if pcol is not None:\n",
    "                                    h['symbol'] = h[sym_col].astype(str).str.split(' ').str[0]\n",
    "                                    h['date'] = pd.to_datetime(h[tcol]).dt.tz_localize(None).dt.normalize()\n",
    "                                    h['price_hist'] = pd.to_numeric(h[pcol], errors='coerce')\n",
    "                                    h = h[['date', 'symbol', 'price_hist']].dropna().drop_duplicates(['date', 'symbol'], keep='last')\n",
    "\n",
    "                                    m = m.merge(h, on=['date', 'symbol'], how='left')\n",
    "                                    m['price'] = m['price'].fillna(m['price_hist'])\n",
    "                                    m.drop(columns=['price_hist'], inplace=True)\n",
    "\n",
    "                # Rebalance reference: first available scaling-day price in each symbol-week.\n",
    "                ref = (\n",
    "                    m.dropna(subset=['price'])\n",
    "                     .sort_values(['week_id', 'symbol', 'day_idx', 'date'])\n",
    "                     .groupby(['week_id', 'symbol'], as_index=False)\n",
    "                     .first()[['week_id', 'symbol', 'price']]\n",
    "                     .rename(columns={'price': 'rebalance_price'})\n",
    "                )\n",
    "\n",
    "                m = m.merge(ref, on=['week_id', 'symbol'], how='left')\n",
    "                m['ret_since_rebal'] = (m['price'] / m['rebalance_price']) - 1.0\n",
    "                m['adverse_move'] = -m['order_dir'] * m['ret_since_rebal']\n",
    "\n",
    "                prof = (\n",
    "                    m.groupby('day_idx', as_index=False)\n",
    "                     .agg(mean_adverse_move=('adverse_move', 'mean'),\n",
    "                          med_adverse_move=('adverse_move', 'median'),\n",
    "                          obs=('adverse_move', lambda s: int(s.notna().sum())))\n",
    "                     .sort_values('day_idx')\n",
    "                )\n",
    "\n",
    "                if len(prof) == 0 or prof['obs'].sum() == 0:\n",
    "                    print('No valid adverse-move observations after price alignment')\n",
    "                else:\n",
    "                    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "                    ax.plot(prof['day_idx'], 100 * prof['mean_adverse_move'], marker='o', linewidth=2, color='firebrick', label='Mean')\n",
    "                    ax.plot(prof['day_idx'], 100 * prof['med_adverse_move'], marker='s', linewidth=1.5, color='steelblue', alpha=0.85, label='Median')\n",
    "                    ax.axhline(0, color='black', alpha=0.5)\n",
    "                    ax.set_title('Adverse Move During Scaling Window', fontsize=13, fontweight='bold')\n",
    "                    ax.set_xlabel('Scale day index')\n",
    "                    ax.set_ylabel('Adverse move (%)')\n",
    "                    ax.set_xticks(sorted(prof['day_idx'].astype(int).unique().tolist()))\n",
    "                    ax.legend()\n",
    "                    ax.grid(alpha=0.3)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "                    print(f'Cycle days used: {cycle_days}')\n",
    "                    print('Observation count by day:')\n",
    "                    display(prof[['day_idx', 'obs']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- This notebook intentionally derives strategy-specific settings (benchmark, volatility target, execution thresholds) from `main.py`.\n",
    "- If the strategy wiring changes substantially, rerun the metadata extraction cell and confirm parsed values.\n",
    "- Optional datasets (`slippage.csv`, `targets.csv`, `order_events.csv`) are handled with skip logic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}