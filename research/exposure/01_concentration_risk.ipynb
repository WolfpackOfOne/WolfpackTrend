{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concentration Risk\n",
    "\n",
    "Concentration diagnostics for the live portfolio weights using `wolfpack/positions.csv`.\n",
    "\n",
    "What this notebook covers:\n",
    "- Daily top-name and top-bucket concentration (`top_1`, `top_3`, `top_5`, `top_10`)\n",
    "- Herfindahl-Hirschman concentration index (HHI) and effective number of bets\n",
    "- Long/short concentration split\n",
    "- Latest-day concentration leaderboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions = read_csv_from_store('wolfpack/positions.csv')\n",
    "df_snapshots = read_csv_from_store('wolfpack/daily_snapshots.csv')\n",
    "\n",
    "if df_positions is None:\n",
    "    raise ValueError('positions.csv is required for concentration analysis.')\n",
    "\n",
    "if 'date' not in df_positions.columns or 'symbol' not in df_positions.columns:\n",
    "    raise ValueError('positions.csv must include date and symbol columns.')\n",
    "\n",
    "df_positions['date'] = pd.to_datetime(df_positions['date'], errors='coerce')\n",
    "df_positions = df_positions[df_positions['date'].notna()].copy()\n",
    "\n",
    "if 'weight' not in df_positions.columns:\n",
    "    if 'market_value' in df_positions.columns and df_snapshots is not None and 'nav' in df_snapshots.columns:\n",
    "        df_snapshots['date'] = pd.to_datetime(df_snapshots['date'], errors='coerce')\n",
    "        merged = df_positions.merge(df_snapshots[['date', 'nav']], on='date', how='left')\n",
    "        merged['weight'] = np.where(merged['nav'].abs() > 1e-9, merged['market_value'] / merged['nav'], 0.0)\n",
    "        df_positions = merged.drop(columns=['nav'])\n",
    "    else:\n",
    "        raise ValueError('positions.csv must include weight or market_value with snapshots nav.')\n",
    "\n",
    "df_positions['weight'] = pd.to_numeric(df_positions['weight'], errors='coerce').fillna(0.0)\n",
    "if 'invested' in df_positions.columns:\n",
    "    df_positions = df_positions[df_positions['invested'].astype(str).isin(['1', 'True', 'true']) | (df_positions['weight'].abs() > 1e-9)].copy()\n",
    "else:\n",
    "    df_positions = df_positions[df_positions['weight'].abs() > 1e-9].copy()\n",
    "\n",
    "print(f'Rows after filtering: {len(df_positions):,}')\n",
    "print(f'Date range: {df_positions[\"date\"].min().date()} to {df_positions[\"date\"].max().date()}')\n",
    "print(f'Unique symbols: {df_positions[\"symbol\"].nunique():,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _top_bucket_sum(weights, n):\n",
    "    if len(weights) == 0:\n",
    "        return np.nan\n",
    "    return weights.nlargest(n).sum()\n",
    "\n",
    "records = []\n",
    "for date, group in df_positions.groupby('date'):\n",
    "    abs_w = group['weight'].abs().astype(float)\n",
    "    gross_w = abs_w.sum()\n",
    "    if gross_w <= 1e-12:\n",
    "        continue\n",
    "\n",
    "    normalized = abs_w / gross_w\n",
    "    hhi = float((normalized ** 2).sum())\n",
    "    eff_n = float(1.0 / hhi) if hhi > 0 else np.nan\n",
    "\n",
    "    long_abs = group.loc[group['weight'] > 0, 'weight'].abs()\n",
    "    short_abs = group.loc[group['weight'] < 0, 'weight'].abs()\n",
    "\n",
    "    records.append({\n",
    "        'date': date,\n",
    "        'positions': int((abs_w > 0).sum()),\n",
    "        'gross_weight_sum': float(gross_w),\n",
    "        'top_1_abs_w': _top_bucket_sum(abs_w, 1),\n",
    "        'top_3_abs_w': _top_bucket_sum(abs_w, 3),\n",
    "        'top_5_abs_w': _top_bucket_sum(abs_w, 5),\n",
    "        'top_10_abs_w': _top_bucket_sum(abs_w, 10),\n",
    "        'top_1_share_of_gross': _top_bucket_sum(abs_w, 1) / gross_w,\n",
    "        'top_3_share_of_gross': _top_bucket_sum(abs_w, 3) / gross_w,\n",
    "        'top_5_share_of_gross': _top_bucket_sum(abs_w, 5) / gross_w,\n",
    "        'top_10_share_of_gross': _top_bucket_sum(abs_w, 10) / gross_w,\n",
    "        'hhi_abs_weight': hhi,\n",
    "        'effective_n_bets': eff_n,\n",
    "        'long_gross_abs': float(long_abs.sum()),\n",
    "        'short_gross_abs': float(short_abs.sum()),\n",
    "        'long_top_1_abs_w': _top_bucket_sum(long_abs, 1) if len(long_abs) > 0 else np.nan,\n",
    "        'short_top_1_abs_w': _top_bucket_sum(short_abs, 1) if len(short_abs) > 0 else np.nan,\n",
    "    })\n",
    "\n",
    "concentration = pd.DataFrame(records).sort_values('date').reset_index(drop=True)\n",
    "if concentration.empty:\n",
    "    raise ValueError('No active daily positions found for concentration analysis.')\n",
    "\n",
    "summary = {\n",
    "    'Days analyzed': len(concentration),\n",
    "    'Avg positions': concentration['positions'].mean(),\n",
    "    'Avg top-1 share of gross': concentration['top_1_share_of_gross'].mean(),\n",
    "    'Avg top-5 share of gross': concentration['top_5_share_of_gross'].mean(),\n",
    "    'Avg HHI': concentration['hhi_abs_weight'].mean(),\n",
    "    'Avg effective N': concentration['effective_n_bets'].mean(),\n",
    "}\n",
    "\n",
    "print('Concentration summary:')\n",
    "for k, v in summary.items():\n",
    "    if 'share' in k.lower() or k == 'Avg HHI':\n",
    "        print(f'  {k}: {100 * v:.2f}%')\n",
    "    elif 'effective' in k.lower() or 'positions' in k.lower():\n",
    "        print(f'  {k}: {v:.2f}')\n",
    "    else:\n",
    "        print(f'  {k}: {v}')\n",
    "\n",
    "display(concentration.tail(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "axes[0].plot(concentration['date'], 100 * concentration['top_1_share_of_gross'], label='Top 1', linewidth=2)\n",
    "axes[0].plot(concentration['date'], 100 * concentration['top_3_share_of_gross'], label='Top 3', linewidth=2)\n",
    "axes[0].plot(concentration['date'], 100 * concentration['top_5_share_of_gross'], label='Top 5', linewidth=2)\n",
    "axes[0].plot(concentration['date'], 100 * concentration['top_10_share_of_gross'], label='Top 10', linewidth=2, alpha=0.8)\n",
    "axes[0].set_title('Concentration Buckets as Share of Gross Exposure')\n",
    "axes[0].set_ylabel('Share of gross (%)')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(concentration['date'], concentration['hhi_abs_weight'], label='HHI', color='#d62728', linewidth=2)\n",
    "axes[1].plot(concentration['date'], concentration['effective_n_bets'], label='Effective N Bets', color='#1f77b4', linewidth=2)\n",
    "axes[1].set_title('HHI and Effective Number of Bets')\n",
    "axes[1].set_ylabel('Index / Count')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend(loc='upper left')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date = concentration['date'].max()\n",
    "latest_positions = (\n",
    "    df_positions[df_positions['date'] == latest_date][['symbol', 'weight']]\n",
    "      .copy()\n",
    ")\n",
    "latest_positions['abs_weight'] = latest_positions['weight'].abs()\n",
    "latest_positions = latest_positions.sort_values('abs_weight', ascending=False)\n",
    "\n",
    "latest_total_abs = latest_positions['abs_weight'].sum()\n",
    "latest_positions['share_of_gross_pct'] = np.where(\n",
    "    latest_total_abs > 1e-12,\n",
    "    100 * latest_positions['abs_weight'] / latest_total_abs,\n",
    "    0.0\n",
    ")\n",
    "\n",
    "print(f'Latest date: {latest_date.date()}')\n",
    "display(latest_positions.head(20))\n",
    "\n",
    "plot_n = min(12, len(latest_positions))\n",
    "plot_df = latest_positions.head(plot_n).iloc[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(plot_df['symbol'], plot_df['share_of_gross_pct'], color='#4c78a8')\n",
    "plt.title(f'Top {plot_n} Symbol Concentration ({latest_date.date()})')\n",
    "plt.xlabel('Share of gross exposure (%)')\n",
    "plt.ylabel('Symbol')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    'top_1_share_of_gross': 0.20,\n",
    "    'top_3_share_of_gross': 0.45,\n",
    "    'top_5_share_of_gross': 0.65,\n",
    "    'hhi_abs_weight': 0.12,\n",
    "}\n",
    "\n",
    "breach_rows = []\n",
    "for metric, threshold in thresholds.items():\n",
    "    series = concentration[metric].dropna()\n",
    "    breach_days = int((series > threshold).sum())\n",
    "    breach_rows.append({\n",
    "        'metric': metric,\n",
    "        'threshold': threshold,\n",
    "        'days_above': breach_days,\n",
    "        'pct_days_above': breach_days / len(series) if len(series) else np.nan,\n",
    "        'latest_value': concentration[metric].iloc[-1]\n",
    "    })\n",
    "\n",
    "breaches = pd.DataFrame(breach_rows)\n",
    "for col in ['threshold', 'pct_days_above', 'latest_value']:\n",
    "    breaches[col] = breaches[col].astype(float)\n",
    "\n",
    "print('Concentration breach scorecard:')\n",
    "display(breaches)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}