{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are We Trading And Why?\n",
    "\n",
    "This notebook turns raw signals into an explainable thesis card.\n",
    "\n",
    "It combines:\n",
    "- Signal anatomy (trend alignment + horizon contributions)\n",
    "- Trade inventory (which symbols, which side, how strong)\n",
    "- Evidence of edge (signed forward returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "from config import TEAM_ID\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j4mouxf4gql",
   "source": [
    "## Data Loading & Attribution Feature Engineering\n",
    "\n",
    "Loads signal and position data, then engineers several diagnostic columns: ATR-normalized distances from price to each SMA, individual horizon contributions to the composite score, an implied magnitude via tanh to cross-check logged values, and a consensus score measuring how many of the three trend horizons agree in direction. Forward 5-day signed returns are merged in from position prices to enable downstream edge measurement. The head display confirms all engineered columns are present before analysis proceeds."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "us99159i0i",
   "source": [
    "## Signal Attribution \u2014 4-Panel Explainability Overview\n",
    "\n",
    "These four plots together tell the story of which trades the strategy selects and why. The top-left bar chart ranks the most signal-active symbols with green/red coloring for net directional bias; the top-right shows which trend horizon (short, medium, long) drives the most composite score on average; the bottom-left breaks signal count down by market setup and direction; and the bottom-right scatter validates the tanh transformation by plotting composite score against logged magnitude. Together they answer the question: are signals coming from where we expect and in the form we intend?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "nnjo5sbopen",
   "source": [
    "## Edge by Setup, Consensus, and Tier Component Tables\n",
    "\n",
    "This section tests whether different trend configurations carry different forward return edge by producing three tables: edge by market setup (trend vs pullback vs mixed), edge by consensus bucket (all three horizons agree vs partial vs low), and average component contributions by tier. Each table reports hit rate, mean signed 5-day return, and average signal strength for direct comparison. Comparing setup and consensus buckets reveals whether the strategy's entry conditions are genuinely filtering for higher-quality trend setups."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4k23jdxzln",
   "source": [
    "## Signal Thesis Card\n",
    "\n",
    "This final section synthesizes the notebook findings into a three-row thesis card answering: what are we trading, why these trades, and where is edge strongest. Supporting evidence includes what fraction of all signals come from full trend-aligned setups and how concentrated the signal book is in the top 10 symbols. The thesis card is designed as a concise one-page summary that can be reviewed before deploying or tuning the strategy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signals = read_csv_from_store(f'{TEAM_ID}/signals.csv')\n",
    "df_positions = read_csv_from_store(f'{TEAM_ID}/positions.csv')\n",
    "\n",
    "if df_signals is None:\n",
    "    raise ValueError('signals.csv is required. Run a backtest with signal logging enabled.')\n",
    "\n",
    "required_cols = ['date', 'symbol', 'direction', 'magnitude', 'price', 'sma_short', 'sma_medium', 'sma_long', 'atr']\n",
    "missing = [c for c in required_cols if c not in df_signals.columns]\n",
    "if missing:\n",
    "    raise ValueError(f'signals.csv missing required columns: {missing}')\n",
    "\n",
    "df = df_signals.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "for col in ['magnitude', 'price', 'sma_short', 'sma_medium', 'sma_long', 'atr']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df['direction'] = df['direction'].astype(str).str.title()\n",
    "df['direction'] = np.where(df['direction'].isin(['Up', 'Down']), df['direction'], np.where(df['magnitude'] >= 0, 'Up', 'Down'))\n",
    "df['direction_sign'] = np.where(df['direction'].eq('Up'), 1.0, -1.0)\n",
    "df['abs_magnitude'] = df['magnitude'].abs()\n",
    "df['tier'] = np.select(\n",
    "    [df['abs_magnitude'] >= 0.7, df['abs_magnitude'] >= 0.3],\n",
    "    ['strong', 'moderate'],\n",
    "    default='weak'\n",
    ")\n",
    "\n",
    "safe_atr = df['atr'].replace(0, np.nan)\n",
    "df['dist_short'] = (df['price'] - df['sma_short']) / safe_atr\n",
    "df['dist_medium'] = (df['price'] - df['sma_medium']) / safe_atr\n",
    "df['dist_long'] = (df['price'] - df['sma_long']) / safe_atr\n",
    "\n",
    "df['contrib_short'] = 0.5 * df['dist_short']\n",
    "df['contrib_medium'] = 0.3 * df['dist_medium']\n",
    "df['contrib_long'] = 0.2 * df['dist_long']\n",
    "df['composite_score'] = df['contrib_short'] + df['contrib_medium'] + df['contrib_long']\n",
    "df['implied_magnitude'] = np.tanh(df['composite_score'])\n",
    "df['magnitude_gap'] = df['magnitude'] - df['implied_magnitude']\n",
    "\n",
    "df['consensus_score'] = np.abs(np.sign(df['dist_short']) + np.sign(df['dist_medium']) + np.sign(df['dist_long']))\n",
    "df['consensus_bucket'] = np.select(\n",
    "    [df['consensus_score'] >= 3, df['consensus_score'] >= 2],\n",
    "    ['full_consensus', 'partial_consensus'],\n",
    "    default='low_consensus'\n",
    ")\n",
    "\n",
    "trend_up = (df['price'] > df['sma_short']) & (df['sma_short'] > df['sma_medium']) & (df['sma_medium'] > df['sma_long'])\n",
    "trend_down = (df['price'] < df['sma_short']) & (df['sma_short'] < df['sma_medium']) & (df['sma_medium'] < df['sma_long'])\n",
    "pullback_up = (df['price'] < df['sma_short']) & (df['price'] > df['sma_medium']) & (df['sma_medium'] > df['sma_long'])\n",
    "pullback_down = (df['price'] > df['sma_short']) & (df['price'] < df['sma_medium']) & (df['sma_medium'] < df['sma_long'])\n",
    "\n",
    "df['setup'] = np.select(\n",
    "    [trend_up, trend_down, pullback_up, pullback_down],\n",
    "    ['trend_up', 'trend_down', 'pullback_up', 'pullback_down'],\n",
    "    default='mixed'\n",
    ")\n",
    "\n",
    "# Build forward 5D returns from price history\n",
    "price_parts = [\n",
    "    df[['date', 'symbol', 'price']].assign(source='signals', source_priority=1)\n",
    "]\n",
    "if df_positions is not None and {'date', 'symbol', 'price'}.issubset(df_positions.columns):\n",
    "    px = df_positions[['date', 'symbol', 'price']].copy()\n",
    "    px['date'] = pd.to_datetime(px['date'])\n",
    "    px['price'] = pd.to_numeric(px['price'], errors='coerce')\n",
    "    price_parts.append(px.assign(source='positions', source_priority=0))\n",
    "\n",
    "prices = pd.concat(price_parts, ignore_index=True)\n",
    "prices = prices.dropna(subset=['date', 'symbol', 'price'])\n",
    "prices = prices.sort_values(['symbol', 'date', 'source_priority'])\n",
    "prices = prices.drop_duplicates(['symbol', 'date'], keep='first')\n",
    "prices = prices.sort_values(['symbol', 'date'])\n",
    "prices['fwd_ret_5d'] = prices.groupby('symbol')['price'].shift(-5) / prices['price'] - 1.0\n",
    "\n",
    "df = df.merge(prices[['date', 'symbol', 'fwd_ret_5d']], on=['date', 'symbol'], how='left')\n",
    "df['signed_ret_5d'] = df['direction_sign'] * df['fwd_ret_5d']\n",
    "\n",
    "print(f\"signal rows: {len(df):,}\")\n",
    "print(f\"symbols: {df['symbol'].nunique()}\")\n",
    "print(f\"mean |magnitude gap|: {df['magnitude_gap'].abs().mean():.4f}\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "symbol_counts = (\n",
    "    df.groupby('symbol', as_index=False)\n",
    "      .agg(signals=('date', 'count'), net_direction=('direction_sign', 'mean'))\n",
    "      .sort_values('signals', ascending=False)\n",
    "      .head(12)\n",
    ")\n",
    "colors = np.where(symbol_counts['net_direction'] >= 0, '#2ca02c', '#d62728')\n",
    "axes[0, 0].bar(symbol_counts['symbol'], symbol_counts['signals'], color=colors)\n",
    "axes[0, 0].set_title('Most Traded Signal Symbols (green=net long, red=net short)')\n",
    "axes[0, 0].set_ylabel('Signal count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=35)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "component_strength = pd.DataFrame({\n",
    "    'component': ['short', 'medium', 'long'],\n",
    "    'mean_abs_contribution': [\n",
    "        df['contrib_short'].abs().mean(),\n",
    "        df['contrib_medium'].abs().mean(),\n",
    "        df['contrib_long'].abs().mean()\n",
    "    ]\n",
    "})\n",
    "sns.barplot(data=component_strength, x='component', y='mean_abs_contribution', ax=axes[0, 1], palette='Blues_d')\n",
    "axes[0, 1].set_title('Average Absolute Contribution To Composite Score')\n",
    "axes[0, 1].set_xlabel('Component')\n",
    "axes[0, 1].set_ylabel('Contribution units')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "setup_counts = (\n",
    "    df.groupby(['setup', 'direction'])['symbol']\n",
    "      .count()\n",
    "      .reset_index(name='signals')\n",
    ")\n",
    "order = ['trend_up', 'pullback_up', 'mixed', 'pullback_down', 'trend_down']\n",
    "sns.barplot(data=setup_counts, x='setup', y='signals', hue='direction', order=order, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Signal Setups By Direction')\n",
    "axes[1, 0].set_xlabel('Setup')\n",
    "axes[1, 0].set_ylabel('Signals')\n",
    "axes[1, 0].tick_params(axis='x', rotation=20)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.scatterplot(data=df, x='composite_score', y='magnitude', hue='tier', alpha=0.7, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Composite Score vs Logged Magnitude')\n",
    "axes[1, 1].set_xlabel('Composite score (weighted ATR distances)')\n",
    "axes[1, 1].set_ylabel('Logged magnitude')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_edge = (\n",
    "    df.groupby('setup', as_index=False)\n",
    "      .agg(\n",
    "          signals=('signed_ret_5d', 'count'),\n",
    "          hit_rate_5d=('signed_ret_5d', lambda s: (s > 0).mean()),\n",
    "          avg_signed_ret_5d=('signed_ret_5d', 'mean'),\n",
    "          median_signed_ret_5d=('signed_ret_5d', 'median'),\n",
    "          avg_abs_magnitude=('abs_magnitude', 'mean')\n",
    "      )\n",
    "      .sort_values('avg_signed_ret_5d', ascending=False)\n",
    ")\n",
    "setup_edge['hit_rate_5d'] = 100 * setup_edge['hit_rate_5d']\n",
    "\n",
    "consensus_edge = (\n",
    "    df.groupby('consensus_bucket', as_index=False)\n",
    "      .agg(\n",
    "          signals=('signed_ret_5d', 'count'),\n",
    "          hit_rate_5d=('signed_ret_5d', lambda s: (s > 0).mean()),\n",
    "          avg_signed_ret_5d=('signed_ret_5d', 'mean'),\n",
    "          avg_abs_magnitude=('abs_magnitude', 'mean')\n",
    "      )\n",
    "      .sort_values('avg_signed_ret_5d', ascending=False)\n",
    ")\n",
    "consensus_edge['hit_rate_5d'] = 100 * consensus_edge['hit_rate_5d']\n",
    "\n",
    "component_by_tier = (\n",
    "    df.groupby('tier', as_index=False)\n",
    "      .agg(\n",
    "          avg_contrib_short=('contrib_short', 'mean'),\n",
    "          avg_contrib_medium=('contrib_medium', 'mean'),\n",
    "          avg_contrib_long=('contrib_long', 'mean'),\n",
    "          avg_abs_magnitude=('abs_magnitude', 'mean')\n",
    "      )\n",
    ")\n",
    "\n",
    "print('Edge by setup (5D signed return)')\n",
    "display(setup_edge)\n",
    "\n",
    "print('Edge by horizon-consensus bucket')\n",
    "display(consensus_edge)\n",
    "\n",
    "print('Average component contributions by tier')\n",
    "display(component_by_tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_signals = len(df)\n",
    "setup_share = (\n",
    "    df['setup'].value_counts(normalize=True)\n",
    "      .rename_axis('setup')\n",
    "      .reset_index(name='share')\n",
    ")\n",
    "setup_share['share'] = 100 * setup_share['share']\n",
    "\n",
    "symbol_share = (\n",
    "    df['symbol'].value_counts(normalize=True)\n",
    "      .head(10)\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "best_setup = None\n",
    "if not df['signed_ret_5d'].dropna().empty:\n",
    "    ranked_setup = setup_edge[setup_edge['signals'] >= 10]\n",
    "    if not ranked_setup.empty:\n",
    "        best_setup = ranked_setup.iloc[0]\n",
    "\n",
    "answer_card = pd.DataFrame([\n",
    "    {\n",
    "        'question': 'What are we trading?',\n",
    "        'answer': 'A concentrated subset of symbols with persistent trend-aligned signals.',\n",
    "        'evidence': f\"Top 10 symbols represent {100 * symbol_share:.1f}% of all signals ({total_signals:,} total).\"\n",
    "    },\n",
    "    {\n",
    "        'question': 'Why these trades?',\n",
    "        'answer': 'Because price/SMA structure and weighted horizon contributions imply directional edge.',\n",
    "        'evidence': f\"Full trend setups share {setup_share.loc[setup_share['setup'].isin(['trend_up', 'trend_down']), 'share'].sum():.1f}% of signals.\"\n",
    "    },\n",
    "    {\n",
    "        'question': 'Where is edge strongest?',\n",
    "        'answer': 'Edge is strongest in specific setup/consensus buckets, not uniformly across all signals.',\n",
    "        'evidence': (\n",
    "            f\"Best eligible setup: {best_setup['setup']} with avg signed 5D return {100 * best_setup['avg_signed_ret_5d']:.2f}% \"\n",
    "            f\"and hit rate {best_setup['hit_rate_5d']:.1f}% (n={int(best_setup['signals'])}).\"\n",
    "            if best_setup is not None else\n",
    "            'Not enough 5D return coverage to rank setups yet.'\n",
    "        )\n",
    "    }\n",
    "])\n",
    "\n",
    "print('Signal Thesis Card')\n",
    "display(answer_card)\n",
    "\n",
    "print('Setup share (%)')\n",
    "display(setup_share)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}