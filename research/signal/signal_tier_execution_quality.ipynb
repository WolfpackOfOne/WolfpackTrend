{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Tier Execution Quality\n",
    "\n",
    "Evaluate execution outcomes by signal tier:\n",
    "- Fill counts/rates\n",
    "- Slippage (bps)\n",
    "- Signed next-day return after fills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "\n",
    "qb = QuantBook()\n",
    "print('QuantBook initialized')\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f'ObjectStore key not found: {key}')\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f'Empty ObjectStore key: {key}')\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f'Error reading {key}: {e}')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpuu0q1amqa",
   "source": "## Data Loading & Tier Attribution\n\nLoads order events, slippage records, position prices, and the signal log, then performs a multi-stage join to assign a signal tier (strong/moderate/weak/exit) to each filled order. The tier is sourced first from explicit order tags embedded at submission time, then inferred from same-day or most-recent prior signals matched by symbol and direction. Diagnostic tables at the end show how many rows were attributed from each source and what fraction of signed next-day returns are usable for quality measurement.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "4hoywcuq2zp",
   "source": "## Execution Quality — 3-Panel Overview by Tier\n\nThese three side-by-side plots summarize execution quality broken out by signal tier. The left panel shows fill counts, confirming how many orders were completed per tier; the center panel compares absolute slippage in basis points, testing whether high-conviction strong-tier orders incur more or less market impact than weak-tier limit orders; and the right panel shows signed next-day return, checking whether fills on strong signals land at prices that subsequently move in the intended direction.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "2ex37blqy2n",
   "source": "## Tier Metrics Summary Table\n\nThis table condenses execution quality into a single row per tier, reporting fill count, average and median slippage in basis points, clean signed-return observations, and hit rate. Use it to compare whether the extra immediacy cost of strong-tier limit-at-market orders is offset by better entry timing relative to the more patient moderate and weak tier limits. A higher hit rate and lower slippage for stronger tiers would validate the signal-strength execution design.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_events = read_csv_from_store('wolfpack/order_events.csv')\n",
    "df_slippage = read_csv_from_store('wolfpack/slippage.csv')\n",
    "df_positions = read_csv_from_store('wolfpack/positions.csv')\n",
    "df_signals = read_csv_from_store('wolfpack/signals.csv')\n",
    "\n",
    "if df_events is None or df_slippage is None:\n",
    "    raise ValueError('order_events.csv and slippage.csv are required.')\n",
    "\n",
    "df_events['date'] = pd.to_datetime(df_events['date'])\n",
    "df_slippage['date'] = pd.to_datetime(df_slippage['date'])\n",
    "\n",
    "for col in ['fill_quantity', 'fill_price']:\n",
    "    df_events[col] = pd.to_numeric(df_events[col], errors='coerce').fillna(0.0)\n",
    "for col in ['quantity', 'expected_price', 'fill_price', 'slippage_dollars']:\n",
    "    df_slippage[col] = pd.to_numeric(df_slippage[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "MAX_NEXT_GAP_DAYS = 7\n",
    "MAX_ABS_RETURN = 0.50\n",
    "\n",
    "def parse_tag(tag, key):\n",
    "    if pd.isna(tag):\n",
    "        return np.nan\n",
    "    m = re.search(rf'{key}=([^;]+)', str(tag))\n",
    "    return m.group(1) if m else np.nan\n",
    "\n",
    "def tier_from_magnitude(mag):\n",
    "    if pd.isna(mag):\n",
    "        return np.nan\n",
    "    x = abs(float(mag))\n",
    "    if x >= 0.7:\n",
    "        return 'strong'\n",
    "    if x >= 0.3:\n",
    "        return 'moderate'\n",
    "    return 'weak'\n",
    "\n",
    "# Build event-derived tier map from filled events\n",
    "fills = df_events[df_events['status'].astype(str).str.contains('Filled', case=False, na=False)].copy()\n",
    "fills = fills[fills['fill_quantity'].abs() > 0].copy()\n",
    "\n",
    "# Try tier from explicit tier=... tag; fallback to signal=... in tag when present\n",
    "fills['tag_tier'] = fills['tag'].apply(lambda t: parse_tag(t, 'tier'))\n",
    "fills['tag_signal'] = pd.to_numeric(fills['tag'].apply(lambda t: parse_tag(t, 'signal')), errors='coerce')\n",
    "fills['event_tier'] = fills['tag_tier']\n",
    "fills.loc[fills['event_tier'].isna(), 'event_tier'] = fills.loc[fills['event_tier'].isna(), 'tag_signal'].apply(tier_from_magnitude)\n",
    "fills['event_tier'] = fills['event_tier'].astype(str).str.lower().str.strip()\n",
    "fills.loc[~fills['event_tier'].isin(['strong', 'moderate', 'weak', 'exit']), 'event_tier'] = np.nan\n",
    "\n",
    "fills['fill_price_4'] = fills['fill_price'].round(4)\n",
    "fills['fill_quantity_4'] = fills['fill_quantity'].round(4)\n",
    "fills['fill_quantity_abs_4'] = fills['fill_quantity'].abs().round(4)\n",
    "\n",
    "strict_map = (\n",
    "    fills[['date', 'symbol', 'fill_price_4', 'fill_quantity_4', 'event_tier']]\n",
    "    .drop_duplicates(['date', 'symbol', 'fill_price_4', 'fill_quantity_4'])\n",
    ")\n",
    "loose_map = (\n",
    "    fills[['date', 'symbol', 'fill_price_4', 'fill_quantity_abs_4', 'event_tier']]\n",
    "    .drop_duplicates(['date', 'symbol', 'fill_price_4', 'fill_quantity_abs_4'])\n",
    ")\n",
    "\n",
    "# Start from slippage rows (one row per filled order)\n",
    "merged = df_slippage.copy()\n",
    "merged['fill_price_4'] = merged['fill_price'].round(4)\n",
    "merged['quantity_4'] = merged['quantity'].round(4)\n",
    "merged['quantity_abs_4'] = merged['quantity'].abs().round(4)\n",
    "merged['order_side'] = np.where(merged['quantity'] >= 0, 'Buy', 'Sell')\n",
    "\n",
    "# Strict join first (signed quantity)\n",
    "merged = merged.merge(\n",
    "    strict_map,\n",
    "    left_on=['date', 'symbol', 'fill_price_4', 'quantity_4'],\n",
    "    right_on=['date', 'symbol', 'fill_price_4', 'fill_quantity_4'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Loose join fallback (absolute quantity), used only for rows still missing tier\n",
    "missing = merged['event_tier'].isna()\n",
    "if missing.any():\n",
    "    fallback = (\n",
    "        merged.loc[missing, ['date', 'symbol', 'fill_price_4', 'quantity_abs_4']]\n",
    "        .assign(_row=lambda x: x.index)\n",
    "        .merge(\n",
    "            loose_map,\n",
    "            left_on=['date', 'symbol', 'fill_price_4', 'quantity_abs_4'],\n",
    "            right_on=['date', 'symbol', 'fill_price_4', 'fill_quantity_abs_4'],\n",
    "            how='left'\n",
    "        )\n",
    "        .sort_values('_row')\n",
    "    )\n",
    "    merged.loc[fallback['_row'].values, 'event_tier'] = fallback['event_tier'].values\n",
    "\n",
    "merged['tier'] = merged['event_tier']\n",
    "merged['tier_source'] = np.where(merged['tier'].notna(), 'event_tag', 'unmapped')\n",
    "merged['signal_lag_days'] = np.nan\n",
    "\n",
    "# If event tags are missing, infer from signal strength/direction\n",
    "if df_signals is not None and {'date', 'symbol', 'direction', 'magnitude'}.issubset(df_signals.columns):\n",
    "    sig = df_signals[['date', 'symbol', 'direction', 'magnitude']].copy()\n",
    "    sig['date'] = pd.to_datetime(sig['date'])\n",
    "    sig['magnitude'] = pd.to_numeric(sig['magnitude'], errors='coerce')\n",
    "    sig = sig.dropna(subset=['date', 'symbol', 'magnitude'])\n",
    "    sig['direction'] = sig['direction'].astype(str).str.title()\n",
    "    sig['signal_side'] = np.where(sig['direction'].eq('Up'), 'Buy', 'Sell')\n",
    "    sig['signal_tier'] = sig['magnitude'].apply(tier_from_magnitude)\n",
    "    sig['abs_magnitude'] = sig['magnitude'].abs()\n",
    "\n",
    "    # One signal row per date/symbol: keep strongest absolute magnitude\n",
    "    sig_daily = (\n",
    "        sig.sort_values(['symbol', 'date', 'abs_magnitude'], ascending=[True, True, False])\n",
    "           .drop_duplicates(['symbol', 'date'])[['date', 'symbol', 'signal_side', 'signal_tier']]\n",
    "    )\n",
    "\n",
    "    # Pass 1: same-day match\n",
    "    merged = merged.merge(sig_daily, on=['date', 'symbol'], how='left')\n",
    "\n",
    "    unmapped = merged['tier'].isna() & merged['signal_tier'].notna()\n",
    "    same_dir = unmapped & (merged['order_side'] == merged['signal_side'])\n",
    "    opposite_dir = unmapped & (merged['order_side'] != merged['signal_side'])\n",
    "\n",
    "    merged.loc[same_dir, 'tier'] = merged.loc[same_dir, 'signal_tier']\n",
    "    merged.loc[same_dir, 'tier_source'] = 'signal_same_day_match'\n",
    "    merged.loc[same_dir, 'signal_lag_days'] = 0\n",
    "\n",
    "    merged.loc[opposite_dir, 'tier'] = 'exit'\n",
    "    merged.loc[opposite_dir, 'tier_source'] = 'signal_same_day_opposite_exit'\n",
    "    merged.loc[opposite_dir, 'signal_lag_days'] = 0\n",
    "\n",
    "    # Pass 2: most recent prior signal for rows still unmapped\n",
    "    remaining = merged['tier'].isna()\n",
    "    if remaining.any() and not sig_daily.empty:\n",
    "        left_all = merged.loc[remaining, ['date', 'symbol', 'order_side']].copy()\n",
    "        left_all = left_all.dropna(subset=['date', 'symbol'])\n",
    "        left_all['_row'] = left_all.index\n",
    "\n",
    "        right_all = sig_daily.rename(columns={'date': 'signal_date'})\n",
    "        right_all = right_all.dropna(subset=['signal_date', 'symbol'])\n",
    "\n",
    "        asof_parts = []\n",
    "        for symbol, left_sym in left_all.groupby('symbol', sort=False):\n",
    "            right_sym = right_all[right_all['symbol'] == symbol]\n",
    "            if right_sym.empty:\n",
    "                continue\n",
    "\n",
    "            left_sym = left_sym.sort_values('date')\n",
    "            right_sym = right_sym[['signal_date', 'signal_side', 'signal_tier']].sort_values('signal_date')\n",
    "\n",
    "            part = pd.merge_asof(\n",
    "                left_sym,\n",
    "                right_sym,\n",
    "                left_on='date',\n",
    "                right_on='signal_date',\n",
    "                direction='backward'\n",
    "            )\n",
    "            asof_parts.append(part)\n",
    "\n",
    "        if asof_parts:\n",
    "            asof = pd.concat(asof_parts, ignore_index=True)\n",
    "            asof['lag_days'] = (asof['date'] - asof['signal_date']).dt.days\n",
    "\n",
    "            # Guardrail: only use reasonably recent signal context\n",
    "            recent = asof['signal_tier'].notna() & asof['lag_days'].notna() & (asof['lag_days'] <= 10)\n",
    "            same_dir = recent & (asof['order_side'] == asof['signal_side'])\n",
    "            opposite_dir = recent & (asof['order_side'] != asof['signal_side'])\n",
    "\n",
    "            idx_same = asof.loc[same_dir, '_row']\n",
    "            idx_opp = asof.loc[opposite_dir, '_row']\n",
    "\n",
    "            merged.loc[idx_same, 'tier'] = asof.loc[same_dir, 'signal_tier'].values\n",
    "            merged.loc[idx_same, 'tier_source'] = 'signal_prior_match'\n",
    "            merged.loc[idx_same, 'signal_lag_days'] = asof.loc[same_dir, 'lag_days'].values\n",
    "\n",
    "            merged.loc[idx_opp, 'tier'] = 'exit'\n",
    "            merged.loc[idx_opp, 'tier_source'] = 'signal_prior_opposite_exit'\n",
    "            merged.loc[idx_opp, 'signal_lag_days'] = asof.loc[opposite_dir, 'lag_days'].values\n",
    "\n",
    "merged['tier'] = merged['tier'].fillna('unknown')\n",
    "merged['notional'] = (merged['fill_price'].abs() * merged['quantity'].abs()).replace(0, np.nan)\n",
    "merged['slippage_bps'] = (merged['slippage_dollars'].abs() / merged['notional']) * 10000\n",
    "\n",
    "if df_positions is not None:\n",
    "    df_positions['date'] = pd.to_datetime(df_positions['date'])\n",
    "    df_positions['price'] = pd.to_numeric(df_positions['price'], errors='coerce')\n",
    "\n",
    "    px = (\n",
    "        df_positions[['date', 'symbol', 'price']]\n",
    "        .dropna(subset=['date', 'symbol', 'price'])\n",
    "        .query('price > 0')\n",
    "        .drop_duplicates(['date', 'symbol'])\n",
    "        .sort_values(['symbol', 'date'])\n",
    "    )\n",
    "\n",
    "    px['next_date'] = px.groupby('symbol')['date'].shift(-1)\n",
    "    px['next_price'] = px.groupby('symbol')['price'].shift(-1)\n",
    "\n",
    "    fills_rt = merged.merge(px[['date', 'symbol', 'price', 'next_date', 'next_price']], on=['date', 'symbol'], how='left')\n",
    "    fills_rt['next_gap_days'] = (fills_rt['next_date'] - fills_rt['date']).dt.days\n",
    "    fills_rt['side_sign'] = np.where(fills_rt['quantity'] >= 0, 1.0, -1.0)\n",
    "\n",
    "    valid_base = (\n",
    "        (fills_rt['fill_price'] > 0) &\n",
    "        (fills_rt['next_price'] > 0) &\n",
    "        fills_rt['next_gap_days'].notna() &\n",
    "        (fills_rt['next_gap_days'] >= 1) &\n",
    "        (fills_rt['next_gap_days'] <= MAX_NEXT_GAP_DAYS)\n",
    "    )\n",
    "\n",
    "    fills_rt['raw_next_day_return'] = np.nan\n",
    "    fills_rt.loc[valid_base, 'raw_next_day_return'] = (\n",
    "        fills_rt.loc[valid_base, 'next_price'] / fills_rt.loc[valid_base, 'fill_price']\n",
    "    ) - 1.0\n",
    "\n",
    "    fills_rt['signed_next_day_return'] = fills_rt['raw_next_day_return'] * fills_rt['side_sign']\n",
    "    fills_rt['signed_next_day_return_clean'] = fills_rt['signed_next_day_return']\n",
    "\n",
    "    extreme_move = fills_rt['signed_next_day_return_clean'].abs() > MAX_ABS_RETURN\n",
    "    fills_rt.loc[extreme_move, 'signed_next_day_return_clean'] = np.nan\n",
    "\n",
    "    fills_rt['return_quality'] = np.select(\n",
    "        [\n",
    "            fills_rt['next_price'].isna(),\n",
    "            fills_rt['fill_price'] <= 0,\n",
    "            fills_rt['next_price'] <= 0,\n",
    "            fills_rt['next_gap_days'].isna(),\n",
    "            fills_rt['next_gap_days'] < 1,\n",
    "            fills_rt['next_gap_days'] > MAX_NEXT_GAP_DAYS,\n",
    "            extreme_move,\n",
    "        ],\n",
    "        [\n",
    "            'missing_next_price',\n",
    "            'non_positive_fill_price',\n",
    "            'non_positive_next_price',\n",
    "            'missing_next_gap',\n",
    "            'non_forward_next_price',\n",
    "            'next_gap_too_large',\n",
    "            f'abs_return_gt_{int(MAX_ABS_RETURN * 100)}pct',\n",
    "        ],\n",
    "        default='ok'\n",
    "    )\n",
    "else:\n",
    "    fills_rt = merged.copy()\n",
    "    fills_rt['raw_next_day_return'] = np.nan\n",
    "    fills_rt['signed_next_day_return'] = np.nan\n",
    "    fills_rt['signed_next_day_return_clean'] = np.nan\n",
    "    fills_rt['next_gap_days'] = np.nan\n",
    "    fills_rt['return_quality'] = 'no_positions_prices'\n",
    "\n",
    "source_breakdown = fills_rt['tier_source'].value_counts(dropna=False).rename_axis('tier_source').reset_index(name='rows')\n",
    "print('Tier source breakdown:')\n",
    "display(source_breakdown)\n",
    "\n",
    "if fills_rt['signal_lag_days'].notna().any():\n",
    "    lag_summary = fills_rt['signal_lag_days'].dropna().describe(percentiles=[0.5, 0.9, 0.99]).to_frame('lag_days')\n",
    "    print('Signal lag (days) for inferred tiers:')\n",
    "    display(lag_summary)\n",
    "\n",
    "quality_breakdown = fills_rt['return_quality'].value_counts(dropna=False).rename_axis('return_quality').reset_index(name='rows')\n",
    "print('Return quality breakdown:')\n",
    "display(quality_breakdown)\n",
    "\n",
    "raw_cov = fills_rt['signed_next_day_return'].notna().mean()\n",
    "clean_cov = fills_rt['signed_next_day_return_clean'].notna().mean()\n",
    "print(f'Usable signed return coverage (raw): {raw_cov:.1%}')\n",
    "print(f'Usable signed return coverage (clean): {clean_cov:.1%}')\n",
    "\n",
    "print(f\"Unknown rows: {int((fills_rt['tier'] == 'unknown').sum()):,} / {len(fills_rt):,}\")\n",
    "display(fills_rt.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78mj72w5wpi",
   "source": "## Bootstrap Confidence Intervals on Signed Next-Day Returns\n\nThis section quantifies uncertainty around tier-level return estimates using 1,000-iteration bootstrapped confidence intervals on both mean and median signed next-day returns. The resulting 95% intervals are plotted as error bars, one panel per statistic. Tiers whose intervals do not cross zero provide the strongest evidence that the observed edge is not due to sampling noise in a relatively small fill dataset.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "w2xawp3t05",
   "source": "## Tagged vs Inferred Tier — Source Split Panels\n\nThis section splits fills into those whose tier came directly from order tags versus those inferred from recent signal history, then compares signed next-day returns for each group side by side. A summary table first shows fill counts and average returns by source group and tier; the two boxplots then make it visually clear whether inferred-tier rows behave similarly to directly tagged ones. Large discrepancies would indicate the inference logic is misclassifying orders and distorting the measured edge by tier.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "a3bhe5fn87i",
   "source": "## Multi-Horizon Signed Returns (1D / 3D / 5D)\n\nThis section extends the analysis from next-day returns to 3- and 5-day holding horizons for each tier, using forward prices from position data cleaned for large date gaps. A summary table reports observations, mean, median, and hit rate for each tier-horizon pair; the line chart and boxplot then show whether any tier's directional edge persists or decays as the holding period lengthens. Sustained or growing edge at longer horizons would support the strategy's 5-day scaling schedule.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "tier_counts = fills_rt['tier'].value_counts().rename_axis('tier').reset_index(name='fills')\n",
    "sns.barplot(data=tier_counts, x='tier', y='fills', order=['strong', 'moderate', 'weak', 'exit', 'unknown'], ax=axes[0])\n",
    "axes[0].set_title('Filled Orders by Tier')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=fills_rt, x='tier', y='slippage_bps', order=['strong', 'moderate', 'weak', 'exit', 'unknown'], ax=axes[1])\n",
    "axes[1].set_title('Absolute Slippage (bps) by Tier')\n",
    "axes[1].set_xlabel('Tier')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "sns.boxplot(data=fills_rt, x='tier', y='signed_next_day_return_clean', order=['strong', 'moderate', 'weak', 'exit', 'unknown'], ax=axes[2])\n",
    "axes[2].set_title('Signed Next-Day Return by Tier (Cleaned)')\n",
    "axes[2].set_xlabel('Tier')\n",
    "axes[2].set_ylabel('Return')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_metrics = (\n",
    "    fills_rt.groupby('tier', as_index=False)\n",
    "            .agg(\n",
    "                fills=('symbol', 'count'),\n",
    "                avg_slippage_bps=('slippage_bps', 'mean'),\n",
    "                med_slippage_bps=('slippage_bps', 'median'),\n",
    "                clean_return_obs=('signed_next_day_return_clean', 'count'),\n",
    "                avg_signed_next_day_return_raw=('signed_next_day_return', 'mean'),\n",
    "                avg_signed_next_day_return_clean=('signed_next_day_return_clean', 'mean'),\n",
    "                hit_rate_clean=('signed_next_day_return_clean', lambda s: (s > 0).mean())\n",
    "            )\n",
    "            .sort_values('fills', ascending=False)\n",
    ")\n",
    "tier_metrics['hit_rate_clean'] = 100 * tier_metrics['hit_rate_clean']\n",
    "display(tier_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Diagnostics\n",
    "\n",
    "These sections add three deeper checks:\n",
    "\n",
    "1. **Bootstrap confidence intervals**: Estimates uncertainty for tier-level mean/median signed next-day returns, so we can distinguish noise from reliable differences.\n",
    "2. **Tier-source split panels**: Separates performance by source of tier attribution (`event_tag` vs inferred) to show whether inferred mapping behaves differently.\n",
    "3. **Multi-horizon returns (1D/3D/5D)**: Compares signed forward returns across holding horizons to test whether edge appears with more time than next-day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals for tier-level 1D cleaned returns\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def bootstrap_ci(values, func=np.mean, n_boot=1000, ci=95):\n",
    "    arr = pd.Series(values).dropna().to_numpy()\n",
    "    n = len(arr)\n",
    "    if n == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    stat = float(func(arr))\n",
    "    if n == 1:\n",
    "        return stat, stat, stat\n",
    "\n",
    "    samples = rng.choice(arr, size=(n_boot, n), replace=True)\n",
    "    boot_stats = np.apply_along_axis(func, 1, samples)\n",
    "    alpha = (100 - ci) / 2.0\n",
    "    lo = float(np.percentile(boot_stats, alpha))\n",
    "    hi = float(np.percentile(boot_stats, 100 - alpha))\n",
    "    return stat, lo, hi\n",
    "\n",
    "rows = []\n",
    "for tier in ['strong', 'moderate', 'weak', 'exit', 'unknown']:\n",
    "    s = fills_rt.loc[fills_rt['tier'] == tier, 'signed_next_day_return_clean']\n",
    "    if s.notna().sum() == 0:\n",
    "        continue\n",
    "\n",
    "    mean_val, mean_lo, mean_hi = bootstrap_ci(s, func=np.mean)\n",
    "    med_val, med_lo, med_hi = bootstrap_ci(s, func=np.median)\n",
    "\n",
    "    rows.append({\n",
    "        'tier': tier,\n",
    "        'n_clean': int(s.notna().sum()),\n",
    "        'mean': mean_val,\n",
    "        'mean_ci_low': mean_lo,\n",
    "        'mean_ci_high': mean_hi,\n",
    "        'median': med_val,\n",
    "        'median_ci_low': med_lo,\n",
    "        'median_ci_high': med_hi,\n",
    "    })\n",
    "\n",
    "ci_df = pd.DataFrame(rows)\n",
    "display(ci_df)\n",
    "\n",
    "if not ci_df.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    axes[0].errorbar(\n",
    "        ci_df['tier'],\n",
    "        ci_df['mean'],\n",
    "        yerr=[ci_df['mean'] - ci_df['mean_ci_low'], ci_df['mean_ci_high'] - ci_df['mean']],\n",
    "        fmt='o',\n",
    "        capsize=4\n",
    "    )\n",
    "    axes[0].axhline(0, color='black', linewidth=1)\n",
    "    axes[0].set_title('Mean Signed 1D Return with 95% Bootstrap CI')\n",
    "    axes[0].set_ylabel('Return')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1].errorbar(\n",
    "        ci_df['tier'],\n",
    "        ci_df['median'],\n",
    "        yerr=[ci_df['median'] - ci_df['median_ci_low'], ci_df['median_ci_high'] - ci_df['median']],\n",
    "        fmt='o',\n",
    "        capsize=4,\n",
    "        color='#ff7f0e'\n",
    "    )\n",
    "    axes[1].axhline(0, color='black', linewidth=1)\n",
    "    axes[1].set_title('Median Signed 1D Return with 95% Bootstrap CI')\n",
    "    axes[1].set_ylabel('Return')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate panels for tagged vs inferred tier sources\n",
    "fills_rt['source_group'] = np.where(fills_rt['tier_source'].eq('event_tag'), 'tagged', 'inferred')\n",
    "order = ['strong', 'moderate', 'weak', 'exit', 'unknown']\n",
    "\n",
    "source_summary = (\n",
    "    fills_rt.groupby(['source_group', 'tier'], as_index=False)\n",
    "            .agg(\n",
    "                fills=('symbol', 'count'),\n",
    "                clean_return_obs=('signed_next_day_return_clean', 'count'),\n",
    "                avg_signed_return_clean=('signed_next_day_return_clean', 'mean'),\n",
    "                avg_slippage_bps=('slippage_bps', 'mean')\n",
    "            )\n",
    "            .sort_values(['source_group', 'fills'], ascending=[True, False])\n",
    ")\n",
    "display(source_summary)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5), sharey=True)\n",
    "for ax, group in zip(axes, ['tagged', 'inferred']):\n",
    "    dfg = fills_rt[fills_rt['source_group'] == group]\n",
    "    if dfg.empty:\n",
    "        ax.text(0.5, 0.5, f'No {group} rows', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'Signed Next-Day Return ({group})')\n",
    "        ax.set_xlabel('Tier')\n",
    "        ax.grid(alpha=0.3)\n",
    "        continue\n",
    "\n",
    "    sns.boxplot(data=dfg, x='tier', y='signed_next_day_return_clean', order=order, ax=ax)\n",
    "    ax.axhline(0, color='black', linewidth=1)\n",
    "    ax.set_title(f'Signed Next-Day Return ({group})')\n",
    "    ax.set_xlabel('Tier')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel('Return')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-horizon signed forward returns (1D, 3D, 5D)\n",
    "if df_positions is None:\n",
    "    raise ValueError('positions.csv is required for 3D/5D return analysis.')\n",
    "\n",
    "horizons = [1, 3, 5]\n",
    "max_gap_days = {1: 7, 3: 14, 5: 21}\n",
    "max_abs_return_by_h = {1: 0.50, 3: 0.75, 5: 1.00}\n",
    "\n",
    "px_h = (\n",
    "    df_positions[['date', 'symbol', 'price']]\n",
    "    .copy()\n",
    ")\n",
    "px_h['date'] = pd.to_datetime(px_h['date'])\n",
    "px_h['price'] = pd.to_numeric(px_h['price'], errors='coerce')\n",
    "px_h = (\n",
    "    px_h.dropna(subset=['date', 'symbol', 'price'])\n",
    "        .query('price > 0')\n",
    "        .drop_duplicates(['date', 'symbol'])\n",
    "        .sort_values(['symbol', 'date'])\n",
    ")\n",
    "\n",
    "for h in horizons:\n",
    "    px_h[f'fwd_date_{h}d'] = px_h.groupby('symbol')['date'].shift(-h)\n",
    "    px_h[f'fwd_price_{h}d'] = px_h.groupby('symbol')['price'].shift(-h)\n",
    "    px_h[f'raw_ret_{h}d'] = px_h[f'fwd_price_{h}d'] / px_h['price'] - 1.0\n",
    "    px_h[f'gap_days_{h}d'] = (px_h[f'fwd_date_{h}d'] - px_h['date']).dt.days\n",
    "\n",
    "base = fills_rt[['date', 'symbol', 'tier', 'tier_source', 'quantity']].copy()\n",
    "base['side_sign'] = np.where(base['quantity'] >= 0, 1.0, -1.0)\n",
    "\n",
    "merge_cols = ['date', 'symbol']\n",
    "for h in horizons:\n",
    "    merge_cols += [f'raw_ret_{h}d', f'gap_days_{h}d']\n",
    "\n",
    "hr = base.merge(px_h[merge_cols], on=['date', 'symbol'], how='left')\n",
    "\n",
    "for h in horizons:\n",
    "    hr[f'signed_ret_{h}d_raw'] = hr[f'raw_ret_{h}d'] * hr['side_sign']\n",
    "\n",
    "    valid = (\n",
    "        hr[f'gap_days_{h}d'].notna() &\n",
    "        (hr[f'gap_days_{h}d'] >= h) &\n",
    "        (hr[f'gap_days_{h}d'] <= max_gap_days[h])\n",
    "    )\n",
    "\n",
    "    hr[f'signed_ret_{h}d_clean'] = hr[f'signed_ret_{h}d_raw']\n",
    "    hr.loc[~valid, f'signed_ret_{h}d_clean'] = np.nan\n",
    "    hr.loc[hr[f'signed_ret_{h}d_clean'].abs() > max_abs_return_by_h[h], f'signed_ret_{h}d_clean'] = np.nan\n",
    "\n",
    "# Build long-format dataframe\n",
    "long_rows = []\n",
    "for h in horizons:\n",
    "    tmp = hr[['tier', 'tier_source', f'signed_ret_{h}d_clean']].copy()\n",
    "    tmp = tmp.rename(columns={f'signed_ret_{h}d_clean': 'signed_return'})\n",
    "    tmp['horizon_days'] = h\n",
    "    long_rows.append(tmp)\n",
    "\n",
    "hret = pd.concat(long_rows, ignore_index=True)\n",
    "\n",
    "horizon_summary = (\n",
    "    hret.groupby(['tier', 'horizon_days'], as_index=False)\n",
    "        .agg(\n",
    "            obs=('signed_return', 'count'),\n",
    "            mean_signed_return=('signed_return', 'mean'),\n",
    "            median_signed_return=('signed_return', 'median'),\n",
    "            hit_rate=('signed_return', lambda s: (s > 0).mean())\n",
    "        )\n",
    ")\n",
    "horizon_summary['hit_rate'] = 100 * horizon_summary['hit_rate']\n",
    "display(horizon_summary.sort_values(['horizon_days', 'tier']))\n",
    "\n",
    "# Plot mean by horizon\n",
    "order = ['strong', 'moderate', 'weak', 'exit', 'unknown']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "for tier in order:\n",
    "    line = horizon_summary[horizon_summary['tier'] == tier].sort_values('horizon_days')\n",
    "    if line.empty:\n",
    "        continue\n",
    "    axes[0].plot(line['horizon_days'], line['mean_signed_return'], marker='o', label=tier)\n",
    "\n",
    "axes[0].axhline(0, color='black', linewidth=1)\n",
    "axes[0].set_title('Mean Signed Return by Horizon')\n",
    "axes[0].set_xlabel('Horizon (days)')\n",
    "axes[0].set_ylabel('Mean signed return')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Boxplot by horizon with tier hue\n",
    "plot_df = hret[hret['signed_return'].notna()].copy()\n",
    "sns.boxplot(data=plot_df, x='horizon_days', y='signed_return', hue='tier', ax=axes[1])\n",
    "axes[1].axhline(0, color='black', linewidth=1)\n",
    "axes[1].set_title('Signed Return Distribution by Horizon and Tier')\n",
    "axes[1].set_xlabel('Horizon (days)')\n",
    "axes[1].set_ylabel('Signed return')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}