{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P&L Attribution\n",
    "\n",
    "Attribution analysis for the WolfpackTrend strategy using position weights and daily returns.\n",
    "\n",
    "**Data Sources:**\n",
    "- `wolfpack/positions.csv` - Daily positions with weights\n",
    "- `wolfpack/daily_snapshots.csv` - Daily NAV and P&L\n",
    "- `wolfpack/slippage.csv` - Per-order slippage\n",
    "\n",
    "**Analysis:**\n",
    "- Per-symbol P&L contribution based on weights\n",
    "- Daily slippage cost overlay\n",
    "- Sector/symbol contribution breakdown\n",
    "\n",
    "**Prerequisites:** Run the WolfpackTrend backtest first to generate ObjectStore data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "from QuantConnect import *\n",
    "from QuantConnect.Research import QuantBook\n",
    "\n",
    "qb = QuantBook()\n",
    "print(\"QuantBook initialized\")\n",
    "\n",
    "\n",
    "def read_csv_from_store(key):\n",
    "    \"\"\"Read CSV from ObjectStore with error handling.\"\"\"\n",
    "    try:\n",
    "        if not qb.ObjectStore.ContainsKey(key):\n",
    "            print(f\"ObjectStore key not found: {key}\")\n",
    "            return None\n",
    "        content = qb.ObjectStore.Read(key)\n",
    "        if not content:\n",
    "            print(f\"Empty ObjectStore key: {key}\")\n",
    "            return None\n",
    "        return pd.read_csv(StringIO(content))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {key}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_trades(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize symbol column\n",
    "    if 'symbol' not in df.columns:\n",
    "        if 'Symbols' in df.columns:\n",
    "            df['symbol'] = df['Symbols']\n",
    "        elif 'ticker' in df.columns:\n",
    "            df['symbol'] = df['ticker']\n",
    "\n",
    "    # Normalize date\n",
    "    if 'date' not in df.columns:\n",
    "        if 'Exit Time' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['Exit Time']).dt.date\n",
    "        elif 'exit_time' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['exit_time']).dt.date\n",
    "        elif 'Entry Time' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['Entry Time']).dt.date\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Normalize realized P&L\n",
    "    if 'realized_pnl' not in df.columns:\n",
    "        if 'P&L' in df.columns:\n",
    "            df['realized_pnl'] = df['P&L']\n",
    "        elif 'pnl' in df.columns:\n",
    "            df['realized_pnl'] = df['pnl']\n",
    "\n",
    "    # Normalize fees\n",
    "    if 'fees' not in df.columns:\n",
    "        df['fees'] = df['Fees'] if 'Fees' in df.columns else 0.0\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positions = read_csv_from_store(\"wolfpack/positions.csv\")\n",
    "df_snapshots = read_csv_from_store(\"wolfpack/daily_snapshots.csv\")\n",
    "df_slippage = read_csv_from_store(\"wolfpack/slippage.csv\")\n",
    "df_signals = read_csv_from_store(\"wolfpack/signals.csv\")\n",
    "df_trades = read_csv_from_store(\"wolfpack/trades.csv\")\n",
    "\n",
    "# Parse dates\n",
    "if df_positions is not None:\n",
    "    df_positions['date'] = pd.to_datetime(df_positions['date'])\n",
    "    print(f\"Positions: {len(df_positions)} records\")\n",
    "    print(f\"  Columns: {list(df_positions.columns)}\")\n",
    "    if 'daily_dividends' in df_positions.columns:\n",
    "        df_positions['daily_dividends'] = pd.to_numeric(df_positions['daily_dividends'], errors='coerce')\n",
    "else:\n",
    "    print(\"WARNING: positions.csv not found - P&L attribution will be limited\")\n",
    "\n",
    "if df_snapshots is not None:\n",
    "    df_snapshots['date'] = pd.to_datetime(df_snapshots['date'])\n",
    "    print(f\"Daily snapshots: {len(df_snapshots)} records\")\n",
    "\n",
    "if df_slippage is not None:\n",
    "    df_slippage['date'] = pd.to_datetime(df_slippage['date'])\n",
    "    print(f\"Slippage: {len(df_slippage)} records\")\n",
    "else:\n",
    "    print(\"Note: slippage.csv not found - slippage overlay will be skipped\")\n",
    "\n",
    "if df_signals is not None:\n",
    "    df_signals['date'] = pd.to_datetime(df_signals['date'])\n",
    "    print(f\"Signals: {len(df_signals)} records\")\n",
    "    print(f\"  Columns: {list(df_signals.columns)}\")\n",
    "else:\n",
    "    print(\"Note: signals.csv not found - horizon attribution will be skipped\")\n",
    "\n",
    "# Fallback to local trades export if ObjectStore is missing\n",
    "if df_trades is None:\n",
    "    local_trades_path = Path(\"../Ugly Apricot Buffalo_trades.csv\")\n",
    "    if local_trades_path.exists():\n",
    "        df_trades = pd.read_csv(local_trades_path)\n",
    "        print(f\"Loaded local trades: {local_trades_path}\")\n",
    "    else:\n",
    "        print(\"Note: trades.csv not found - realized P&L will not be included\")\n",
    "\n",
    "df_trades = normalize_trades(df_trades)\n",
    "if df_trades is not None:\n",
    "    print(f\"Trades: {len(df_trades)} records (realized P&L from closed positions)\")\n",
    "\n",
    "# Initialize shared vars to avoid NameError in later cells\n",
    "symbol_col = None\n",
    "weight_col = None\n",
    "pnl_col = None\n",
    "daily_position_pnl = None\n",
    "attribution_label = None\n",
    "merged = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio P&L Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_snapshots is not None:\n",
    "    # Calculate daily P&L if not present\n",
    "    if 'daily_pnl' not in df_snapshots.columns:\n",
    "        df_snapshots['daily_pnl'] = df_snapshots['nav'].diff()\n",
    "    \n",
    "    if 'cumulative_pnl' not in df_snapshots.columns:\n",
    "        df_snapshots['cumulative_pnl'] = df_snapshots['daily_pnl'].cumsum()\n",
    "    \n",
    "    print(\"\\nPortfolio P&L Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total P&L: ${df_snapshots['daily_pnl'].sum():,.2f}\")\n",
    "    print(f\"Best day: ${df_snapshots['daily_pnl'].max():,.2f}\")\n",
    "    print(f\"Worst day: ${df_snapshots['daily_pnl'].min():,.2f}\")\n",
    "    print(f\"Average daily P&L: ${df_snapshots['daily_pnl'].mean():,.2f}\")\n",
    "    \n",
    "    # Plot daily P&L\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    axes[0].bar(df_snapshots['date'], df_snapshots['daily_pnl'], \n",
    "                color=np.where(df_snapshots['daily_pnl'] >= 0, 'green', 'red'), alpha=0.7)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[0].set_title('Daily P&L', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('P&L ($)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(df_snapshots['date'], df_snapshots['cumulative_pnl'], linewidth=2, color='steelblue')\n",
    "    axes[1].fill_between(df_snapshots['date'], 0, df_snapshots['cumulative_pnl'], \n",
    "                         where=df_snapshots['cumulative_pnl'] >= 0, alpha=0.3, color='green')\n",
    "    axes[1].fill_between(df_snapshots['date'], 0, df_snapshots['cumulative_pnl'], \n",
    "                         where=df_snapshots['cumulative_pnl'] < 0, alpha=0.3, color='red')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[1].set_title('Cumulative P&L', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Cumulative P&L ($)')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-Based P&L Attribution\n",
    "\n",
    "Compute per-symbol P&L contribution using position weights and price returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is not None and df_snapshots is not None:\n",
    "    # Identify columns\n",
    "    symbol_col = None\n",
    "    for col in ['symbol', 'ticker', 'underlying']:\n",
    "        if col in df_positions.columns:\n",
    "            symbol_col = col\n",
    "            break\n",
    "\n",
    "    weight_col = None\n",
    "    for col in ['weight', 'portfolio_weight']:\n",
    "        if col in df_positions.columns:\n",
    "            weight_col = col\n",
    "            break\n",
    "\n",
    "    print(f\"Symbol column: {symbol_col}\")\n",
    "    print(f\"Weight column: {weight_col}\")\n",
    "\n",
    "    symbol_pnl = pd.Series(dtype=float)\n",
    "    pnl_col = None\n",
    "    daily_position_pnl = None\n",
    "    attribution_label = None\n",
    "\n",
    "    USE_RETURNS_ATTRIBUTION = False\n",
    "    fees_already_included = False\n",
    "\n",
    "    # Prefer daily total net P&L if available (realized + unrealized - fees)\n",
    "    if symbol_col and (not USE_RETURNS_ATTRIBUTION) and ('daily_total_net_pnl' in df_positions.columns):\n",
    "        pnl_col = 'daily_total_net_pnl'\n",
    "        attribution_label = 'positions daily_total_net_pnl (realized + unrealized - fees)'\n",
    "        fees_already_included = True\n",
    "        print(f\"P&L attribution method: {attribution_label}\")\n",
    "\n",
    "        symbol_pnl = df_positions.groupby(symbol_col)[pnl_col].sum().sort_values(ascending=False)\n",
    "        print()\n",
    "        print(\"Total P&L by Symbol (attributed):\")\n",
    "        print(\"=\" * 60)\n",
    "        display(symbol_pnl.head(20).to_frame('Total P&L'))\n",
    "\n",
    "        daily_position_pnl = df_positions.groupby('date')[pnl_col].sum().reset_index()\n",
    "        daily_position_pnl.columns = ['date', 'position_pnl']\n",
    "        print()\n",
    "        print(f\"Total attributed from held positions: ${daily_position_pnl['position_pnl'].sum():,.2f}\")\n",
    "\n",
    "        if df_trades is not None and 'realized_pnl' in df_trades.columns and 'symbol' in df_trades.columns:\n",
    "            realized_by_symbol = df_trades.groupby('symbol')['realized_pnl'].sum()\n",
    "            print()\n",
    "            print(f\"Total realized P&L from trades file: ${realized_by_symbol.sum():,.2f}\")\n",
    "    # Prefer daily MTM P&L if available (aligns with EOD holdings)\n",
    "    elif symbol_col and (not USE_RETURNS_ATTRIBUTION) and ('daily_pnl' in df_positions.columns):\n",
    "        pnl_col = 'daily_pnl'\n",
    "        attribution_label = 'positions daily_pnl (unrealized delta)'\n",
    "        print(f\"P&L attribution method: {attribution_label}\")\n",
    "\n",
    "        symbol_pnl = df_positions.groupby(symbol_col)[pnl_col].sum().sort_values(ascending=False)\n",
    "        print()\n",
    "        print(\"Total P&L by Symbol (attributed):\")\n",
    "        print(\"=\" * 60)\n",
    "        display(symbol_pnl.head(20).to_frame('Total P&L'))\n",
    "\n",
    "        daily_position_pnl = df_positions.groupby('date')[pnl_col].sum().reset_index()\n",
    "        daily_position_pnl.columns = ['date', 'position_pnl']\n",
    "        print()\n",
    "        print(f\"Total attributed from held positions: ${daily_position_pnl['position_pnl'].sum():,.2f}\")\n",
    "\n",
    "        if df_trades is not None and 'realized_pnl' in df_trades.columns and 'symbol' in df_trades.columns:\n",
    "            realized_by_symbol = df_trades.groupby('symbol')['realized_pnl'].sum()\n",
    "            print()\n",
    "            print(f\"Total realized P&L from trades file: ${realized_by_symbol.sum():,.2f}\")\n",
    "    else:\n",
    "        # Ensure price column for returns\n",
    "        if 'price' not in df_positions.columns:\n",
    "            if 'market_value' in df_positions.columns and 'quantity' in df_positions.columns:\n",
    "                df_positions['price'] = df_positions['market_value'] / df_positions['quantity']\n",
    "                print(\"Derived price from market_value / quantity\")\n",
    "            else:\n",
    "                print(\"WARNING: No price or market_value/quantity columns; cannot compute returns\")\n",
    "\n",
    "        # Ensure market_value column\n",
    "        if 'market_value' not in df_positions.columns:\n",
    "            if weight_col and 'nav' in df_snapshots.columns:\n",
    "                df_positions = df_positions.merge(df_snapshots[['date', 'nav']], on='date', how='left')\n",
    "                df_positions['market_value'] = df_positions[weight_col] * df_positions['nav']\n",
    "                print(\"Derived market_value from weight * nav\")\n",
    "            elif 'price' in df_positions.columns and 'quantity' in df_positions.columns:\n",
    "                df_positions['market_value'] = df_positions['price'] * df_positions['quantity']\n",
    "                print(\"Derived market_value from price * quantity\")\n",
    "\n",
    "        # Primary attribution: weights + daily returns\n",
    "        if symbol_col and 'price' in df_positions.columns:\n",
    "            df_positions = df_positions.sort_values(['date', symbol_col])\n",
    "            df_positions['prev_price'] = df_positions.groupby(symbol_col)['price'].shift(1)\n",
    "            df_positions['return'] = df_positions['price'] / df_positions['prev_price'] - 1\n",
    "\n",
    "            pnl_method = None\n",
    "            if weight_col and 'nav' in df_snapshots.columns:\n",
    "                nav_by_date = df_snapshots[['date', 'nav']].sort_values('date').copy()\n",
    "                nav_by_date['prev_nav'] = nav_by_date['nav'].shift(1)\n",
    "                df_positions = df_positions.merge(nav_by_date[['date', 'prev_nav']], on='date', how='left')\n",
    "                df_positions['prev_weight'] = df_positions.groupby(symbol_col)[weight_col].shift(1)\n",
    "                df_positions['position_pnl'] = df_positions['prev_weight'] * df_positions['prev_nav'] * df_positions['return']\n",
    "                pnl_method = \"weights * returns (prev weight * prev NAV)\"\n",
    "            elif 'market_value' in df_positions.columns:\n",
    "                df_positions['prev_market_value'] = df_positions.groupby(symbol_col)['market_value'].shift(1)\n",
    "                df_positions['position_pnl'] = df_positions['prev_market_value'] * df_positions['return']\n",
    "                pnl_method = \"returns * prior market value\"\n",
    "\n",
    "            if pnl_method:\n",
    "                df_positions['position_pnl'] = df_positions['position_pnl'].fillna(0)\n",
    "                pnl_col = 'position_pnl'\n",
    "                attribution_label = pnl_method\n",
    "                print(f\"P&L attribution method: {pnl_method}\")\n",
    "\n",
    "                symbol_pnl = df_positions.groupby(symbol_col)[pnl_col].sum().sort_values(ascending=False)\n",
    "\n",
    "                print()\n",
    "                print(\"Total P&L by Symbol (attributed):\")\n",
    "                print(\"=\" * 60)\n",
    "                display(symbol_pnl.head(20).to_frame('Total P&L'))\n",
    "\n",
    "                daily_position_pnl = df_positions.groupby('date')[pnl_col].sum().reset_index()\n",
    "                daily_position_pnl.columns = ['date', 'position_pnl']\n",
    "                print()\n",
    "                print(f\"Total attributed from held positions: ${daily_position_pnl['position_pnl'].sum():,.2f}\")\n",
    "\n",
    "                if df_trades is not None and 'realized_pnl' in df_trades.columns and 'symbol' in df_trades.columns:\n",
    "                    realized_by_symbol = df_trades.groupby('symbol')['realized_pnl'].sum()\n",
    "                    print()\n",
    "                    print(f\"Total realized P&L from trades file: ${realized_by_symbol.sum():,.2f}\")\n",
    "            else:\n",
    "                print()\n",
    "                print(\"WARNING: Unable to compute returns-based attribution\")\n",
    "        else:\n",
    "            print()\n",
    "            print(\"WARNING: Missing symbol or price data for attribution\")\n",
    "\n",
    "    # Fallbacks if returns-based attribution isn't possible\n",
    "    if pnl_col is None:\n",
    "        if 'daily_total_net_pnl' in df_positions.columns:\n",
    "            pnl_col = 'daily_total_net_pnl'\n",
    "            attribution_label = 'daily_total_net_pnl column (realized + unrealized - fees)'\n",
    "            fees_already_included = True\n",
    "            print(\"Using daily_total_net_pnl column from positions.csv\")\n",
    "        elif 'daily_pnl' in df_positions.columns:\n",
    "            pnl_col = 'daily_pnl'\n",
    "            attribution_label = 'daily_pnl column'\n",
    "            print(\"Using daily_pnl column from positions.csv\")\n",
    "        elif 'pnl' in df_positions.columns:\n",
    "            pnl_col = 'pnl'\n",
    "            attribution_label = 'pnl column'\n",
    "        elif 'mtm_pnl' in df_positions.columns:\n",
    "            pnl_col = 'mtm_pnl'\n",
    "            attribution_label = 'mtm_pnl column'\n",
    "\n",
    "        if pnl_col:\n",
    "            symbol_pnl = df_positions.groupby(symbol_col)[pnl_col].sum().sort_values(ascending=False)\n",
    "            print()\n",
    "            print(\"Total P&L by Symbol (fallback):\")\n",
    "            print(\"=\" * 60)\n",
    "            display(symbol_pnl.head(20).to_frame('Total P&L'))\n",
    "\n",
    "            daily_position_pnl = df_positions.groupby('date')[pnl_col].sum().reset_index()\n",
    "            daily_position_pnl.columns = ['date', 'position_pnl']\n",
    "            print()\n",
    "            print(f\"Total attributed from held positions: ${daily_position_pnl['position_pnl'].sum():,.2f}\")\n",
    "\n",
    "        elif 'unrealized_pnl' in df_positions.columns and symbol_col:\n",
    "            print()\n",
    "            print(\"No daily_pnl column - computing from unrealized_pnl changes...\")\n",
    "            print(\"NOTE: Re-run backtest with updated logger for accurate attribution\")\n",
    "\n",
    "            positions = df_positions[['date', symbol_col, 'unrealized_pnl']].copy()\n",
    "            positions = positions.sort_values(['date', symbol_col])\n",
    "            positions['prev_unrealized'] = positions.groupby(symbol_col)['unrealized_pnl'].shift(1)\n",
    "            positions['daily_pnl'] = positions['unrealized_pnl'] - positions['prev_unrealized']\n",
    "            positions['daily_pnl'] = positions['daily_pnl'].fillna(positions['unrealized_pnl'])\n",
    "\n",
    "            pnl_col = 'daily_pnl'\n",
    "            attribution_label = 'unrealized_pnl delta'\n",
    "            df_positions = df_positions.merge(\n",
    "                positions[['date', symbol_col, 'daily_pnl']],\n",
    "                on=['date', symbol_col],\n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            symbol_pnl = df_positions.groupby(symbol_col)['daily_pnl'].sum().sort_values(ascending=False)\n",
    "            print()\n",
    "            print(\"Estimated P&L by Symbol:\")\n",
    "            print(\"=\" * 60)\n",
    "            display(symbol_pnl.head(20).to_frame('Total P&L'))\n",
    "\n",
    "            daily_position_pnl = df_positions.groupby('date')[pnl_col].sum().reset_index()\n",
    "            daily_position_pnl.columns = ['date', 'position_pnl']\n",
    "        else:\n",
    "            print()\n",
    "            print(\"WARNING: No P&L data available for attribution\")\n",
    "else:\n",
    "    print(\"Positions data not available for attribution analysis\")\n",
    "    pnl_col = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## P&L Reconciliation: Attributed vs NAV Change\n\nCompare the P&L we're accounting for (from positions) against the actual daily NAV change to identify any gaps.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "if df_snapshots is not None and (\n",
    "    ('daily_position_pnl' in dir() and daily_position_pnl is not None)\n",
    "    or (df_positions is not None and pnl_col and pnl_col in df_positions.columns)\n",
    "):\n",
    "    # Calculate NAV change from snapshots\n",
    "    df_snapshots_sorted = df_snapshots.sort_values('date').copy()\n",
    "    if 'daily_pnl' in df_snapshots_sorted.columns:\n",
    "        df_snapshots_sorted['nav_change'] = df_snapshots_sorted['daily_pnl']\n",
    "    else:\n",
    "        df_snapshots_sorted['nav_change'] = df_snapshots_sorted['nav'].diff()\n",
    "\n",
    "    # Daily P&L from attribution\n",
    "    if 'daily_position_pnl' in dir() and daily_position_pnl is not None:\n",
    "        daily_attributed = daily_position_pnl.copy()\n",
    "    else:\n",
    "        daily_position_pnl = df_positions.groupby('date')[pnl_col].sum().reset_index()\n",
    "        daily_position_pnl.columns = ['date', 'position_pnl']\n",
    "        daily_attributed = daily_position_pnl.copy()\n",
    "\n",
    "    fees_already_included = fees_already_included if 'fees_already_included' in dir() else False\n",
    "\n",
    "    # Optional realized P&L from trades file (not included in MTM attribution)\n",
    "    if df_trades is not None and 'realized_pnl' in df_trades.columns:\n",
    "        daily_realized = df_trades.groupby('date')['realized_pnl'].sum().reset_index()\n",
    "        daily_realized.columns = ['date', 'realized_pnl']\n",
    "        daily_attributed = daily_attributed.merge(daily_realized, on='date', how='left')\n",
    "\n",
    "    if 'realized_pnl' not in daily_attributed.columns:\n",
    "        daily_attributed['realized_pnl'] = 0.0\n",
    "    else:\n",
    "        daily_attributed['realized_pnl'] = daily_attributed['realized_pnl'].fillna(0.0)\n",
    "\n",
    "    # Fees from positions (preferred) or trades (subtract from NAV change)\n",
    "    if not fees_already_included:\n",
    "        if df_positions is not None and 'daily_fees' in df_positions.columns:\n",
    "            daily_fees = df_positions.groupby('date')['daily_fees'].sum().reset_index()\n",
    "            daily_fees.columns = ['date', 'fees']\n",
    "            daily_attributed = daily_attributed.merge(daily_fees, on='date', how='left')\n",
    "        elif df_trades is not None and 'fees' in df_trades.columns:\n",
    "            daily_fees = df_trades.groupby('date')['fees'].sum().reset_index()\n",
    "            daily_fees.columns = ['date', 'fees']\n",
    "            daily_attributed = daily_attributed.merge(daily_fees, on='date', how='left')\n",
    "\n",
    "    if 'fees' not in daily_attributed.columns:\n",
    "        daily_attributed['fees'] = 0.0\n",
    "    else:\n",
    "        daily_attributed['fees'] = daily_attributed['fees'].fillna(0.0)\n",
    "\n",
    "    # Dividends from positions (if available)\n",
    "    if df_positions is not None and 'daily_dividends' in df_positions.columns:\n",
    "        daily_dividends = df_positions.groupby('date')['daily_dividends'].sum().reset_index()\n",
    "        daily_dividends.columns = ['date', 'dividends']\n",
    "        daily_attributed = daily_attributed.merge(daily_dividends, on='date', how='left')\n",
    "\n",
    "    if 'dividends' not in daily_attributed.columns:\n",
    "        daily_attributed['dividends'] = 0.0\n",
    "    else:\n",
    "        daily_attributed['dividends'] = daily_attributed['dividends'].fillna(0.0)\n",
    "\n",
    "    daily_attributed['attributed_pnl'] = daily_attributed['position_pnl']\n",
    "    if fees_already_included:\n",
    "        daily_attributed['fees'] = 0.0\n",
    "        daily_attributed['attributed_net'] = daily_attributed['attributed_pnl']\n",
    "    else:\n",
    "        daily_attributed['attributed_net'] = daily_attributed['attributed_pnl'] - daily_attributed['fees']\n",
    "\n",
    "    # Merge with NAV changes\n",
    "    reconcile = df_snapshots_sorted[['date', 'nav', 'nav_change']].merge(\n",
    "        daily_attributed, on='date', how='left'\n",
    "    )\n",
    "    reconcile['position_pnl'] = reconcile['position_pnl'].fillna(0)\n",
    "    reconcile['realized_pnl'] = reconcile['realized_pnl'].fillna(0)\n",
    "    reconcile['attributed_pnl'] = reconcile['attributed_pnl'].fillna(0)\n",
    "    reconcile['fees'] = reconcile['fees'].fillna(0)\n",
    "    reconcile['attributed_net'] = reconcile['attributed_net'].fillna(0)\n",
    "    reconcile['dividends'] = reconcile['dividends'].fillna(0)\n",
    "\n",
    "    # Calculate unexplained (dividends informational under Adjusted)\n",
    "    reconcile['unexplained'] = reconcile['nav_change'] - reconcile['attributed_net']\n",
    "\n",
    "    # Summary statistics\n",
    "    label = attribution_label if 'attribution_label' in dir() and attribution_label else \"held positions\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"P&L RECONCILIATION: Attributed vs NAV Change\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(f\"Total NAV Change: ${reconcile['nav_change'].sum():,.2f}\")\n",
    "    print(f\"  - From held positions ({label}): ${reconcile['position_pnl'].sum():,.2f}\")\n",
    "    print(f\"  - Dividends (informational; adjusted pricing): ${reconcile['dividends'].sum():,.2f}\")\n",
    "    if df_trades is not None and 'realized_pnl' in df_trades.columns:\n",
    "        print(f\"  - Realized P&L in trades file (not included): ${reconcile['realized_pnl'].sum():,.2f}\")\n",
    "    if fees_already_included:\n",
    "        print(\"  - Fees: included in position P&L\")\n",
    "    elif df_positions is not None and 'daily_fees' in df_positions.columns:\n",
    "        print(f\"  - Fees (from positions file): ${reconcile['fees'].sum():,.2f}\")\n",
    "    elif df_trades is not None and 'fees' in df_trades.columns:\n",
    "        print(f\"  - Fees (from trades file): ${reconcile['fees'].sum():,.2f}\")\n",
    "    if fees_already_included:\n",
    "        print(f\"Total Attributed Net ({label}): ${reconcile['attributed_net'].sum():,.2f}\")\n",
    "    else:\n",
    "        print(f\"Total Attributed Net ({label} - fees): ${reconcile['attributed_net'].sum():,.2f}\")\n",
    "    print(f\"Total Unexplained: ${reconcile['unexplained'].sum():,.2f}\")\n",
    "\n",
    "    nav_change_total = reconcile['nav_change'].sum()\n",
    "    if nav_change_total != 0:\n",
    "        pct_unexplained = 100 * abs(reconcile['unexplained'].sum()) / abs(nav_change_total)\n",
    "        print()\n",
    "        print(f\"Unexplained as % of NAV Change: {pct_unexplained:.2f}%\")\n",
    "        if pct_unexplained < 1:\n",
    "            print(\"GOOD: Reconciliation is within 1%\")\n",
    "        elif pct_unexplained < 5:\n",
    "            print(\"WARNING: Reconciliation gap is 1-5%\")\n",
    "        else:\n",
    "            print(\"ALERT: Significant reconciliation gap (>5%)\")\n",
    "            print()\n",
    "            print(\"Possible causes of gap:\")\n",
    "            print(\"  - Rebalances or intraday trades (weights assumed from prior close)\")\n",
    "            print(\"  - Slippage, fees, interest, dividends\")\n",
    "            print(\"  - Dividends on adjusted pricing (NAV may not move on ex-date)\")\n",
    "            print(\"  - Data logged at different times than NAV snapshot\")\n",
    "\n",
    "    # Plot reconciliation\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "    # Plot 1: NAV Change vs Attributed Net P&L\n",
    "    axes[0].plot(reconcile['date'], reconcile['nav_change'], 'b-', linewidth=2, label='NAV Change', alpha=0.8)\n",
    "    axes[0].plot(reconcile['date'], reconcile['attributed_net'], 'g--', linewidth=2, label='Attributed Net P&L', alpha=0.8)\n",
    "    axes[0].set_title('Daily P&L Reconciliation: NAV Change vs Attributed Net', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('P&L ($)')\n",
    "    axes[0].legend(loc='upper left')\n",
    "    axes[0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Unexplained P&L\n",
    "    axes[1].bar(reconcile['date'], reconcile['unexplained'],\n",
    "                color=np.where(reconcile['unexplained'] >= 0, 'orange', 'purple'), alpha=0.7)\n",
    "    axes[1].set_title('Daily Unexplained P&L (NAV Change - Attributed Net)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Unexplained P&L ($)')\n",
    "    axes[1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Cumulative comparison\n",
    "    reconcile['cumulative_nav_change'] = reconcile['nav_change'].cumsum()\n",
    "    reconcile['cumulative_attributed_net'] = reconcile['attributed_net'].cumsum()\n",
    "    reconcile['cumulative_unexplained'] = reconcile['unexplained'].cumsum()\n",
    "\n",
    "    axes[2].plot(reconcile['date'], reconcile['cumulative_nav_change'], 'b-', linewidth=2, label='Cumulative NAV Change')\n",
    "    axes[2].plot(reconcile['date'], reconcile['cumulative_attributed_net'], 'g--', linewidth=2, label='Cumulative Attributed Net')\n",
    "    axes[2].fill_between(reconcile['date'], reconcile['cumulative_attributed_net'], reconcile['cumulative_nav_change'],\n",
    "                         alpha=0.3, color='red', label='Cumulative Gap')\n",
    "    axes[2].set_title('Cumulative P&L: NAV Change vs Attributed Net', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].set_ylabel('Cumulative P&L ($)')\n",
    "    axes[2].legend(loc='upper left')\n",
    "    axes[2].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Show days with largest unexplained P&L\n",
    "    if reconcile['unexplained'].abs().max() > 100:\n",
    "        print()\n",
    "        print(\"Days with largest unexplained P&L (top 10):\")\n",
    "        print(\"=\" * 80)\n",
    "        top_unexplained = reconcile.copy()\n",
    "        top_unexplained['unexplained_abs'] = top_unexplained['unexplained'].abs()\n",
    "        top_unexplained = top_unexplained.nlargest(10, 'unexplained_abs')[\n",
    "            ['date', 'nav_change', 'position_pnl', 'fees', 'attributed_net', 'unexplained']\n",
    "        ]\n",
    "        display(top_unexplained)\n",
    "\n",
    "else:\n",
    "    print(\"Insufficient data for P&L reconciliation\")\n",
    "    print(f\"  pnl_col: {pnl_col if 'pnl_col' in dir() else 'not set'}\")\n",
    "    if df_positions is not None:\n",
    "        print(f\"  positions columns: {list(df_positions.columns)}\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top/Bottom Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'symbol_pnl' in dir() and len(symbol_pnl) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Top contributors\n",
    "    top_10 = symbol_pnl.head(10)\n",
    "    axes[0].barh(top_10.index, top_10.values, color='green', alpha=0.7)\n",
    "    axes[0].set_title('Top 10 Contributors (P&L)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('P&L ($)')\n",
    "    axes[0].invert_yaxis()\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Bottom contributors\n",
    "    bottom_10 = symbol_pnl.tail(10)\n",
    "    axes[1].barh(bottom_10.index, bottom_10.values, color='red', alpha=0.7)\n",
    "    axes[1].set_title('Bottom 10 Contributors (P&L)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('P&L ($)')\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nP&L Attribution Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    total_pnl = symbol_pnl.sum()\n",
    "    positive_pnl = symbol_pnl[symbol_pnl > 0].sum()\n",
    "    negative_pnl = symbol_pnl[symbol_pnl < 0].sum()\n",
    "    \n",
    "    print(f\"Total P&L: ${total_pnl:,.2f}\")\n",
    "    print(f\"Winning positions P&L: ${positive_pnl:,.2f}\")\n",
    "    print(f\"Losing positions P&L: ${negative_pnl:,.2f}\")\n",
    "    print(f\"Win/Loss ratio: {abs(positive_pnl/negative_pnl):.2f}\" if negative_pnl != 0 else \"N/A\")\n",
    "    print(f\"Symbols with profit: {(symbol_pnl > 0).sum()} / {len(symbol_pnl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P&L Contribution Over Time by Symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if df_positions is not None and symbol_col and pnl_col:\n    # Get top symbols by total P&L contribution (by absolute value for better visibility)\n    top_symbols = symbol_pnl.abs().nlargest(10).index.tolist()\n    \n    # Aggregate daily P&L by symbol\n    daily_symbol_pnl = df_positions.groupby(['date', symbol_col])[pnl_col].sum().reset_index()\n    \n    # Pivot for top symbols\n    pivot = daily_symbol_pnl[daily_symbol_pnl[symbol_col].isin(top_symbols)].pivot_table(\n        index='date', columns=symbol_col, values=pnl_col, fill_value=0\n    ).sort_index()\n    \n    # Cumulative P&L\n    cumulative = pivot.cumsum()\n    \n    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n    \n    # Daily P&L as line plot (stacked area doesn't work with mixed +/- values)\n    pivot.plot(ax=axes[0], linewidth=1.5, alpha=0.8)\n    axes[0].set_title('Daily P&L by Symbol (Top 10 by Absolute Contribution)', fontsize=14, fontweight='bold')\n    axes[0].set_ylabel('Daily P&L ($)')\n    axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    axes[0].legend(loc='upper left', ncol=2, fontsize=8)\n    axes[0].grid(True, alpha=0.3)\n    \n    # Cumulative P&L\n    cumulative.plot(ax=axes[1], linewidth=2, alpha=0.8)\n    axes[1].set_title('Cumulative P&L by Symbol (Top 10)', fontsize=14, fontweight='bold')\n    axes[1].set_xlabel('Date')\n    axes[1].set_ylabel('Cumulative P&L ($)')\n    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n    axes[1].legend(loc='upper left', ncol=2, fontsize=8)\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.xticks(rotation=45, ha='right')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slippage Cost Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_slippage is not None and df_snapshots is not None:\n",
    "    # Find slippage column\n",
    "    slippage_col = None\n",
    "    for col in ['slippage', 'slippage_dollars', 'cost']:\n",
    "        if col in df_slippage.columns:\n",
    "            slippage_col = col\n",
    "            break\n",
    "    \n",
    "    if slippage_col is None:\n",
    "        # Try to compute from expected vs fill price\n",
    "        for exp_col in ['expected_price', 'signal_price']:\n",
    "            for fill_col in ['fill_price', 'execution_price']:\n",
    "                if exp_col in df_slippage.columns and fill_col in df_slippage.columns:\n",
    "                    df_slippage['slippage_dollars'] = df_slippage[fill_col] - df_slippage[exp_col]\n",
    "                    slippage_col = 'slippage_dollars'\n",
    "                    break\n",
    "    \n",
    "    if slippage_col:\n",
    "        # Daily slippage\n",
    "        daily_slippage = df_slippage.groupby('date')[slippage_col].sum().reset_index()\n",
    "        daily_slippage.columns = ['date', 'daily_slippage']\n",
    "        \n",
    "        # Merge with P&L data\n",
    "        merged = df_snapshots[['date', 'daily_pnl']].merge(daily_slippage, on='date', how='left')\n",
    "        merged['daily_slippage'] = merged['daily_slippage'].fillna(0)\n",
    "        \n",
    "        # Gross P&L (before slippage)\n",
    "        merged['gross_pnl'] = merged['daily_pnl'] + merged['daily_slippage']\n",
    "        \n",
    "        print(\"\\nSlippage Impact:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total slippage cost: ${merged['daily_slippage'].sum():,.2f}\")\n",
    "        print(f\"Net P&L: ${merged['daily_pnl'].sum():,.2f}\")\n",
    "        print(f\"Gross P&L (before slippage): ${merged['gross_pnl'].sum():,.2f}\")\n",
    "        print(f\"Slippage as % of gross P&L: {merged['daily_slippage'].sum() / merged['gross_pnl'].sum() * 100:.2f}%\" if merged['gross_pnl'].sum() != 0 else \"N/A\")\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        merged['cumulative_net'] = merged['daily_pnl'].cumsum()\n",
    "        merged['cumulative_gross'] = merged['gross_pnl'].cumsum()\n",
    "        merged['cumulative_slippage'] = merged['daily_slippage'].cumsum()\n",
    "        \n",
    "        ax.plot(merged['date'], merged['cumulative_gross'], linewidth=2, label='Gross P&L', color='blue', alpha=0.7)\n",
    "        ax.plot(merged['date'], merged['cumulative_net'], linewidth=2, label='Net P&L', color='green', alpha=0.7)\n",
    "        ax.fill_between(merged['date'], merged['cumulative_gross'], merged['cumulative_net'], \n",
    "                        alpha=0.3, color='red', label='Slippage Cost')\n",
    "        \n",
    "        ax.set_title('Cumulative P&L: Gross vs Net (Slippage Impact)', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Cumulative P&L ($)')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Could not identify slippage column for overlay\")\n",
    "else:\n",
    "    print(\"Slippage data not available for overlay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P&L by Direction (Long vs Short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_positions is not None and weight_col and pnl_col:\n",
    "    # Classify positions by direction\n",
    "    df_positions['direction'] = np.where(df_positions[weight_col] > 0, 'Long', 'Short')\n",
    "    \n",
    "    # Aggregate by direction\n",
    "    direction_pnl = df_positions.groupby('direction')[pnl_col].sum()\n",
    "    \n",
    "    print(\"\\nP&L by Direction:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(direction_pnl.to_frame('Total P&L'))\n",
    "    \n",
    "    # Daily breakdown\n",
    "    daily_direction = df_positions.groupby(['date', 'direction'])[pnl_col].sum().reset_index()\n",
    "    pivot_direction = daily_direction.pivot(index='date', columns='direction', values=pnl_col).fillna(0)\n",
    "    \n",
    "    # Cumulative\n",
    "    cumulative_direction = pivot_direction.cumsum()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    for col in cumulative_direction.columns:\n",
    "        color = 'green' if col == 'Long' else 'red'\n",
    "        ax.plot(cumulative_direction.index, cumulative_direction[col], linewidth=2, label=col, color=color)\n",
    "    \n",
    "    ax.set_title('Cumulative P&L by Direction', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Cumulative P&L ($)')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif df_positions is not None and weight_col:\n",
    "    print(\"P&L column not found - direction analysis requires P&L data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P&L Attribution by Signal Horizon (Short / Medium / Long)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if df_positions is not None and df_signals is not None and pnl_col and symbol_col:\n",
    "    # Build per-signal component scores from SMA distances (mirrors alpha model logic)\n",
    "    signals = df_signals.copy()\n",
    "\n",
    "    # Normalize symbol column\n",
    "    if 'symbol' not in signals.columns:\n",
    "        if symbol_col in signals.columns:\n",
    "            signals['symbol'] = signals[symbol_col]\n",
    "        else:\n",
    "            print(\"Signals file missing symbol column; cannot do horizon attribution\")\n",
    "            signals = None\n",
    "\n",
    "    required_cols = ['price', 'sma_short', 'sma_medium', 'sma_long', 'atr']\n",
    "    if signals is not None:\n",
    "        missing = [c for c in required_cols if c not in signals.columns]\n",
    "        if missing:\n",
    "            print(f\"Signals file missing columns: {missing} - cannot do horizon attribution\")\n",
    "            signals = None\n",
    "\n",
    "    if signals is not None:\n",
    "        # Ensure numeric values\n",
    "        for col in required_cols:\n",
    "            signals[col] = pd.to_numeric(signals[col], errors='coerce')\n",
    "        signals = signals.dropna(subset=required_cols).copy()\n",
    "\n",
    "        # Deduplicate signals per symbol/date (keep latest)\n",
    "        signals = signals.sort_values('date').groupby(['date', 'symbol']).tail(1)\n",
    "\n",
    "        # Compute SMA distance components\n",
    "        atr_safe = signals['atr'].replace(0, np.nan)\n",
    "        signals['dist_short'] = (signals['price'] - signals['sma_short']) / atr_safe\n",
    "        signals['dist_medium'] = (signals['price'] - signals['sma_medium']) / atr_safe\n",
    "        signals['dist_long'] = (signals['price'] - signals['sma_long']) / atr_safe\n",
    "\n",
    "        # Weights from alpha model (update if changed in main.py)\n",
    "        weights = {'short': 0.5, 'medium': 0.3, 'long': 0.2}\n",
    "        signals['comp_short'] = weights['short'] * signals['dist_short']\n",
    "        signals['comp_medium'] = weights['medium'] * signals['dist_medium']\n",
    "        signals['comp_long'] = weights['long'] * signals['dist_long']\n",
    "        signals['abs_total'] = signals[['comp_short', 'comp_medium', 'comp_long']].abs().sum(axis=1)\n",
    "\n",
    "        signals = signals[signals['abs_total'] > 0].copy()\n",
    "\n",
    "        # Daily position P&L by symbol\n",
    "        position_pnl_daily = df_positions.groupby(['date', symbol_col])[pnl_col].sum().reset_index()\n",
    "        position_pnl_daily = position_pnl_daily.rename(columns={symbol_col: 'symbol', pnl_col: 'position_pnl'})\n",
    "\n",
    "        # Attach most recent signal per symbol (forward-fill)\n",
    "        signals_sorted = signals.sort_values(['symbol', 'date'])\n",
    "        pnl_sorted = position_pnl_daily.sort_values(['symbol', 'date'])\n",
    "\n",
    "        # Defensive datetime conversion (no-op if already datetime)\n",
    "        signals_sorted['date'] = pd.to_datetime(signals_sorted['date'])\n",
    "        pnl_sorted['date'] = pd.to_datetime(pnl_sorted['date'])\n",
    "\n",
    "        merged_horizon = pnl_sorted.merge(\n",
    "            signals_sorted[['date', 'symbol', 'comp_short', 'comp_medium', 'comp_long', 'abs_total']],\n",
    "            on=['symbol', 'date'],\n",
    "            how='left'\n",
    "        ).sort_values(['symbol', 'date'])\n",
    "\n",
    "        merged_horizon[['comp_short', 'comp_medium', 'comp_long', 'abs_total']] = (\n",
    "            merged_horizon.groupby('symbol')[['comp_short', 'comp_medium', 'comp_long', 'abs_total']]\n",
    "            .ffill()\n",
    "        )\n",
    "\n",
    "        merged_horizon = merged_horizon[merged_horizon['abs_total'].notna() & (merged_horizon['abs_total'] > 0)].copy()\n",
    "\n",
    "        if merged_horizon.empty:\n",
    "            print(\"No overlapping signal history with positions; horizon attribution skipped\")\n",
    "        else:\n",
    "            # Allocate daily P&L proportional to component magnitudes\n",
    "            merged_horizon['short_share'] = merged_horizon['comp_short'].abs() / merged_horizon['abs_total']\n",
    "            merged_horizon['medium_share'] = merged_horizon['comp_medium'].abs() / merged_horizon['abs_total']\n",
    "            merged_horizon['long_share'] = merged_horizon['comp_long'].abs() / merged_horizon['abs_total']\n",
    "\n",
    "            merged_horizon['short_pnl'] = merged_horizon['position_pnl'] * merged_horizon['short_share']\n",
    "            merged_horizon['medium_pnl'] = merged_horizon['position_pnl'] * merged_horizon['medium_share']\n",
    "            merged_horizon['long_pnl'] = merged_horizon['position_pnl'] * merged_horizon['long_share']\n",
    "\n",
    "            # Daily horizon P&L\n",
    "            daily_horizon = merged_horizon.groupby('date')[['short_pnl', 'medium_pnl', 'long_pnl']].sum().reset_index()\n",
    "\n",
    "            # Summary table\n",
    "            totals = daily_horizon[['short_pnl', 'medium_pnl', 'long_pnl']].sum()\n",
    "            totals = totals.rename({'short_pnl': 'Short', 'medium_pnl': 'Medium', 'long_pnl': 'Long'})\n",
    "            totals_df = totals.to_frame('Total P&L')\n",
    "            total_attributed = totals_df['Total P&L'].sum()\n",
    "            if total_attributed != 0:\n",
    "                totals_df['Pct of Attributed'] = totals_df['Total P&L'] / total_attributed\n",
    "            else:\n",
    "                totals_df['Pct of Attributed'] = 0.0\n",
    "\n",
    "            print()\n",
    "            print(\"Horizon Attribution Summary (heuristic allocation):\")\n",
    "            print(\"=\" * 60)\n",
    "            display(totals_df)\n",
    "\n",
    "            # Coverage vs total position P&L\n",
    "            covered_pnl = merged_horizon['position_pnl'].sum()\n",
    "            total_position_pnl = position_pnl_daily['position_pnl'].sum()\n",
    "            if total_position_pnl != 0:\n",
    "                coverage_pct = covered_pnl / total_position_pnl * 100\n",
    "                print(f\"Coverage of position P&L by signals: {coverage_pct:.2f}%\")\n",
    "            else:\n",
    "                print(\"Coverage of position P&L by signals: N/A (zero total P&L)\")\n",
    "\n",
    "            # Plot 1: Cumulative P&L by horizon\n",
    "            cumulative_horizon = daily_horizon.set_index('date').cumsum()\n",
    "            fig, ax = plt.subplots(figsize=(14, 6))\n",
    "            ax.plot(cumulative_horizon.index, cumulative_horizon['short_pnl'], label='Short', color='#1f77b4', linewidth=2)\n",
    "            ax.plot(cumulative_horizon.index, cumulative_horizon['medium_pnl'], label='Medium', color='#ff7f0e', linewidth=2)\n",
    "            ax.plot(cumulative_horizon.index, cumulative_horizon['long_pnl'], label='Long', color='#2ca02c', linewidth=2)\n",
    "            ax.set_title('Cumulative P&L by Signal Horizon', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Cumulative P&L ($)')\n",
    "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(loc='upper left')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Plot 2: Monthly P&L stacked by horizon\n",
    "            monthly_horizon = daily_horizon.copy()\n",
    "            monthly_horizon['year_month'] = monthly_horizon['date'].dt.to_period('M')\n",
    "            monthly_horizon = monthly_horizon.groupby('year_month')[['short_pnl', 'medium_pnl', 'long_pnl']].sum()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(14, 6))\n",
    "            monthly_horizon.plot(kind='bar', stacked=True, ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "            ax.set_title('Monthly P&L by Signal Horizon (Stacked)', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Month')\n",
    "            ax.set_ylabel('P&L ($)')\n",
    "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            ax.set_xticklabels([str(p) for p in monthly_horizon.index], rotation=45, ha='right')\n",
    "            ax.legend(loc='upper left')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"Signal horizon attribution skipped (missing positions/signals or P&L column)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly P&L Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_snapshots is not None:\n",
    "    # Add month column\n",
    "    df_snapshots['year_month'] = df_snapshots['date'].dt.to_period('M')\n",
    "    \n",
    "    # Monthly P&L\n",
    "    monthly_pnl = df_snapshots.groupby('year_month')['daily_pnl'].sum()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    colors = ['green' if x >= 0 else 'red' for x in monthly_pnl.values]\n",
    "    ax.bar(range(len(monthly_pnl)), monthly_pnl.values, color=colors, alpha=0.7)\n",
    "    ax.set_xticks(range(len(monthly_pnl)))\n",
    "    ax.set_xticklabels([str(p) for p in monthly_pnl.index], rotation=45, ha='right')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    ax.set_title('Monthly P&L', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('P&L ($)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMonthly P&L Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Best month: ${monthly_pnl.max():,.2f} ({monthly_pnl.idxmax()})\")\n",
    "    print(f\"Worst month: ${monthly_pnl.min():,.2f} ({monthly_pnl.idxmin()})\")\n",
    "    print(f\"Average month: ${monthly_pnl.mean():,.2f}\")\n",
    "    print(f\"Positive months: {(monthly_pnl > 0).sum()} / {len(monthly_pnl)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"P&L ATTRIBUTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if df_snapshots is not None:\n",
    "    total_pnl = df_snapshots['daily_pnl'].sum()\n",
    "    print(f\"\\nTotal P&L: ${total_pnl:,.2f}\")\n",
    "    print(f\"Trading days: {len(df_snapshots)}\")\n",
    "    print(f\"Average daily P&L: ${df_snapshots['daily_pnl'].mean():,.2f}\")\n",
    "    print(f\"Daily P&L std: ${df_snapshots['daily_pnl'].std():,.2f}\")\n",
    "\n",
    "if 'symbol_pnl' in dir() and len(symbol_pnl) > 0:\n",
    "    print(f\"\\nSymbol Attribution:\")\n",
    "    print(f\"  Symbols traded: {len(symbol_pnl)}\")\n",
    "    print(f\"  Profitable symbols: {(symbol_pnl > 0).sum()}\")\n",
    "    print(f\"  Top contributor: {symbol_pnl.index[0]} (${symbol_pnl.iloc[0]:,.2f})\")\n",
    "    print(f\"  Bottom contributor: {symbol_pnl.index[-1]} (${symbol_pnl.iloc[-1]:,.2f})\")\n",
    "\n",
    "if df_slippage is not None and merged is not None and 'daily_slippage' in merged.columns:\n",
    "    total_slippage = merged['daily_slippage'].sum()\n",
    "    print(f\"\\nSlippage Impact:\")\n",
    "    print(f\"  Total slippage: ${total_slippage:,.2f}\")\n",
    "    print(f\"  Slippage % of gross: {total_slippage / merged['gross_pnl'].sum() * 100:.2f}%\" if merged['gross_pnl'].sum() != 0 else \"N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}